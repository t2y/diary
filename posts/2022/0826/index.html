<!doctype html><html lang=en><head><title>簡単な現象の組み合わせ障害 :: forest nook</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="0時に寝て6時に起きた。
eks クラスター障害の原因判明 過去に2回発生していた eks クラスター障害 の原因がようやくわかった。テスト環境も本番環境は5日ごとに再現していて、datadog で k8s のダッシュボードでそれぞれの pod 単位のメモリ使用量をみると datadog-agent の pod がメモリリークしていることに気付いた。そこから当たりをつけて datadog-agent の issue を調べると次のバグに遭遇していた。
[BUG] agent leaves defunct processes with version 7.38.0 #12997 ゾンビプロセスが生成されて、それが os のプロセス数上限に達してしまい、それによってプロセス (スレッド) が生成できなくなって、その結果として aws/amazon-vpc-cni-k8s の aws-node という eks クラスターの管理アプリケーションが動かなくなって、それが動かないと k8s ノードのステータスが NotReady になってしまって、通常の pod のアプリケーションも動かなくなってしまうという現象が発生していた。datadog-agent のアップグレードは私が行ったものだし、その後の k8s ノードの監視や調査で気付きが足りなかったと反省した。
datadog-agent の新しいバージョンをテスト環境でもうしばらく検証してもよかった datadog-agent をリソースリークの可能性を私の中の調査対象から外していた 世の中で使われているものに致命的なバグが起きないだろうという先入観があった プロセスを生成できない原因として考えられる背景を調査すべきだった ulimit を確認してリソース制限はないようにみえた プロセス数やゾンビプロセスを調べていなかった kernel に /proc/sys/kernel/pid_max という上限設定があることを知らなかった テスト環境と本番環境で5日程度で落ちるという周期性から気付くべきだった たしかにテスト環境から1日遅れて本番環境で障害が発生していた 周期性があることでリソースリークの可能性は高いとすぐに調査すべきだった datadog で k8s のダッシュボードを調べるべきだった すでに用意されているものがあったのでみようと思えばみえた aws のインフラ要因ではないかと疑っていた ごめんなさい これは悔しい。自分の無能さや気付きの低さを実感した事件だった。私が注意深く観察していればもう1週間早く気付けた。そのせいで余分な障害と調査に時間を費やした。1つ1つは全く難しくない現象が巧妙に絡みあって隠蔽された結果としての状況に気付けなかった。注意して1つずつ観察して追跡していけばすぐに気付けた。本当に悔しい。"><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=/diary/posts/2022/0826/><link rel=stylesheet href=/diary/assets/style.css><link rel=stylesheet href=/diary/assets/green.css><link rel=stylesheet href=/diary/style.css><link rel=apple-touch-icon href=/diary/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=/diary/favicon.ico><meta name=twitter:card content="summary"><meta name=twitter:site content="t2y"><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="簡単な現象の組み合わせ障害"><meta property="og:description" content="0時に寝て6時に起きた。
eks クラスター障害の原因判明 過去に2回発生していた eks クラスター障害 の原因がようやくわかった。テスト環境も本番環境は5日ごとに再現していて、datadog で k8s のダッシュボードでそれぞれの pod 単位のメモリ使用量をみると datadog-agent の pod がメモリリークしていることに気付いた。そこから当たりをつけて datadog-agent の issue を調べると次のバグに遭遇していた。
[BUG] agent leaves defunct processes with version 7.38.0 #12997 ゾンビプロセスが生成されて、それが os のプロセス数上限に達してしまい、それによってプロセス (スレッド) が生成できなくなって、その結果として aws/amazon-vpc-cni-k8s の aws-node という eks クラスターの管理アプリケーションが動かなくなって、それが動かないと k8s ノードのステータスが NotReady になってしまって、通常の pod のアプリケーションも動かなくなってしまうという現象が発生していた。datadog-agent のアップグレードは私が行ったものだし、その後の k8s ノードの監視や調査で気付きが足りなかったと反省した。
datadog-agent の新しいバージョンをテスト環境でもうしばらく検証してもよかった datadog-agent をリソースリークの可能性を私の中の調査対象から外していた 世の中で使われているものに致命的なバグが起きないだろうという先入観があった プロセスを生成できない原因として考えられる背景を調査すべきだった ulimit を確認してリソース制限はないようにみえた プロセス数やゾンビプロセスを調べていなかった kernel に /proc/sys/kernel/pid_max という上限設定があることを知らなかった テスト環境と本番環境で5日程度で落ちるという周期性から気付くべきだった たしかにテスト環境から1日遅れて本番環境で障害が発生していた 周期性があることでリソースリークの可能性は高いとすぐに調査すべきだった datadog で k8s のダッシュボードを調べるべきだった すでに用意されているものがあったのでみようと思えばみえた aws のインフラ要因ではないかと疑っていた ごめんなさい これは悔しい。自分の無能さや気付きの低さを実感した事件だった。私が注意深く観察していればもう1週間早く気付けた。そのせいで余分な障害と調査に時間を費やした。1つ1つは全く難しくない現象が巧妙に絡みあって隠蔽された結果としての状況に気付けなかった。注意して1つずつ観察して追跡していけばすぐに気付けた。本当に悔しい。"><meta property="og:url" content="/diary/posts/2022/0826/"><meta property="og:site_name" content="forest nook"><meta property="og:image" content="/diary"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2022-08-26 12:03:43 +0900 +0900"></head><body class=green><div class="container full headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/diary><div class=logo>forest nook</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/diary/about>自己紹介</a></li><li><a href=/diary/dates>月別一覧</a></li><li><a href=/diary/tags>タグ一覧</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/diary/about>自己紹介</a></li><li><a href=/diary/dates>月別一覧</a></li><li><a href=/diary/tags>タグ一覧</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=/diary/posts/2022/0826/>簡単な現象の組み合わせ障害</a></h1><div class=post-meta><span class=post-date>2022-08-26</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/datadog/>datadog</a>&nbsp;
#<a href=/diary/tags/operation/>operation</a>&nbsp;
#<a href=/diary/tags/drinking/>drinking</a>&nbsp;
#<a href=/diary/tags/tax/>tax</a>&nbsp;</span><div class=post-content><div><p>0時に寝て6時に起きた。</p><h2 id=eks-クラスター障害の原因判明>eks クラスター障害の原因判明<a href=#eks-クラスター障害の原因判明 class=hanchor arialabel=Anchor>&#8983;</a></h2><p><a href=/diary/posts/2022/0820/#aws-インフラの調子が悪い>過去に2回発生していた eks クラスター障害</a> の原因がようやくわかった。テスト環境も本番環境は5日ごとに再現していて、datadog で k8s のダッシュボードでそれぞれの pod 単位のメモリ使用量をみると datadog-agent の pod がメモリリークしていることに気付いた。そこから当たりをつけて datadog-agent の issue を調べると次のバグに遭遇していた。</p><ul><li><a href=https://github.com/DataDog/datadog-agent/issues/12997>[BUG] agent leaves defunct processes with version 7.38.0 #12997</a></li></ul><p>ゾンビプロセスが生成されて、それが os のプロセス数上限に達してしまい、それによってプロセス (スレッド) が生成できなくなって、その結果として <a href=https://github.com/aws/amazon-vpc-cni-k8s>aws/amazon-vpc-cni-k8s</a> の <code>aws-node</code> という eks クラスターの管理アプリケーションが動かなくなって、それが動かないと k8s ノードのステータスが NotReady になってしまって、通常の pod のアプリケーションも動かなくなってしまうという現象が発生していた。datadog-agent のアップグレードは私が行ったものだし、その後の k8s ノードの監視や調査で気付きが足りなかったと反省した。</p><ul><li>datadog-agent の新しいバージョンをテスト環境でもうしばらく検証してもよかった</li><li>datadog-agent をリソースリークの可能性を私の中の調査対象から外していた<ul><li>世の中で使われているものに致命的なバグが起きないだろうという先入観があった</li></ul></li><li>プロセスを生成できない原因として考えられる背景を調査すべきだった<ul><li>ulimit を確認してリソース制限はないようにみえた</li><li>プロセス数やゾンビプロセスを調べていなかった</li><li>kernel に <code>/proc/sys/kernel/pid_max</code> という上限設定があることを知らなかった</li></ul></li><li>テスト環境と本番環境で5日程度で落ちるという周期性から気付くべきだった<ul><li>たしかにテスト環境から1日遅れて本番環境で障害が発生していた</li><li>周期性があることでリソースリークの可能性は高いとすぐに調査すべきだった</li></ul></li><li>datadog で k8s のダッシュボードを調べるべきだった<ul><li>すでに用意されているものがあったのでみようと思えばみえた</li></ul></li><li>aws のインフラ要因ではないかと疑っていた<ul><li>ごめんなさい</li></ul></li></ul><p>これは悔しい。自分の無能さや気付きの低さを実感した事件だった。私が注意深く観察していればもう1週間早く気付けた。そのせいで余分な障害と調査に時間を費やした。1つ1つは全く難しくない現象が巧妙に絡みあって隠蔽された結果としての状況に気付けなかった。注意して1つずつ観察して追跡していけばすぐに気付けた。本当に悔しい。</p><p>1つだけ言い訳をさせてもらうと、私は本番環境にアクセスできない。だからテスト環境と本番環境で発生している現象が同じかどうかを判断できず、調査を進める確証をもてなかった。</p><h2 id=呑み>呑み<a href=#呑み class=hanchor arialabel=Anchor>&#8983;</a></h2><p>あまりに悔しかったのと調査してたら遅くなって晩ご飯食べる気力もなかったので気分転換に仲のよい焼き鳥屋さんに寄ってみた。あとから常連客のセブンイレブンの店長さんも来られて、私は初対面かなと思ってたんだけど先方は知っていると言ってたから以前にもカウンターでご一緒していたみたい。何気はなしに3人で2時前ぐらいまで雑談していた。</p><p>その店長さんがロレックスを購入しようと考えているという話しになって、資産または投資商品としてのロレックスの話しになった。たまたまヒカキンが1億円で買ったロレックスがいま2億円になっているといった話しがあったそうで、いまがバブルな状態らしいが、ロレックスをはじめとした高級時計の資産価値が上がっているらしい。私は腕時計を身につけないし高級時計もまったく興味はないが、投資商品の1つなんだというところに関心がもてた。</p><div class=video-container><iframe src=https://www.youtube.com/embed/1knsQZLeh7U allowfullscreen title=1億円で買った時計が大変なことになってしまいました…></iframe></div><p>中小企業の社長の一般的な節税方法の1つに外車を買ったり売ったりするという話しがある。儲かったときに経費で外車を買って、赤字のときに外車を売って雑所得に変える。車は社用車として経費で落とせるから可能なことだが、高級時計はどうなのだろうか？ 結論から言うと、普通の会社では高級時計は経費にできない。経費の原則は売上を上げるために必要な支出を経費とできる。普通の会社は高級時計で売上を上げることはできない。一方で経費として認められる職業もある。芸能人がそうだという。それは番組のために必要だという理屈で経費で落とせる。おそらくヒカキンも経費で高級時計を購入して、そのことを動画にしているのも仕事で必要だという言い訳作りの目的もあるのだと推測する。</p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=/diary/posts/2022/0827/><span class=button__icon>←</span>
<span class=button__text>eks クラスター障害のふりかえり</span></a></span>
<span class="button next"><a href=/diary/posts/2022/0825/><span class=button__text>vuetify のコンポーネント調査</span>
<span class=button__icon>→</span></a></span></div></div></div></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>© 2021 Tetsuya Morimoto</span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=/diary/assets/main.js></script>
<script src=/diary/assets/prism.js></script></div></body></html>