---
title: "簡単な現象の組み合わせ障害"
date: "2022-08-26T12:03:43+09:00"
dates: [2022/08]
cover: ""
tags: [kubernetes, datadog, operation]
showFullContent: true
---

0時に寝て6時に起きた。

## eks クラスター障害の原因判明

[過去に2回発生していた eks クラスター障害]({{< ref "posts/2022/0820.md#aws-インフラの調子が悪い" >}}) の原因がようやくわかった。テスト環境も本番環境は5日ごとに再現していて、datadog で k8s のダッシュボードでそれぞれの pod 単位のメモリ使用量をみると datadog-agent の pod がメモリリークしていることに気付いた。そこから当たりをつけて datadog-agent の issue を調べると次のバグに遭遇していた。

* [[BUG] agent leaves defunct processes with version 7.38.0 #12997](https://github.com/DataDog/datadog-agent/issues/12997)

ゾンビプロセスが生成されて、それが os のプロセス数上限に達してしまい、それによってプロセス (スレッド) が生成できなくなって、その結果として [aws/amazon-vpc-cni-k8s](https://github.com/aws/amazon-vpc-cni-k8s) の `aws-node` という eks クラスターの管理アプリケーションが動かなくなって、それが動かないと k8s ノードのステータスが NotReady になってしまって、通常の pod のアプリケーションも動かなくなってしまうという現象が発生していた。datadog-agent のアップグレードは私が行ったものだし、その後の k8s ノードの監視や調査で気付きが足りなかったと反省した。

* datadog-agent の新しいバージョンをテスト環境でもうしばらく検証してもよかった
* datadog-agent をリソースリークの可能性を私の中の調査対象から外していた
  * 世の中で使われているものに致命的なバグが起きないだろうという先入観があった
* プロセスを生成できない原因として考えられる背景を調査すべきだった
  * ulimit を確認してリソース制限はないようにみえた
  * プロセス数やゾンビプロセスを調べていなかった
  * kernel に `/proc/sys/kernel/pid_max` という上限設定があることを知らなかった
* テスト環境と本番環境で5日程度で落ちるという周期性から気付くべきだった
  * たしかにテスト環境から1日遅れて本番環境で障害が発生していた
  * 周期性があることでリソースリークの可能性は高いとすぐに調査すべきだった
* datadog で k8s のダッシュボードを調べるべきだった
  * すでに用意されているものがあったのでみようと思えばみえた
* aws のインフラ要因ではないかと疑っていた
  * ごめんなさい

これは悔しい。自分の無能さや気付きの低さを実感した事件だった。私が注意深く観察していればもう1週間早く気付けた。そのせいで余分な障害と調査に時間を費やした。1つ1つは全く難しくない現象が巧妙に絡みあって隠蔽された結果としての状況に気付けなかった。注意して1つずつ観察して追跡していけばすぐに気付けた。本当に悔しい。

1つだけ言い訳をさせてもらうと、私は本番環境にアクセスできない。だからテスト環境と本番環境で発生している現象が同じかどうかを判断できず、調査を進める確証をもてなかった。
