---
title: "vimgrep 検索の嬉しさ"
date: "2021-10-13T09:02:46+09:00"
dates: [2021/10]
cover: "img/2021/1013_vimgrep.png"
tags: ["morning activity", book, event]
showFullContent: true
---

2時頃に寝て6時に起きる。普段、日記は vim で書いている。ちょっとした過去の日記の検索に `vimgrep` でこと足りるのが嬉しい。テキストで日記を書いていることの利点かな。夜に [fin-pyコードリーディング会#4](https://fin-py.connpass.com/event/226933/) に参加した。事前に [hackmd](https://hackmd.io/bFBFaPbYS1Kqfc97HMlp7Q?view) に発表内容のメモを書いてた。いろんな発表者の視点があってコードリーディングのイベントはおもしろかった。

## 朝活

[【三宮.dev オンライン】リモート朝活もくもく会](https://kobe-sannomiya-dev.connpass.com/event/227678/) に参加してみた。何もなかったらだいたい7時頃に起きるのがなにか目的があると6時に起きれる。人体の不思議。せっかく起きたので [前に More Joel on Software を読んだとき]({{< ref "posts/2021/0929.md#joel-on-software" >}}) に学生向けのアドバイスにあったミクロ経済学の勉強のためにその入門書を読み始めた。参加者が勉強会の常連ばかりだったので朝からわりと雑談してた。2人転職するという話で2人とも東京の会社でフルリモートワークで働くらしい。働き方が変わったなと感じる。その後、第1章の無差別曲線を読んだ。

## YouTube 配信と集中力

あんちぽさんの [2021年10月9日](https://kentarokuribayashi.com/journal/2021/10/09/2021%e5%b9%b410%e6%9c%889%e6%97%a5) の日記でスライド作成の興がのらないので YouTube 配信しながらやったら集中できてよかったと書いてあったのでちょっと眺めてみた。なんかスライドの作成のやり方とか、自分と違うのかな？とか思いながらみたけど、やり方自体は普通だった。ただ集中できてよかったとあるので普段のやり方とは異なることをすることに意義があるのかな？とも思えた。試しに YouTube Live やってみようとしたら初期設定？に24時間かかるとのこと。代わりに [kazam](https://launchpad.net/kazam) というスクリーンリコーダーの使い方を調べてた。勉強会で作業したログとかを録画しておいてなにかに使えたりするかもしれない。

## データ指向アプリケーションデザイン

半日ほどかかって3章ストレージと抽出を読んだ。読みながら書いているので時間がかかる。今日はこれだけ。まとめはこんな感じ。

> データベースのシステムには2つの用途があり、その特性やパフォーマンスを最適化するためにストレージエンジンやデータ構造が異なるもので運用されるようになってきた。
> 
> * オンライントランザクション処理 (OLTP)
>     * 行指向、トランザクション処理
> * オンライン分析処理 (OLAP)
>     * 列指向、分析クエリ
> 
> OLTP には2つの主要なストレージエンジンがある。
> 
> * B ツリー
>     * 1970年代からあり、成熟していて且つ効率的なインデックスのデータ構造
> * LSM ツリー
>     * 比較的最近開発された、ディスク上でのランダムアクセスをシステム的にシーケンシャルアクセスに変換して、書き込みのスループットを高める手法。もとは Google の BigTable の論文？
> 
> OLAP の典型的なデータウェアハウスの高レベルでのアーキテクチャでは、大量の行をシーケンシャルにスキャンしなければならないクエリの場合、インデックスはあまり関係なく、データを非常にコンパクトにエンコードし、クエリがディスクから読まなければならないデータの量を最小限にとどめることが重要となる。この目標を達成するのに列指向のストレージが役立つ。

過去に Cassandra を使ったプロダクトの開発に関わっていたから B ツリーと LSM ツリーの概要は知っていて3章で書いてあることはだいたい理解できた。データウェアハウスに関しては、前にお手伝いしていた会社で普通のログを Amazon Athena で処理すると1時間とかかかって分析クエリが Parquet に変換すると数分で完了したりするのを目の当たりにしてた。分析処理で読み込むデータ量を削減する列指向の考え方は理解しておく必要がある。行指向のデータを列指向フォーマットである [Parquet](https://parquet.apache.org/) に変換する [columnify](https://github.com/reproio/columnify) のコードも読んだことがあったので内容のイメージはできるけど、実務経験が少ないと全体像がわかっておらず、本書を読みながら学び直ししてた。
