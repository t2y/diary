<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>infrastructure on forest nook</title><link>/diary/tags/infrastructure/</link><description>Recent content in infrastructure on forest nook</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><copyright>© 2021 Tetsuya Morimoto</copyright><lastBuildDate>Tue, 05 Jul 2022 08:26:22 +0900</lastBuildDate><atom:icon>/diary/favicon.ico</atom:icon><icon>/diary/favicon.ico</icon><atom:link href="/diary/tags/infrastructure/index.xml" rel="self" type="application/rss+xml"/><item><title>サービスインは突然に</title><link>/diary/posts/2022/0705/</link><pubDate>Tue, 05 Jul 2022 08:26:22 +0900</pubDate><guid>/diary/posts/2022/0705/</guid><description>1時に寝て6時に起きた。暑くてあまり眠れない。今朝も雨降りで徒歩通勤。
cdk の ECS サービスに紐づくセキュリティグループの設定 明日がサービスイン初日だと思っていたら、私の勘違いで今日だった。私が直近でやっている作業はアプリケーションの直接的な機能ではなく、インフラやバッチ処理などの間接的な機能を作っているわけだけど、それでも1日調整を間違えていて、あれーって感じでサービスインが始まった。とはいえ、私は本番環境にアクセスできなければ、ログすらもみれないので同僚ががんばっているのを傍から応援しつつ、平常通りタスクをこなしていくだけのはずであった。
のほほんと通常通りのタスクをやっていたら、本番環境の ecs サービスと通信できないという連絡がくる。私が前任者から引き継いで構築したインフラなので何だろう？と調査していて、本番環境でセキュリティグループの設定が漏れていることがわかった。これはわかりにくい問題で cdk の FargateService で ecs サービスを構築している。このプロパティは securityGroups のパラメーターをもっている。このパラメーターを指定しない場合、新規にセキュリティグループそのものは作成してくれるけれど、その ecs サービスへ通信するポートへのインバウンドルールは作ってくれない。
securityGroups?
Type: ISecurityGroup[] (optional, default: A new security group is created.)
The security groups to associate with the service. If you do not specify a security group, a new security group is created.
https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_ecs.FargateService.html#securitygroups
テスト環境はセキュリティグループに対して、インバウンドルールを管理画面から手動で追加していたために疎通できていた。セキュリティグループは ecs サービスに紐付いているものだから、ecs サービスを再作成しない限りはインバウンドルールが消えることもなくてこの作業漏れに気付けなかったという落ちだった。さらに、そのセキュリティグループのインバウンドルールを設定したのは私ではない。その説明欄に次のコメントが書かれていた。
atode-kesu
今すぐ消してやろうかという気持ちを抑えつつ、cdk でインバウンドルールを設定したセキュリティグループを紐付けるようにして解決した。疎通ができないと、ロードバランサーのヘルスチェックが通らず、ecs のタスクが延々と再起動を繰り返すというわかりにくい障害となっていた。1時間ぐらい唸っていた。
未検証の本番環境 前節で障害の原因自体はわかりにくいものだが、なぜサービスインの初日にこんなことが起こるのだろうか？という当然の疑問。そう。これまでこのインフラの本番環境は一切検証されていなかった。4月から5月にかけて構築されたインフラだった。この後にデータを格納するための s3 bucket がない、一部の設定はテスト環境の設定しかない、アプリケーションのコード中にテスト環境の設定がハードコードされているとか。追加であちこち直してデプロイしていた。私は本番環境に一切アクセスできないので過去にこれらの検証をすることはできなかったわけではあるけど、いろいろ思うところはあるなぁと感慨に浸っていた。</description><content>&lt;p>1時に寝て6時に起きた。暑くてあまり眠れない。今朝も雨降りで徒歩通勤。&lt;/p>
&lt;h2 id="cdk-の-ecs-サービスに紐づくセキュリティグループの設定">cdk の ECS サービスに紐づくセキュリティグループの設定&lt;/h2>
&lt;p>明日がサービスイン初日だと思っていたら、私の勘違いで今日だった。私が直近でやっている作業はアプリケーションの直接的な機能ではなく、インフラやバッチ処理などの間接的な機能を作っているわけだけど、それでも1日調整を間違えていて、あれーって感じでサービスインが始まった。とはいえ、私は本番環境にアクセスできなければ、ログすらもみれないので同僚ががんばっているのを傍から応援しつつ、平常通りタスクをこなしていくだけのはずであった。&lt;/p>
&lt;p>のほほんと通常通りのタスクをやっていたら、本番環境の ecs サービスと通信できないという連絡がくる。私が前任者から引き継いで構築したインフラなので何だろう？と調査していて、本番環境でセキュリティグループの設定が漏れていることがわかった。これはわかりにくい問題で cdk の FargateService で ecs サービスを構築している。このプロパティは securityGroups のパラメーターをもっている。このパラメーターを指定しない場合、新規にセキュリティグループそのものは作成してくれるけれど、その ecs サービスへ通信するポートへのインバウンドルールは作ってくれない。&lt;/p>
&lt;blockquote>
&lt;p>securityGroups?&lt;/p>
&lt;p>Type: ISecurityGroup[] (optional, default: A new security group is created.)&lt;/p>
&lt;p>The security groups to associate with the service.
If you do not specify a security group, a new security group is created.&lt;/p>
&lt;p>&lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_ecs.FargateService.html#securitygroups">https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_ecs.FargateService.html#securitygroups&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>テスト環境はセキュリティグループに対して、インバウンドルールを管理画面から手動で追加していたために疎通できていた。セキュリティグループは ecs サービスに紐付いているものだから、ecs サービスを再作成しない限りはインバウンドルールが消えることもなくてこの作業漏れに気付けなかったという落ちだった。さらに、そのセキュリティグループのインバウンドルールを設定したのは私ではない。その説明欄に次のコメントが書かれていた。&lt;/p>
&lt;blockquote>
&lt;p>atode-kesu&lt;/p>
&lt;/blockquote>
&lt;p>今すぐ消してやろうかという気持ちを抑えつつ、cdk でインバウンドルールを設定したセキュリティグループを紐付けるようにして解決した。疎通ができないと、ロードバランサーのヘルスチェックが通らず、ecs のタスクが延々と再起動を繰り返すというわかりにくい障害となっていた。1時間ぐらい唸っていた。&lt;/p>
&lt;h2 id="未検証の本番環境">未検証の本番環境&lt;/h2>
&lt;p>前節で障害の原因自体はわかりにくいものだが、なぜサービスインの初日にこんなことが起こるのだろうか？という当然の疑問。そう。これまでこのインフラの本番環境は一切検証されていなかった。4月から5月にかけて構築されたインフラだった。この後にデータを格納するための s3 bucket がない、一部の設定はテスト環境の設定しかない、アプリケーションのコード中にテスト環境の設定がハードコードされているとか。追加であちこち直してデプロイしていた。私は本番環境に一切アクセスできないので過去にこれらの検証をすることはできなかったわけではあるけど、いろいろ思うところはあるなぁと感慨に浸っていた。&lt;/p></content></item><item><title>m1 chip macbook と cdk の追加調査</title><link>/diary/posts/2022/0704/</link><pubDate>Mon, 04 Jul 2022 08:11:50 +0900</pubDate><guid>/diary/posts/2022/0704/</guid><description>0時に寝て7時に起きた。昨日はずっと寝てて、今朝は雨降りで徒歩通勤。週始めからしんどい。
デバッグ調査を断念 以前 m1 chip macbook で aws-lambda-python-alpha のデプロイができない ことについて書いた。同僚がそのためにデプロイできないと運用上の不都合があるので調査してみることにした。ワークアラウンドの1つとして、ビルドに使う Docker イメージを任意のものに置き換える仕組みを使えば、arm64 アーキテクチャでビルド処理のプロセス自体は通ることを確認していた。しかし、その後の python distribution が生成されていなかった。同僚にデバッグを手伝ってもらってログをみていると、python distribution をバンドルする処理のログが出ていない。
おそらく docker イメージのビルド処理ではなく、aws-cdk の core 側の bundling のところに原因がありそうに思える。core のライブラリをみると、たしかにいくつか条件次第で bundling をスキップする実装はあったし、ログが出力されていないことからも意図したステップが実行されていないことだけはわかった。その周辺から当たりをつけて aws-cdk の issues なども検索してみたけど、それっぽい issue をみつけることはできなかった。何よりも私がもっていないマシン環境の、様々な環境設定を調べることもできず、aws-cdk の core をデバッグするのも大変かなと午前中いっぱい調べて断念することに決めた。もしかしたら同僚のホスト環境に特化した問題が発生している可能性もある。
アプリケーションレベルで再現できた問題を解決できないというのは情けないけど、自分でデバッグできないものは仕方ないかと諦めることにした。本当に悔しいけれど。</description><content>&lt;p>0時に寝て7時に起きた。昨日はずっと寝てて、今朝は雨降りで徒歩通勤。週始めからしんどい。&lt;/p>
&lt;h2 id="デバッグ調査を断念">デバッグ調査を断念&lt;/h2>
&lt;p>以前 &lt;a href="/diary/diary/posts/2022/0630/#m1-chip-macbook-で-aws-lambda-python-alpha-のデプロイができない">m1 chip macbook で aws-lambda-python-alpha のデプロイができない&lt;/a> ことについて書いた。同僚がそのためにデプロイできないと運用上の不都合があるので調査してみることにした。ワークアラウンドの1つとして、ビルドに使う Docker イメージを任意のものに置き換える仕組みを使えば、arm64 アーキテクチャでビルド処理のプロセス自体は通ることを確認していた。しかし、その後の python distribution が生成されていなかった。同僚にデバッグを手伝ってもらってログをみていると、python distribution をバンドルする処理のログが出ていない。&lt;/p>
&lt;p>おそらく docker イメージのビルド処理ではなく、aws-cdk の core 側の bundling のところに原因がありそうに思える。core のライブラリをみると、たしかにいくつか条件次第で bundling をスキップする実装はあったし、ログが出力されていないことからも意図したステップが実行されていないことだけはわかった。その周辺から当たりをつけて aws-cdk の issues なども検索してみたけど、それっぽい issue をみつけることはできなかった。何よりも私がもっていないマシン環境の、様々な環境設定を調べることもできず、aws-cdk の core をデバッグするのも大変かなと午前中いっぱい調べて断念することに決めた。もしかしたら同僚のホスト環境に特化した問題が発生している可能性もある。&lt;/p>
&lt;p>アプリケーションレベルで再現できた問題を解決できないというのは情けないけど、自分でデバッグできないものは仕方ないかと諦めることにした。本当に悔しいけれど。&lt;/p></content></item><item><title>m1 chip macbook と cdk/aws-lambda は相性が悪い</title><link>/diary/posts/2022/0630/</link><pubDate>Thu, 30 Jun 2022 08:20:27 +0900</pubDate><guid>/diary/posts/2022/0630/</guid><description>0時に寝て6時に起きた。
m1 chip macbook で aws-lambda-python-alpha のデプロイができない 少し前に aws lambda の管理を serverless framework から cdk 移行した 。lambda 関数は python スクリプトで実装されているので @aws-cdk/aws-lambda-python-alpha を使っている。このライブラリでは python distribution を作るときの python インタープリターをローカルのものではなく docker イメージを使って管理しているようにみえる。私の環境 (linux, x86_64) では何の問題もなかったのだけど、同僚が m1 chip macbook を使っていて、そのマシンからだと docker イメージを使ったビルド処理でエラーが発生する。それは既知の問題で次の issue で報告されている。
(lambda-python): arm64 architecture is not respected #18696 (aws-lambda): Ability to specify CPU architecturefor building image #20907 このワークアラウンドの1つとして Custom Bundling の仕組みがある。任意の Dockerfile を指定することで任意の Docker イメージやプラットフォーム向けにビルド用の python インタープリターを設定できる。そうしたらビルド処理そのものは通るようになったけど、python distribution (python の依存関係を含めたスクリプト群) が asset として生成されない。この現象自体も cdk でよくある issue として報告されていて cdk.</description><content>&lt;p>0時に寝て6時に起きた。&lt;/p>
&lt;h2 id="m1-chip-macbook-で-aws-lambda-python-alpha-のデプロイができない">m1 chip macbook で aws-lambda-python-alpha のデプロイができない&lt;/h2>
&lt;p>少し前に aws lambda の管理を &lt;a href="/diary/diary/posts/2022/0609/#serverless-framework-から-cdk-移行の背景">serverless framework から cdk 移行した&lt;/a> 。lambda 関数は python スクリプトで実装されているので &lt;a href="https://www.npmjs.com/package/@aws-cdk/aws-lambda-python-alpha">@aws-cdk/aws-lambda-python-alpha&lt;/a> を使っている。このライブラリでは python distribution を作るときの python インタープリターをローカルのものではなく docker イメージを使って管理しているようにみえる。私の環境 (linux, x86_64) では何の問題もなかったのだけど、同僚が m1 chip macbook を使っていて、そのマシンからだと docker イメージを使ったビルド処理でエラーが発生する。それは既知の問題で次の issue で報告されている。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/18696">(lambda-python): arm64 architecture is not respected #18696&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/20907">(aws-lambda): Ability to specify CPU architecturefor building image #20907&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>このワークアラウンドの1つとして &lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-lambda-python-alpha-readme.html#custom-bundling">Custom Bundling&lt;/a> の仕組みがある。任意の Dockerfile を指定することで任意の Docker イメージやプラットフォーム向けにビルド用の python インタープリターを設定できる。そうしたらビルド処理そのものは通るようになったけど、python distribution (python の依存関係を含めたスクリプト群) が asset として生成されない。この現象自体も cdk でよくある issue として報告されていて &lt;code>cdk.out&lt;/code> を削除して再実行したら直ったという報告もいくつかあるものの、同僚のマシンではそれでは解決しなかった。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/18459">(aws-lambda-nodejs): Uploaded file must be a non-empty zip #18459&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>私が m1 chip macbook をもっていないので cdk のコードを修正して push して同僚に git pull して実行してもらうみたいな作業になっている。このデバッグはなかなか大変。&lt;/p></content></item><item><title>cdk で既存の eks クラスターを管理すべきか</title><link>/diary/posts/2022/0629/</link><pubDate>Wed, 29 Jun 2022 08:19:59 +0900</pubDate><guid>/diary/posts/2022/0629/</guid><description>0時に寝て6時に起きた。
cdk から既存の eks クラスターを制御する 1ヶ月ほど前に検証していた cdk による eks クラスターの helm 管理 を再検証した。kubectlRoleArn にどういった権限をもつ iam role を設定したらよいかがよくわからなくて苦労していた。最終的にそれが理解できて helm 管理もできるようになったのでまとめておく。
kubectlRoleArn - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually.</description><content>&lt;p>0時に寝て6時に起きた。&lt;/p>
&lt;h2 id="cdk-から既存の-eks-クラスターを制御する">cdk から既存の eks クラスターを制御する&lt;/h2>
&lt;p>1ヶ月ほど前に検証していた &lt;a href="/diary/diary/posts/2022/0518/#cdk-のパッチ検証">cdk による eks クラスターの helm 管理&lt;/a> を再検証した。kubectlRoleArn にどういった権限をもつ iam role を設定したらよいかがよくわからなくて苦労していた。最終的にそれが理解できて helm 管理もできるようになったのでまとめておく。&lt;/p>
&lt;blockquote>
&lt;p>kubectlRoleArn - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually. The trust policy of this role should include the the arn:aws::iam::${accountId}:root principal in order to allow the execution role of the kubectl resource to assume it.&lt;/p>
&lt;p>&lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters">https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>aws-auth の configmap に設定されている system:masters に所属している iam role を調べる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl describe configmap -n kube-system aws-auth
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>この iam role には &lt;code>sts:AssumeRole&lt;/code> 権限を与え、trust relationships に &lt;code>arn:aws:iam::${accountId}:root&lt;/code> といった root ユーザーを含める必要がある。この root ユーザーの設定がないと次のような権限エラーが発生する。この権限エラーの修正方法がわからなくて苦労していた。結果的には関係なかった kubectlLambdaRole の設定も必要なんじゃないかと検証していたのが前回の作業の中心だった。&lt;/p>
&lt;pre tabindex="0">&lt;code>An error occurred (AccessDenied) when calling the AssumeRole operation:
User: arn:aws:sts::${accountId}:assumed-role/xxx is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::${accountId}:role/myrole
Error: Kubernetes cluster unreachable: Get &amp;#34;https://xxx.gr7.ap-northeast-1.eks.amazonaws.com/version
&lt;/code>&lt;/pre>&lt;p>ようやく cdk で既存の eks クラスターをインポートして helm パッケージを管理できるようになった。とはいえ、cdk/cf の実行時間を測ってみると次のようになった。&lt;/p>
&lt;ul>
&lt;li>helm パッケージの新規インストール: 約5分&lt;/li>
&lt;li>helm パッケージのアンインストール: 約25分&lt;/li>
&lt;/ul>
&lt;p>これは cdk が helm パッケージを管理するための lambda 環境を構築/削除するときの時間になる。cdk はアプリケーションの stack から nested stack を作成して、そこに lambda や iam role などをまとめて作成する。一度作成してしまえば、バージョンのアップグレードは30秒ほどで完了した。&lt;/p>
&lt;p>この振る舞いを検証した上で、cdk で eks クラスターをインポートする管理はやめようとチームに提案した。正しい設定を作ってしまえば運用は楽になると言える一面もあるが、新規に helm パッケージを追加するときのちょっとした typo や設定ミスなどがあると、1回の試行に30分かかる。私がこの検証に1週間以上のデバッグ時間を割いている理由がそれに相当する。お手伝い先の運用ではテスト/本番環境ともにローカルから接続できる状態なので helm コマンドを直接実行した方が遥かに管理コストや保守コストを下げると言える。cdk を使って嬉しいことは helm コマンドでわかるバージョン情報と設定内容が cdk のコードとして管理されているぐらいでしかない。ドキュメントと helm コマンドで管理する方が現状ではよいだろうと私は結論付けた。同じような理由で eks クラスターも cdk ではなく eksctl コマンドで管理されている。&lt;/p>
&lt;p>1週間以上の労力と時間を費やしてやらない方がよいとわかったという、一般的には失敗と呼ばれる作業に終わったわけだけど、eks/cdk の勉強にはなった。&lt;/p></content></item><item><title>接待もどき</title><link>/diary/posts/2022/0617/</link><pubDate>Fri, 17 Jun 2022 19:15:58 +0900</pubDate><guid>/diary/posts/2022/0617/</guid><description>親戚と親の来訪 姪が体調悪くて病院で検査を受けるという話しで姉夫婦が車で神戸にやってきて、それに便乗して親もやってきた。朝から道案内したりとかしてた。お昼ご飯は ヒシミツ醤油 というお店に行った。週末とか前を通ると10人は並んでいる。なんでこんなに人気があるんだろう？とずっと不思議に思ってた。どうやら醤油も売っているんだけど、醤油によくあう和食の定食も食べられる飲食店らしい。ご飯が8種類あってお替り自由なのでバリエーションの広さを楽しめる。普通のランチよりちょっと贅沢な雰囲気がするので接待などにも向きそう。但し、ランチのピーク時間外さないと並ばないといけない。
その後、親戚のお土産に 亀井堂総本店のバターサンド を買ってきた。西元町にあるお店で、4年ほど前にたまたまお店の前の通りかかったきっかけで買ってみたらおいしかったのでそれからずっと印象に残っていて、4年ぶりに買ってみた。老舗のお菓子なので接待のお土産にはちょうどよさそうな気がする。たまたま親戚がきたから接待モードになってお店とお土産を開拓してた。
バッチ処理のプラットフォーム検討 昨日の続き。aws batch の機能説明や faq を読んでいたらよさそうなので触ってみた。事前に次の3つのリソースを設定しないといけない。
コンピューティング環境 ジョブキュー ジョブ定義 これらの設定をした後、ジョブというリソースを発行することでジョブ定義の処理が実行される。ecs, fargate, ec2, ec2 spot instace から環境を選べる。ec2 spot instance を使えば安くていいかと思っていたんだけど、セキュリティを考慮すると外部のよく分からんインスタンスでバッチ処理を実行するのは懸念があるなぁと思い始めてやめることにした。aws lambda の代わりに aws batch を使うのはセキュリティの懸念さえなければ悪くはないんだけど、インフラの面倒さはどっちも同じぐらいで immutable infrastructure でバッチ処理のようなものを作るのはなかなか面倒くさい。
チームメンバーと3つの選択肢について議論した。
aws batch を使う eks (k8s) を使う github actions を使う github actions の self-hosted runner に ec2 spot instance を使う記事もみかけた。これもいいかなと思ったんだけど、aws batch 同様、セキュリティの懸念は払拭できないのでダメだと断念した。
Extra CI flexibility with Github Runner on AWS Spot Instances 消去法で eks (k8s) でやることにした。CronJob を使って実装していくことになりそう。</description><content>&lt;h2 id="親戚と親の来訪">親戚と親の来訪&lt;/h2>
&lt;p>姪が体調悪くて病院で検査を受けるという話しで姉夫婦が車で神戸にやってきて、それに便乗して親もやってきた。朝から道案内したりとかしてた。お昼ご飯は &lt;a href="https://hisimitu.thebase.in/">ヒシミツ醤油&lt;/a> というお店に行った。週末とか前を通ると10人は並んでいる。なんでこんなに人気があるんだろう？とずっと不思議に思ってた。どうやら醤油も売っているんだけど、醤油によくあう和食の定食も食べられる飲食店らしい。ご飯が8種類あってお替り自由なのでバリエーションの広さを楽しめる。普通のランチよりちょっと贅沢な雰囲気がするので接待などにも向きそう。但し、ランチのピーク時間外さないと並ばないといけない。&lt;/p>
&lt;figure>&lt;img src="/diary/diary/img/2022/0617_lunch.jpg"/>
&lt;/figure>
&lt;p>その後、親戚のお土産に &lt;a href="https://www.kameido.co.jp/tonowa-index.html">亀井堂総本店のバターサンド&lt;/a> を買ってきた。西元町にあるお店で、4年ほど前にたまたまお店の前の通りかかったきっかけで買ってみたらおいしかったのでそれからずっと印象に残っていて、4年ぶりに買ってみた。老舗のお菓子なので接待のお土産にはちょうどよさそうな気がする。たまたま親戚がきたから接待モードになってお店とお土産を開拓してた。&lt;/p>
&lt;h2 id="バッチ処理のプラットフォーム検討">バッチ処理のプラットフォーム検討&lt;/h2>
&lt;p>昨日の続き。aws batch の機能説明や faq を読んでいたらよさそうなので触ってみた。事前に次の3つのリソースを設定しないといけない。&lt;/p>
&lt;ul>
&lt;li>コンピューティング環境&lt;/li>
&lt;li>ジョブキュー&lt;/li>
&lt;li>ジョブ定義&lt;/li>
&lt;/ul>
&lt;p>これらの設定をした後、ジョブというリソースを発行することでジョブ定義の処理が実行される。ecs, fargate, ec2, ec2 spot instace から環境を選べる。ec2 spot instance を使えば安くていいかと思っていたんだけど、セキュリティを考慮すると外部のよく分からんインスタンスでバッチ処理を実行するのは懸念があるなぁと思い始めてやめることにした。aws lambda の代わりに aws batch を使うのはセキュリティの懸念さえなければ悪くはないんだけど、インフラの面倒さはどっちも同じぐらいで immutable infrastructure でバッチ処理のようなものを作るのはなかなか面倒くさい。&lt;/p>
&lt;p>チームメンバーと3つの選択肢について議論した。&lt;/p>
&lt;ul>
&lt;li>aws batch を使う&lt;/li>
&lt;li>eks (k8s) を使う&lt;/li>
&lt;li>github actions を使う&lt;/li>
&lt;/ul>
&lt;p>github actions の self-hosted runner に ec2 spot instance を使う記事もみかけた。これもいいかなと思ったんだけど、aws batch 同様、セキュリティの懸念は払拭できないのでダメだと断念した。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.incredibuild.com/blog/extra-ci-flexibility-with-github-runner-on-aws-spot-instances">Extra CI flexibility with Github Runner on AWS Spot Instances&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>消去法で eks (k8s) でやることにした。&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob&lt;/a> を使って実装していくことになりそう。&lt;/p></content></item><item><title>バッチ処理のインフラ構築に着手</title><link>/diary/posts/2022/0616/</link><pubDate>Thu, 16 Jun 2022 12:23:17 +0900</pubDate><guid>/diary/posts/2022/0616/</guid><description>0時に寝て5時に起きた。なんか体調がいまいちな雰囲気。
ブログ記事のレビュー 昨日の続き。会社ブログの担当者にもレビューも通って体裁を整えて会社ブログにも転記された。あとは情シスグループのリーダーレビューが最終チェックになるみたい。
aws batch バッチ処理のインフラを構築しようと考えていて、これまで aws lambda で作られているのが非効率だなと思うようになってきた。aws batch を使えないかを検討している。いまバッチ処理のための web api のエンドポイントを実装するスタイルになっていて、この流れでやっていくと将来的にバッチ処理が api サーバーに増えていく。たまたま負荷の高いバッチ処理と通常のリクエストがスパイクするとサーバーを過負荷にして通常業務に影響を及ぼす懸念がある。ec2 を作って cron でバッチ処理動かせばいいやんという気もするけど、せっかく cdk を使っているのでフルマネージドな仕組みで構築できればカッコいいなとも思ったりする。
インフレ研究会 たまたまみかけたので fin-py のインフレ研究会に参加した。前は twitter spaces でやっていて、私はスマホに twitter アプリが入っていないから参加しにくくて微妙だった。最近は brave talk でやっているのでパソコンからも参加しやすくなった。物価、金利、中央銀行の金融政策などの話しを私はまったくわからないのでほとんど聞いているだけ。聞いているうちに世の中のお金の流れが少しずつわかってくればいいかな。国債の値段が下がれば金利が上がるみたいなそういう話しを聞いてそうなんだという感じ。</description><content>&lt;p>0時に寝て5時に起きた。なんか体調がいまいちな雰囲気。&lt;/p>
&lt;h2 id="ブログ記事のレビュー">ブログ記事のレビュー&lt;/h2>
&lt;p>昨日の続き。会社ブログの担当者にもレビューも通って体裁を整えて会社ブログにも転記された。あとは情シスグループのリーダーレビューが最終チェックになるみたい。&lt;/p>
&lt;h2 id="aws-batch">aws batch&lt;/h2>
&lt;p>バッチ処理のインフラを構築しようと考えていて、これまで aws lambda で作られているのが非効率だなと思うようになってきた。&lt;a href="https://aws.amazon.com/jp/batch/">aws batch&lt;/a> を使えないかを検討している。いまバッチ処理のための web api のエンドポイントを実装するスタイルになっていて、この流れでやっていくと将来的にバッチ処理が api サーバーに増えていく。たまたま負荷の高いバッチ処理と通常のリクエストがスパイクするとサーバーを過負荷にして通常業務に影響を及ぼす懸念がある。ec2 を作って cron でバッチ処理動かせばいいやんという気もするけど、せっかく cdk を使っているのでフルマネージドな仕組みで構築できればカッコいいなとも思ったりする。&lt;/p>
&lt;h2 id="インフレ研究会">インフレ研究会&lt;/h2>
&lt;p>たまたまみかけたので &lt;a href="https://fin-py.connpass.com/">fin-py&lt;/a> のインフレ研究会に参加した。前は twitter spaces でやっていて、私はスマホに twitter アプリが入っていないから参加しにくくて微妙だった。最近は &lt;a href="https://brave.com/ja/talk/">brave talk&lt;/a> でやっているのでパソコンからも参加しやすくなった。物価、金利、中央銀行の金融政策などの話しを私はまったくわからないのでほとんど聞いているだけ。聞いているうちに世の中のお金の流れが少しずつわかってくればいいかな。国債の値段が下がれば金利が上がるみたいなそういう話しを聞いてそうなんだという感じ。&lt;/p></content></item><item><title>コンテンツは狙ってバズらない</title><link>/diary/posts/2022/0609/</link><pubDate>Thu, 09 Jun 2022 13:02:23 +0900</pubDate><guid>/diary/posts/2022/0609/</guid><description>1時に寝て8時に起きた。昨晩はたくさん話してテンション上がって眠れなくてバテ気味。
serverless framework から cdk 移行の背景 木曜日はスプリントレビューがある。ステークホルダーが出席する唯一のスプリントイベントなので、大半はステークホルダーとの情報共有や質疑応答、プロジェクトにとっての大きい括りでの現状の共有になる。大半はお手伝い先の社員さん同士のやり取りで、協力会社の開発者は詳細が必要になったときだけ説明するといったイベント。前スプリントで 既存の lambda 関数を serverless framework から cdk へ移行 した。プロジェクトメンバーではないのだけど、業務のリーダークラスの方が cdk に関心をもって質問してくれた。聞くところによると、他プロジェクトでも cdk を使うようになってきているらしく、なぜいま移行しているのか？という質問だった。普段インフラ作業を孤独にやっているのもあって、業務の人が関心を示してくれたのが嬉しくて、変なスイッチが入っていろいろ説明した。serverless framework は2015年10月リリース、cdk は2018年7月リリースで、歴史的に serverless framework が普及して、その後に cdk が台頭してきたので実績や機能性から serverless framework が広く利用されている。但し、いま aws のインフラ管理をするのであれば、cdk は serverless framework 以上の管理ができることから cdk に一元管理した方がツールの学習コストを減らし、保守コストを下げることにつながるといった話しを丁寧に説明した。相手がそこまでの回答を求めていたかはわからないけど、関心を示してくれたことそのもので私が救われた気がした。
terapyon channel のコンテンツ公開 昨日の今日で公開された。ほとんど無編集だったのかな。web サイトのコンテンツ紹介の内容も手伝って夕方には公開された。
Podcast terapyon channel 「#58 もりもとさんの年1ゲスト会 マイクロ法人と開発ワークフローのカイゼン話ほか」を共有 https://t.co/aQfbuA6XrO #terapyon_channel
&amp;mdash; terapyon (@terapyon) June 9, 2022 私の中ではいろんな話題を楽しく話せて充実感があった。一部にだけ関心をもつ人にも聞きたいところだけ聞けていいんじゃないかと思えた。基本的に自分の podcast を聞き直すことはないんだけど、今回は自分が充実感をもって話せたせいか、2回ほど聞き直しておもしろい話だなぁと自画自賛してた (笑) 。自分がよいものは周りもそう思うはずだと、ついつい先入観で考えがちだけど、全然そうじゃなかった。全くいいねがつかなかったので周りからみたら私の自己満足のコンテンツでしかなかった (笑) 。コンテンツあるあるだけど、狙ってコンテンツをバズらせるのは難しい。ブログでもがんばって書いた記事が全く読まれないことはあるし、手間暇かけずに軽く書いた記事がめっちゃバズったこともある。コンテンツがバズるかどうかは、最初にみた人たちが拡散するかにも依ってくる。いずれにしても、他者が関心をもつようなコンテンツかどうかは本人ではわからないというのは確かかな。
今期から会社のマーケティングも少しずつやっていく予定になる。自分がよいと思ったコンテンツに全く関心を示されないことは今後も多々あるだろう。作ったコンテンツを多くの人に見聞きしてもらおうと思ったらやることは1つだけで、当たるまでひたすら作り続けて、いつか当たるのを気長に待つという戦略しか、いまのところ思いつかない。</description><content>&lt;p>1時に寝て8時に起きた。昨晩はたくさん話してテンション上がって眠れなくてバテ気味。&lt;/p>
&lt;h2 id="serverless-framework-から-cdk-移行の背景">serverless framework から cdk 移行の背景&lt;/h2>
&lt;p>木曜日はスプリントレビューがある。ステークホルダーが出席する唯一のスプリントイベントなので、大半はステークホルダーとの情報共有や質疑応答、プロジェクトにとっての大きい括りでの現状の共有になる。大半はお手伝い先の社員さん同士のやり取りで、協力会社の開発者は詳細が必要になったときだけ説明するといったイベント。前スプリントで &lt;a href="/diary/diary/posts/2022/0601/#mvpminimum-viable-productで対応した">既存の lambda 関数を serverless framework から cdk へ移行&lt;/a> した。プロジェクトメンバーではないのだけど、業務のリーダークラスの方が cdk に関心をもって質問してくれた。聞くところによると、他プロジェクトでも cdk を使うようになってきているらしく、なぜいま移行しているのか？という質問だった。普段インフラ作業を孤独にやっているのもあって、業務の人が関心を示してくれたのが嬉しくて、変なスイッチが入っていろいろ説明した。serverless framework は2015年10月リリース、cdk は2018年7月リリースで、歴史的に serverless framework が普及して、その後に cdk が台頭してきたので実績や機能性から serverless framework が広く利用されている。但し、いま aws のインフラ管理をするのであれば、cdk は serverless framework 以上の管理ができることから cdk に一元管理した方がツールの学習コストを減らし、保守コストを下げることにつながるといった話しを丁寧に説明した。相手がそこまでの回答を求めていたかはわからないけど、関心を示してくれたことそのもので私が救われた気がした。&lt;/p>
&lt;h2 id="terapyon-channel-のコンテンツ公開">terapyon channel のコンテンツ公開&lt;/h2>
&lt;p>昨日の今日で公開された。ほとんど無編集だったのかな。web サイトのコンテンツ紹介の内容も手伝って夕方には公開された。&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="ja" dir="ltr">Podcast terapyon channel 「#58 もりもとさんの年1ゲスト会 マイクロ法人と開発ワークフローのカイゼン話ほか」を共有 &lt;a href="https://t.co/aQfbuA6XrO">https://t.co/aQfbuA6XrO&lt;/a> &lt;a href="https://twitter.com/hashtag/terapyon_channel?src=hash&amp;amp;ref_src=twsrc%5Etfw">#terapyon_channel&lt;/a>&lt;/p>&amp;mdash; terapyon (@terapyon) &lt;a href="https://twitter.com/terapyon/status/1534817292080451584?ref_src=twsrc%5Etfw">June 9, 2022&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>私の中ではいろんな話題を楽しく話せて充実感があった。一部にだけ関心をもつ人にも聞きたいところだけ聞けていいんじゃないかと思えた。基本的に自分の podcast を聞き直すことはないんだけど、今回は自分が充実感をもって話せたせいか、2回ほど聞き直しておもしろい話だなぁと自画自賛してた (笑) 。自分がよいものは周りもそう思うはずだと、ついつい先入観で考えがちだけど、全然そうじゃなかった。全くいいねがつかなかったので周りからみたら私の自己満足のコンテンツでしかなかった (笑) 。コンテンツあるあるだけど、狙ってコンテンツをバズらせるのは難しい。ブログでもがんばって書いた記事が全く読まれないことはあるし、手間暇かけずに軽く書いた記事がめっちゃバズったこともある。コンテンツがバズるかどうかは、最初にみた人たちが拡散するかにも依ってくる。いずれにしても、他者が関心をもつようなコンテンツかどうかは本人ではわからないというのは確かかな。&lt;/p>
&lt;p>今期から会社のマーケティングも少しずつやっていく予定になる。自分がよいと思ったコンテンツに全く関心を示されないことは今後も多々あるだろう。作ったコンテンツを多くの人に見聞きしてもらおうと思ったらやることは1つだけで、当たるまでひたすら作り続けて、いつか当たるのを気長に待つという戦略しか、いまのところ思いつかない。&lt;/p></content></item><item><title>aws sns を介した pubsub</title><link>/diary/posts/2022/0606/</link><pubDate>Mon, 06 Jun 2022 04:43:44 +0900</pubDate><guid>/diary/posts/2022/0606/</guid><description>0時に寝て4時半に起きた。21時頃からオフィスで作業してたらそのまま寝落ちした。朝の掃除機をかける音で目覚めて、始業までワーケーションのふりかえりをしていた。
lambda 関数と sns の連携 定期実行で数百程度の web api 呼び出しを行いたい。これまで定期実行を lambda 関数で実装してきたが、負荷分散を考慮して作ってほしいと言われたので sns を使ってメッセージ分割した上で1つ1つの lambda 関数は sns のメッセージを受け取って実行されるように構成した。lambda 関数を使って pubsub するときは sns を使えばよいらしい。sns はあまり使ったことがないので私自身ノウハウをもっていないし、運用の勘所もよくわかっていない。ドキュメントをいくつか読みながら cdk のコードを書いてた。producer と consumer を lambda 関数で作成し、sns を介してリクエストの負荷分散を図る。lambda 関数は同時並行数を設定できるのでこれがスロットル制限のような役割にもなる。インフラやスクリプトのコードはすぐに実装できたが、lambda 関数の destroy にやたら時間がかかる。権限周りでいくつか設定を試すために destroy しながら検証をしたかった。destroy するのに20分はかかるので deploy や設定の手作業などをやっていると1つの設定を試すのに平気で1時間ぐらいかかってしまう。3回ぐらいやって疲れて検証作業はやや妥協した。
Amazon SNS でのAWS Lambdaの使用 Managing Lambda reserved concurrency Amazon SNS security best practices</description><content>&lt;p>0時に寝て4時半に起きた。21時頃からオフィスで作業してたらそのまま寝落ちした。朝の掃除機をかける音で目覚めて、始業までワーケーションのふりかえりをしていた。&lt;/p>
&lt;h2 id="lambda-関数と-sns-の連携">lambda 関数と sns の連携&lt;/h2>
&lt;p>定期実行で数百程度の web api 呼び出しを行いたい。これまで定期実行を lambda 関数で実装してきたが、負荷分散を考慮して作ってほしいと言われたので sns を使ってメッセージ分割した上で1つ1つの lambda 関数は sns のメッセージを受け取って実行されるように構成した。lambda 関数を使って pubsub するときは sns を使えばよいらしい。sns はあまり使ったことがないので私自身ノウハウをもっていないし、運用の勘所もよくわかっていない。ドキュメントをいくつか読みながら cdk のコードを書いてた。producer と consumer を lambda 関数で作成し、sns を介してリクエストの負荷分散を図る。lambda 関数は同時並行数を設定できるのでこれがスロットル制限のような役割にもなる。インフラやスクリプトのコードはすぐに実装できたが、lambda 関数の destroy にやたら時間がかかる。権限周りでいくつか設定を試すために destroy しながら検証をしたかった。destroy するのに20分はかかるので deploy や設定の手作業などをやっていると1つの設定を試すのに平気で1時間ぐらいかかってしまう。3回ぐらいやって疲れて検証作業はやや妥協した。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.aws.amazon.com/ja_jp/lambda/latest/dg/with-sns.html">Amazon SNS でのAWS Lambdaの使用&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html">Managing Lambda reserved concurrency&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/sns/latest/dg/sns-security-best-practices.html">Amazon SNS security best practices&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>draw.io で描いたインフラ構成図</title><link>/diary/posts/2022/0602/</link><pubDate>Thu, 02 Jun 2022 08:51:06 +0900</pubDate><guid>/diary/posts/2022/0602/</guid><description>0時に寝て4時に起きて7時までだらだらしてた。なんか調子悪い。
draw.io を描いてみた 先日、draw.io で aws 構成図を描く調査 をした。割り込みの作業をやっていてシステム構成図の作成を先延ばししていた。だいたいの調査は終わっていたのであとは根を詰めて描くだけ。次のサンプル構成図をみながら同じように描いていく。
AWS のアーキテクチャ図を描きたい ! でもどうすれば良いの ? 2つの環境があって、そのうちの1つを作成した。新規構築した環境でスクラッチから描いたものの、インフラリソースの構成要素が少なかったのでサンプル構成図を参考にしながらすぐに描けた。半角スペースで文字位置を調整したりすると、github 上で svg 表示したときに文字の位置がずれたりするのでそういうやり方はダメだとわかった。あと draw.io の振る舞いなのか、vscode のプラグインのせいなのかわからないけど、オブジェクトの配置の前後関係をうまく調整できなくてコピペし直したり、なにかの操作をしたタイミングでインフラリソースのアイコンが後ろに隠蔽されていたりもした。リソース間の接続のための線も自動的に繋がるときもあって便利なのだが、誤動作して変な位置にレイアウトされることもあって制御が難しい。私の感覚では、多少の利便性のために自動化されるよりも、自分で思い通りに制御出来る方を好む。draw.io の自動調整機能の制御が難しいなと思った。</description><content>&lt;p>0時に寝て4時に起きて7時までだらだらしてた。なんか調子悪い。&lt;/p>
&lt;h2 id="drawio-を描いてみた">draw.io を描いてみた&lt;/h2>
&lt;p>先日、&lt;a href="/diary/diary/posts/2022/0523/">draw.io で aws 構成図を描く調査&lt;/a> をした。割り込みの作業をやっていてシステム構成図の作成を先延ばししていた。だいたいの調査は終わっていたのであとは根を詰めて描くだけ。次のサンプル構成図をみながら同じように描いていく。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aws.amazon.com/jp/builders-flash/202204/way-to-draw-architecture/">AWS のアーキテクチャ図を描きたい ! でもどうすれば良いの ?&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>2つの環境があって、そのうちの1つを作成した。新規構築した環境でスクラッチから描いたものの、インフラリソースの構成要素が少なかったのでサンプル構成図を参考にしながらすぐに描けた。半角スペースで文字位置を調整したりすると、github 上で svg 表示したときに文字の位置がずれたりするのでそういうやり方はダメだとわかった。あと draw.io の振る舞いなのか、vscode のプラグインのせいなのかわからないけど、オブジェクトの配置の前後関係をうまく調整できなくてコピペし直したり、なにかの操作をしたタイミングでインフラリソースのアイコンが後ろに隠蔽されていたりもした。リソース間の接続のための線も自動的に繋がるときもあって便利なのだが、誤動作して変な位置にレイアウトされることもあって制御が難しい。私の感覚では、多少の利便性のために自動化されるよりも、自分で思い通りに制御出来る方を好む。draw.io の自動調整機能の制御が難しいなと思った。&lt;/p></content></item><item><title>aws サービスと ipv6</title><link>/diary/posts/2022/0530/</link><pubDate>Mon, 30 May 2022 08:22:52 +0900</pubDate><guid>/diary/posts/2022/0530/</guid><description>0時に寝て6時頃に起きて7時半に起きた。
cloudfront と waf と s3 と ipv6 と aws サービスについて ipv6 について調べると、少しずつ対応してきた経緯があって、ipv4 と ipv6 の両対応のことを aws は dualstack と読んでいる。エンドポイントのサブドメインに dualstack があれば、ipv6 対応していると考えてよいと思う。リモートワークしている開発者がテスト環境に接続するときに、その開発者の IP アドレスを登録する aws lambda 関数がある。その lambda 関数にリクスとすると、リクエストしたクライアントの IP アドレスを waf の IP sets に登録することで、パケットフィルタリングのルールを更新している。たまたま、そのインフラを cdk 移行したところ、ある開発者は ipv6 で登録しようとしてエラーになるということがわかった。移行のときに api gateway を使わずに直接 lambda の fuction url を使うようにしたところ、lambda は ipv6 対応しているのでそのまま ipv6 の IP アドレスを登録しようとして waf の設定が ipv4 しか対応していなかったためにエラーになっていた。
AWS Lambda がインバウンド接続用のインターネットプロトコルバージョン 6 (IPv6) エンドポイントのサポートを開始 cloudfront, waf, s3 の ipv6 対応は2016年頃に対応していて、この3つのサービスに対して一緒にブログ記事を書いているのは、これらのサービスを一緒に使うことを想定しているとみなすべきだろう。</description><content>&lt;p>0時に寝て6時頃に起きて7時半に起きた。&lt;/p>
&lt;h2 id="cloudfront-と-waf-と-s3-と-ipv6-と">cloudfront と waf と s3 と ipv6 と&lt;/h2>
&lt;p>aws サービスについて ipv6 について調べると、少しずつ対応してきた経緯があって、ipv4 と ipv6 の両対応のことを aws は dualstack と読んでいる。エンドポイントのサブドメインに dualstack があれば、ipv6 対応していると考えてよいと思う。リモートワークしている開発者がテスト環境に接続するときに、その開発者の IP アドレスを登録する aws lambda 関数がある。その lambda 関数にリクスとすると、リクエストしたクライアントの IP アドレスを waf の IP sets に登録することで、パケットフィルタリングのルールを更新している。たまたま、そのインフラを cdk 移行したところ、ある開発者は ipv6 で登録しようとしてエラーになるということがわかった。移行のときに api gateway を使わずに直接 lambda の fuction url を使うようにしたところ、lambda は ipv6 対応しているのでそのまま ipv6 の IP アドレスを登録しようとして waf の設定が ipv4 しか対応していなかったためにエラーになっていた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aws.amazon.com/jp/about-aws/whats-new/2021/12/aws-lambda-ipv6-endpoints-inbound-connections/">AWS Lambda がインバウンド接続用のインターネットプロトコルバージョン 6 (IPv6) エンドポイントのサポートを開始&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>cloudfront, waf, s3 の ipv6 対応は2016年頃に対応していて、この3つのサービスに対して一緒にブログ記事を書いているのは、これらのサービスを一緒に使うことを想定しているとみなすべきだろう。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aws.amazon.com/jp/blogs/news/ipv6-support-update-cloudfront-waf-and-s3-transfer-acceleration/">IPv6 サポートの更新 – CloudFront、WAF、S3 Transfer Acceleration&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>cdk のコードで cloudfront, waf は ipv6 対応したのだけど、cloudfront のオリジンに当たる s3 の ipv6 対応を cdk からどうやって設定していいかわからなかった。&lt;a href="https://github.com/aws/aws-cdk/tree/master/packages/%40aws-cdk/aws-cloudfront-origins">aws-cloudfront-origins&lt;/a> の &lt;code>S3Origin&lt;/code> のコードを読むと、どうも s3 の ipv6 対応のエンドポイント (dualstack) を考慮して制御しているようにはみえない。バグではないけど、設定方法がわからないので feature request として issue 登録してみた。aws-cdk に issue 登録するのは初めて。余談だけど、&lt;a href="https://github.com/aws/aws-cdk/tree/master/.github/ISSUE_TEMPLATE">ISSUE_TEMPLATE&lt;/a> もよく出来ていて、これを参考にして自分たちのリポジトリにも取り入れてもよさそう。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/20550">(aws-cloudfront-origins): Support dualstack domain name (ipv6) for S3 origin #20550&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>severless framework vs cdk</title><link>/diary/posts/2022/0525/</link><pubDate>Wed, 25 May 2022 08:11:17 +0900</pubDate><guid>/diary/posts/2022/0525/</guid><description>22時に寝て1時に起きて5時半に起きた。前日はあまり寝てなかったので知らないうちに寝てしまった。
severless framework と cdk の比較 serverless framework というツールがある。マルチクラウド対応で aws で言えば lambda のような、サーバーレスのアプリケーションを簡単にデプロイするためのツール。よく使われているようで、私がお手伝いしている会社でも lambda 関数のデプロイにこのツールを使っている。いろいろ調べていたら、serverless framework は cdk と真正面から競合になるツールで、cdk をすでに使っているなら cdk に一元管理した方が保守コストが下がっていいだろうと考え、新たに lambda 関数を作る作業を cdk でやってみた。私はほとんど serverless framework を使ったことがないので正しく理解できていない可能性は高いが、ざっくり比較すると次になる。
serverless framework メリット
cdk より学習コストが低い yaml 設定だけで簡単にデプロイできる デメリット
リソース管理のための s3 バケットを必要とする lambda に関連するリソースしかデプロイできない 開発者が明示的に設定しなくても裏でリソースを勝手に生成するので意図せず適切に管理されていないインフラを作ってしまう懸念がある 大半の実務レベルのアプリケーションでは cf テンプレートの dsl を yaml に設定する必要があり、設定が煩雑になったり見通しが悪くなる cdk メリット
任意の aws インフラのリソースを管理できる プログラミング言語で記述できるので動的なリソースの依存関係を定義できる cf は裏に隠蔽されているが、serverless framework のような dsl を記述する必要はない デメリット
学習コストが高い リファレンス
AWS CDK vs Serverless Framework Serverless Framework vs SAM vs AWS CDK 結論から言うと、将来的に cdk を使うならまず cdk を使った方がよい。本当にシンプルな要件で lambda 関数のインフラしか扱わないなら serverless framework でもいいかもしれない。serverless framework は cdk がない時代に作られたツールだろうからいまから新規に導入する場合は、多少の学習コストを払っても cdk を学んでおけば、将来的に役に立つ場面が多いと思う。</description><content>&lt;p>22時に寝て1時に起きて5時半に起きた。前日はあまり寝てなかったので知らないうちに寝てしまった。&lt;/p>
&lt;h2 id="severless-framework-と-cdk-の比較">severless framework と cdk の比較&lt;/h2>
&lt;p>&lt;a href="https://www.serverless.com/">serverless framework&lt;/a> というツールがある。マルチクラウド対応で aws で言えば lambda のような、サーバーレスのアプリケーションを簡単にデプロイするためのツール。よく使われているようで、私がお手伝いしている会社でも lambda 関数のデプロイにこのツールを使っている。いろいろ調べていたら、serverless framework は cdk と真正面から競合になるツールで、cdk をすでに使っているなら cdk に一元管理した方が保守コストが下がっていいだろうと考え、新たに lambda 関数を作る作業を cdk でやってみた。私はほとんど serverless framework を使ったことがないので正しく理解できていない可能性は高いが、ざっくり比較すると次になる。&lt;/p>
&lt;h4 id="serverless-framework">serverless framework&lt;/h4>
&lt;p>メリット&lt;/p>
&lt;ul>
&lt;li>cdk より学習コストが低い&lt;/li>
&lt;li>yaml 設定だけで簡単にデプロイできる&lt;/li>
&lt;/ul>
&lt;p>デメリット&lt;/p>
&lt;ul>
&lt;li>リソース管理のための s3 バケットを必要とする&lt;/li>
&lt;li>lambda に関連するリソースしかデプロイできない&lt;/li>
&lt;li>開発者が明示的に設定しなくても裏でリソースを勝手に生成するので意図せず適切に管理されていないインフラを作ってしまう懸念がある&lt;/li>
&lt;li>大半の実務レベルのアプリケーションでは cf テンプレートの dsl を yaml に設定する必要があり、設定が煩雑になったり見通しが悪くなる&lt;/li>
&lt;/ul>
&lt;h4 id="cdk">cdk&lt;/h4>
&lt;p>メリット&lt;/p>
&lt;ul>
&lt;li>任意の aws インフラのリソースを管理できる&lt;/li>
&lt;li>プログラミング言語で記述できるので動的なリソースの依存関係を定義できる&lt;/li>
&lt;li>cf は裏に隠蔽されているが、serverless framework のような dsl を記述する必要はない&lt;/li>
&lt;/ul>
&lt;p>デメリット&lt;/p>
&lt;ul>
&lt;li>学習コストが高い&lt;/li>
&lt;/ul>
&lt;p>リファレンス&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.secjuice.com/aws-cdk-vs-serverless-framework/">AWS CDK vs Serverless Framework&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://dev.to/tastefulelk/serverless-framework-vs-sam-vs-aws-cdk-1g9g">Serverless Framework vs SAM vs AWS CDK&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>結論から言うと、将来的に cdk を使うならまず cdk を使った方がよい。本当にシンプルな要件で lambda 関数のインフラしか扱わないなら serverless framework でもいいかもしれない。serverless framework は cdk がない時代に作られたツールだろうからいまから新規に導入する場合は、多少の学習コストを払っても cdk を学んでおけば、将来的に役に立つ場面が多いと思う。&lt;/p></content></item><item><title>ドキュメントをちゃんと書く</title><link>/diary/posts/2022/0519/</link><pubDate>Thu, 19 May 2022 13:33:49 +0900</pubDate><guid>/diary/posts/2022/0519/</guid><description>20時に寝て22時に起きて、それから作業して3時に寝て6時に起きた。
インフラのドキュメント作成 今日からインフラのドキュメント作成に着手した。4月から1ヶ月以上に渡って インフラエンジニア のようなお仕事をしていた。具体的には新しい環境のインフラ構築、ならびに既存インフラのリファクタリングというよりは再構築といった作業をしていた。約1ヶ月で大きなインフラのタスクは完了して、その後もこれまで cdk 管理していなかったインフラリソースの管理なども含め、より再現可能な管理されたインフラとなるように改善してきた。それもだいたい終えてきたので、そろそろ他の開発者にも引き継げるようにドキュメントを書くことにした。私以外は若い開発者が多いせいか、cdk/cf の知識というよりもインフラそのもののやネットワークの知識が少ないメンバーが多い。そういった運用経験の浅い開発者にも適切な教育が行えるよう、ドキュメントやチュートリアルなどを書いていく。数日ぐらいかけてしっかり書いてから勉強会を開催する。それをもって引き継ぎしていくかなぁ。
私が前任者から引き継いだ README に helm の説明が次のように書かれてた。
まず helm がわかってない人はググってくること。
こんな README を私はみたことなくて書いている人が訳分からず作業しているんだなという印象を受けた。私が書くドキュメントには cdk とは何か？から説明している。もちろん aws のドキュメントをすべて読めばよいのだが、それはコストがかかる。私の経験と私が理解した cdk の概念を簡潔に、なるべく自分たちの業務にとって大事なことを要約して書くことに意義があると私は考えている。README にググれみたいなことを書いて誰もなにも言わない開発文化を改善していきたい。</description><content>&lt;p>20時に寝て22時に起きて、それから作業して3時に寝て6時に起きた。&lt;/p>
&lt;h2 id="インフラのドキュメント作成">インフラのドキュメント作成&lt;/h2>
&lt;p>今日からインフラのドキュメント作成に着手した。4月から1ヶ月以上に渡って &lt;a href="/diary/diary/posts/2022/0405/">インフラエンジニア&lt;/a> のようなお仕事をしていた。具体的には新しい環境のインフラ構築、ならびに既存インフラのリファクタリングというよりは再構築といった作業をしていた。約1ヶ月で大きなインフラのタスクは完了して、その後もこれまで cdk 管理していなかったインフラリソースの管理なども含め、より再現可能な管理されたインフラとなるように改善してきた。それもだいたい終えてきたので、そろそろ他の開発者にも引き継げるようにドキュメントを書くことにした。私以外は若い開発者が多いせいか、cdk/cf の知識というよりもインフラそのもののやネットワークの知識が少ないメンバーが多い。そういった運用経験の浅い開発者にも適切な教育が行えるよう、ドキュメントやチュートリアルなどを書いていく。数日ぐらいかけてしっかり書いてから勉強会を開催する。それをもって引き継ぎしていくかなぁ。&lt;/p>
&lt;p>私が前任者から引き継いだ README に helm の説明が次のように書かれてた。&lt;/p>
&lt;blockquote>
&lt;p>まず helm がわかってない人はググってくること。&lt;/p>
&lt;/blockquote>
&lt;p>こんな README を私はみたことなくて書いている人が訳分からず作業しているんだなという印象を受けた。私が書くドキュメントには cdk とは何か？から説明している。もちろん aws のドキュメントをすべて読めばよいのだが、それはコストがかかる。私の経験と私が理解した cdk の概念を簡潔に、なるべく自分たちの業務にとって大事なことを要約して書くことに意義があると私は考えている。README にググれみたいなことを書いて誰もなにも言わない開発文化を改善していきたい。&lt;/p></content></item><item><title>cdk のビルドが難しい話し</title><link>/diary/posts/2022/0518/</link><pubDate>Wed, 18 May 2022 09:55:29 +0900</pubDate><guid>/diary/posts/2022/0518/</guid><description>0時に寝て6時半に起きた。暑くなってきたせいかバテ気味。水曜日は本番リリースの日。3つほど本場環境のインフラ移行作業があったので社員さんの実作業をリモートから指示しながらサポートしていた。私に本番環境の操作権限があれば、この工数はほぼ半分に削減できる。
cdk のパッチ検証 先日 cdk による eks クラスターの helm 管理の調査中断 について書いた。バグっているから適切な設定が反映されないという話しで一時中断していたんだけど、そのときにクラスメソッドの担当者さんとも相談していた。そしたら、その担当者さんが問題を修正して pr を送ってくれた。感謝。
fix(eks): Cluster.FromClusterAttributes ignores KubectlLambdaRole #20373 そこまでやってくれると思ってなかったのですごいなぁと感心しながら、せっかくなのでパッチを当てた cdk で検証してみようと、Contributing to the AWS Cloud Development Kit をみながらローカルでのビルドを試みた。ビルド自体はできたんだけど、パッケージを作るのがうまくいかなくて、cdk はツール自体が大きいので実行時間がかかる。だいたいビルドやパッケージングのそれぞれに20-30分ぐらいかかる。エラーの原因がよく分からなくて面倒になって断念した。私が javascript のパッケージングに疎いせいもあると思うけど、ドキュメントに書いてある通りにうまくいかなかったので早々に諦めた。</description><content>&lt;p>0時に寝て6時半に起きた。暑くなってきたせいかバテ気味。水曜日は本番リリースの日。3つほど本場環境のインフラ移行作業があったので社員さんの実作業をリモートから指示しながらサポートしていた。私に本番環境の操作権限があれば、この工数はほぼ半分に削減できる。&lt;/p>
&lt;h2 id="cdk-のパッチ検証">cdk のパッチ検証&lt;/h2>
&lt;p>先日 &lt;a href="/diary/diary/posts/2022/0516/#eks-クラスターの-helm-管理の調査">cdk による eks クラスターの helm 管理の調査中断&lt;/a> について書いた。バグっているから適切な設定が反映されないという話しで一時中断していたんだけど、そのときにクラスメソッドの担当者さんとも相談していた。そしたら、その担当者さんが問題を修正して pr を送ってくれた。感謝。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/pull/20373">fix(eks): Cluster.FromClusterAttributes ignores KubectlLambdaRole #20373&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>そこまでやってくれると思ってなかったのですごいなぁと感心しながら、せっかくなのでパッチを当てた cdk で検証してみようと、&lt;a href="https://github.com/aws/aws-cdk/blob/master/CONTRIBUTING.md">Contributing to the AWS Cloud Development Kit&lt;/a> をみながらローカルでのビルドを試みた。ビルド自体はできたんだけど、パッケージを作るのがうまくいかなくて、cdk はツール自体が大きいので実行時間がかかる。だいたいビルドやパッケージングのそれぞれに20-30分ぐらいかかる。エラーの原因がよく分からなくて面倒になって断念した。私が javascript のパッケージングに疎いせいもあると思うけど、ドキュメントに書いてある通りにうまくいかなかったので早々に諦めた。&lt;/p></content></item><item><title>helm 調査の一時中断</title><link>/diary/posts/2022/0516/</link><pubDate>Mon, 16 May 2022 08:31:44 +0900</pubDate><guid>/diary/posts/2022/0516/</guid><description>0時に寝て6時半に起きた。
eks クラスターの helm 管理の調査 先週から調査 していて、調査結果から kubectlRoleArn を生成してデプロイを実行してみた。以前発生していた権限エラーは解消したものの、lambda 内からの kubectl と k8s クラスターの認証に失敗する。もう1つ kubectlLambdaRole という設定もあるので、ここに system:masters 権限をもつ iam ロールを設定してみたものの、エラー結果は変わらない。お手上げかなと思ってたら aws-eks: Cluster.FromClusterAttributes ignores KubectlLambdaRole #20008 という issue があって、まさにいま起こっている現象と合致するのでこのせいかもしれない。cdk のバグならそれが解消しないと検証を進められないため、この調査は一旦打ち切って、cdk のバグが修正されて新しいバージョンがリリースされてから再試行することに決めた。
backlog のいろいろ たまたまタイムラインでスライドをみかけて、backlog の開発をしている方の記事をみつけた。scala と play framework で実装されているらしい。もう10年以上開発しているプロダクトなのかな。それ自体はすごいなぁと感心しながらスライドを眺めてた。
ヌーラボ入社後に書いたブログ記事＆プレゼン資料まとめ</description><content>&lt;p>0時に寝て6時半に起きた。&lt;/p>
&lt;h2 id="eks-クラスターの-helm-管理の調査">eks クラスターの helm 管理の調査&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2022/0513/#eks-クラスターの-helm-管理の調査">先週から調査&lt;/a> していて、調査結果から &lt;code>kubectlRoleArn&lt;/code> を生成してデプロイを実行してみた。以前発生していた権限エラーは解消したものの、lambda 内からの kubectl と k8s クラスターの認証に失敗する。もう1つ &lt;code>kubectlLambdaRole&lt;/code> という設定もあるので、ここに &lt;code>system:masters&lt;/code> 権限をもつ iam ロールを設定してみたものの、エラー結果は変わらない。お手上げかなと思ってたら &lt;a href="https://github.com/aws/aws-cdk/issues/20008">aws-eks: Cluster.FromClusterAttributes ignores KubectlLambdaRole #20008&lt;/a> という issue があって、まさにいま起こっている現象と合致するのでこのせいかもしれない。cdk のバグならそれが解消しないと検証を進められないため、この調査は一旦打ち切って、cdk のバグが修正されて新しいバージョンがリリースされてから再試行することに決めた。&lt;/p>
&lt;h2 id="backlog-のいろいろ">backlog のいろいろ&lt;/h2>
&lt;p>たまたまタイムラインでスライドをみかけて、backlog の開発をしている方の記事をみつけた。scala と play framework で実装されているらしい。もう10年以上開発しているプロダクトなのかな。それ自体はすごいなぁと感心しながらスライドを眺めてた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://muziyoshiz.hatenablog.com/entry/2021/08/28/154859">ヌーラボ入社後に書いたブログ記事＆プレゼン資料まとめ&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>マーケティング施策の取り組み開始</title><link>/diary/posts/2022/0513/</link><pubDate>Fri, 13 May 2022 08:42:18 +0900</pubDate><guid>/diary/posts/2022/0513/</guid><description>23時に寝て1時に起きて漫画を読んで6時に起きて漫画読んでた。
隔週の雑談 顧問のはらさんと隔週の打ち合わせ。今日の議題は 先日作成した第4期の展望 について雑談した。あまり深く考えずに起業して初期の頃に作った10ヶ年計画に対してわりとその通りに推移している。来期ぐらいで業務委託のお仕事は終える予定。来期か再来期ぐらいからプロダクト開発の期間に入る。自社プロダクトを作る前から徐々にマーケティングもしていかないといけない。そのため、今期からマーケティング施策を少しずつ増やしていて、まずは会社の信頼度を上げるところからやっていく。売上を上げるためのマーケティングではなく信頼度を上げるためのマーケティングを行う。金森氏が言うようにどんなに優れたプロダクトを作ったとしても売れるかどうかは別問題だ。
eks クラスターの helm 管理の調査 昨日の続き。権限設定がなんもわからんみたいな様相になったので調査のやり方を変えることにした。検証用の eks クラスターを cdk から新規に作成して helm をインストールするときのリソースや権限設定がどうなるのかを調査した。lambda 関数が5個ぐらい、ロールは10個ぐらい作成された。lambda の生成に時間がかかるのか？新規作成するのに25分、削除するときは30分ぐらいかかった。rds もそうだけど、eks のような複雑なインフラを cdk で管理すると実行するのにけっこう時間がかかる。設定が難しくなければ別によいけど、eks のような権限やリソースの設定が複雑なインフラはトライ&amp;amp;エラーで何度も実行する必要があるから、こういうものは cdk で管理するのには向かないインフラだと言えると思う。一定の設定のプラクティスが溜まるまでは eks は cdk で管理しない方がよいかもしれない。
CreationRole というのが設定されて trust relationships に次のような設定が追加される。
{ &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Principal&amp;#34;: { &amp;#34;AWS&amp;#34;: &amp;#34;arn:aws:iam::${accountId}:root&amp;#34; }, &amp;#34;Action&amp;#34;: &amp;#34;sts:AssumeRole&amp;#34; } ] } このロールに含まれるカスタムポリシーには次のような設定がある。たぶんこんな感じのロールを新規に作成して kubectlRoleArn として指定してやればいいんじゃないかと思う。
{ &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Action&amp;#34;: &amp;#34;iam:PassRole&amp;#34;, &amp;#34;Resource&amp;#34;: &amp;#34;arn:aws:iam::${accountId}:role/${EksClusterIamRole}&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: [ &amp;#34;eks:CreateCluster&amp;#34;, &amp;#34;eks:DescribeCluster&amp;#34;, &amp;#34;eks:DescribeUpdate&amp;#34;, &amp;#34;eks:DeleteCluster&amp;#34;, &amp;#34;eks:UpdateClusterVersion&amp;#34;, &amp;#34;eks:UpdateClusterConfig&amp;#34;, &amp;#34;eks:CreateFargateProfile&amp;#34;, &amp;#34;eks:TagResource&amp;#34;, &amp;#34;eks:UntagResource&amp;#34; ], &amp;#34;Resource&amp;#34;: [ &amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:cluster/tmp-test-eks-cluster-by-morimoto&amp;#34;, &amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:cluster/tmp-test-eks-cluster-by-morimoto/*&amp;#34; ], &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: [ &amp;#34;eks:DescribeFargateProfile&amp;#34;, &amp;#34;eks:DeleteFargateProfile&amp;#34; ], &amp;#34;Resource&amp;#34;: &amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:fargateprofile/tmp-test-eks-cluster-by-morimoto/*&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: [ &amp;#34;iam:GetRole&amp;#34;, &amp;#34;iam:listAttachedRolePolicies&amp;#34; ], &amp;#34;Resource&amp;#34;: &amp;#34;*&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: &amp;#34;iam:CreateServiceLinkedRole&amp;#34;, &amp;#34;Resource&amp;#34;: &amp;#34;*&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: [ &amp;#34;ec2:DescribeInstances&amp;#34;, &amp;#34;ec2:DescribeNetworkInterfaces&amp;#34;, &amp;#34;ec2:DescribeSecurityGroups&amp;#34;, &amp;#34;ec2:DescribeSubnets&amp;#34;, &amp;#34;ec2:DescribeRouteTables&amp;#34;, &amp;#34;ec2:DescribeDhcpOptions&amp;#34;, &amp;#34;ec2:DescribeVpcs&amp;#34; ], &amp;#34;Resource&amp;#34;: &amp;#34;*&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; } ] }</description><content>&lt;p>23時に寝て1時に起きて漫画を読んで6時に起きて漫画読んでた。&lt;/p>
&lt;h2 id="隔週の雑談">隔週の雑談&lt;/h2>
&lt;p>顧問のはらさんと隔週の打ち合わせ。今日の議題は &lt;a href="/diary/diary/posts/2022/0503/#打ち合わせ資料の作成">先日作成した第4期の展望&lt;/a> について雑談した。あまり深く考えずに起業して初期の頃に作った10ヶ年計画に対してわりとその通りに推移している。来期ぐらいで業務委託のお仕事は終える予定。来期か再来期ぐらいからプロダクト開発の期間に入る。自社プロダクトを作る前から徐々にマーケティングもしていかないといけない。そのため、今期からマーケティング施策を少しずつ増やしていて、まずは会社の信頼度を上げるところからやっていく。売上を上げるためのマーケティングではなく信頼度を上げるためのマーケティングを行う。金森氏が言うようにどんなに優れたプロダクトを作ったとしても売れるかどうかは別問題だ。&lt;/p>
&lt;iframe width="500" height="250" scrolling="no" src="https://alu.jp/series/%E6%98%A0%E5%83%8F%E7%A0%94%E3%81%AB%E3%81%AF%E6%89%8B%E3%82%92%E5%87%BA%E3%81%99%E3%81%AA%EF%BC%81/crop/embed/X8MNLWdDgPeZ5RIF25sD/0?referer=oembed" style="margin: auto;">&lt;/iframe>
&lt;h2 id="eks-クラスターの-helm-管理の調査">eks クラスターの helm 管理の調査&lt;/h2>
&lt;p>昨日の続き。権限設定がなんもわからんみたいな様相になったので調査のやり方を変えることにした。検証用の eks クラスターを cdk から新規に作成して helm をインストールするときのリソースや権限設定がどうなるのかを調査した。lambda 関数が5個ぐらい、ロールは10個ぐらい作成された。lambda の生成に時間がかかるのか？新規作成するのに25分、削除するときは30分ぐらいかかった。rds もそうだけど、eks のような複雑なインフラを cdk で管理すると実行するのにけっこう時間がかかる。設定が難しくなければ別によいけど、eks のような権限やリソースの設定が複雑なインフラはトライ&amp;amp;エラーで何度も実行する必要があるから、こういうものは cdk で管理するのには向かないインフラだと言えると思う。一定の設定のプラクティスが溜まるまでは eks は cdk で管理しない方がよいかもしれない。&lt;/p>
&lt;p>CreationRole というのが設定されて trust relationships に次のような設定が追加される。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Version&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2012-10-17&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Statement&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Principal&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;AWS&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;arn:aws:iam::${accountId}:root&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;sts:AssumeRole&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>このロールに含まれるカスタムポリシーには次のような設定がある。たぶんこんな感じのロールを新規に作成して &lt;code>kubectlRoleArn&lt;/code> として指定してやればいいんじゃないかと思う。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Version&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2012-10-17&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Statement&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;iam:PassRole&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;arn:aws:iam::${accountId}:role/${EksClusterIamRole}&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:CreateCluster&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:DescribeCluster&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:DescribeUpdate&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:DeleteCluster&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:UpdateClusterVersion&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:UpdateClusterConfig&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:CreateFargateProfile&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:TagResource&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:UntagResource&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:cluster/tmp-test-eks-cluster-by-morimoto&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:cluster/tmp-test-eks-cluster-by-morimoto/*&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:DescribeFargateProfile&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:DeleteFargateProfile&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:fargateprofile/tmp-test-eks-cluster-by-morimoto/*&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;iam:GetRole&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;iam:listAttachedRolePolicies&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;*&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;iam:CreateServiceLinkedRole&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;*&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeInstances&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeNetworkInterfaces&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeSecurityGroups&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeSubnets&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeRouteTables&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeDhcpOptions&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeVpcs&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;*&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>cdk と eks と lambda と iam がわからん</title><link>/diary/posts/2022/0512/</link><pubDate>Thu, 12 May 2022 11:42:54 +0900</pubDate><guid>/diary/posts/2022/0512/</guid><description>0時に寝て3時に起きて漫画読んで5時に寝て8時に起きた。
eks クラスターの helm 管理 昨日の続き。helm のよさはわかったので dapr を helm で管理しようとしている。その際になるべく cdk で管理できた方がよい。eks は cdk の外部で管理しているのだけど、既存の eks クラスターをインポートする機能も提供されていることに気付いた。
Using existing clusters それなら既存の eks クラスターをインポートして cdk で helm だけ管理しようと思って始めたものの、これがとても難しくて丸1日作業してわからなかった。設定項目は少ないけど、権限の問題で動かない。1回あたりの実行に15分ぐらいかかるのでトライ&amp;amp;エラーするのもなかなか大変。
kubectlRoleArn - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually.</description><content>&lt;p>0時に寝て3時に起きて漫画読んで5時に寝て8時に起きた。&lt;/p>
&lt;h2 id="eks-クラスターの-helm-管理">eks クラスターの helm 管理&lt;/h2>
&lt;p>昨日の続き。helm のよさはわかったので dapr を helm で管理しようとしている。その際になるべく cdk で管理できた方がよい。eks は cdk の外部で管理しているのだけど、既存の eks クラスターをインポートする機能も提供されていることに気付いた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters">Using existing clusters&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>それなら既存の eks クラスターをインポートして cdk で helm だけ管理しようと思って始めたものの、これがとても難しくて丸1日作業してわからなかった。設定項目は少ないけど、権限の問題で動かない。1回あたりの実行に15分ぐらいかかるのでトライ&amp;amp;エラーするのもなかなか大変。&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>kubectlRoleArn&lt;/code> - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually. The trust policy of this role should include the the arn:aws::iam::${accountId}:root principal in order to allow the execution role of the kubectl resource to assume it.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;code>kubectlRoleArn&lt;/code> の設定をどうするかだけなんだが、この説明でどう設定していいか理解できなかった。cdk で新規に eks クラスターを作成するなら自動で作ってくれるけど、既存の eks クラスターの場合は自分で設定しないといけない。ややこしいことに cdk は kubectl の実行を lambda 経由で実行するので eks と lambda と iam のロールやポリシーを適切に設定する必要がある。lambda にどういう権限を設定するのが適切なのかは本当に難しい。サーバーレスはよいアイディアだとは思うけど、lambda は難し過ぎて私はなるべく使いたくないサービスではある。結局わからなくて翌日に持ち越し。&lt;/p></content></item><item><title>障害調査と先入観</title><link>/diary/posts/2022/0427/</link><pubDate>Wed, 27 Apr 2022 07:37:11 +0900</pubDate><guid>/diary/posts/2022/0427/</guid><description>23時に寝て5時過ぎに起きた。
インフラの不具合調査 本番環境に初回デプロイして ecs から secrets manager のアクセスに失敗しているエラーが出ていたので調査した。エラーメッセージでググるとクラスメソッドさんの記事が出てきた。
ECSでコンテナ環境変数をSecretManagerから取得する際にResourceInitializationErrorが発生したときの対処方法 この記事によると、次のどちらかの原因かなと推測していた。
実行 IAM ロールの権限不足 SecretsManager エンドポイントへの不到達 調べども調べどもテスト環境との違いがわからなくてはまってしまった。ある検証をしていたときに、テスト環境を手動で構築した担当者から datadog の api key を設定してないんじゃない？と言われて、まさにそれが原因だった。cdk のコード上は設定済みのものとして ecs の datadog のサイドカーに設定していた。先入観で rds の credential 情報を取得できずにエラーになっていると思い込んでいたが、サイドカーの datadog の api key が原因ではまるべくしてはまったという感じ。
一方で secrets manager に登録するサードパーティの api key をどこで管理するかというのは難しい問題でもある。cdk のコードの中に書いてしまうというのも1つのやり方ではあるが、昨今の github 関連のサードパーティから派生したセキュリティインシデントで private リポジトリのソースコードにアクセスされることも発生しているのでソースコードには書きたくない。で、手動で secrets manager に設定しないといけないから、今回みたいな初回デプロイ時に誰も気付かないみたいなことが起きる。</description><content>&lt;p>23時に寝て5時過ぎに起きた。&lt;/p>
&lt;h2 id="インフラの不具合調査">インフラの不具合調査&lt;/h2>
&lt;p>本番環境に初回デプロイして ecs から secrets manager のアクセスに失敗しているエラーが出ていたので調査した。エラーメッセージでググるとクラスメソッドさんの記事が出てきた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://dev.classmethod.jp/articles/tsnote-ecs-resourceinitializationerror/">ECSでコンテナ環境変数をSecretManagerから取得する際にResourceInitializationErrorが発生したときの対処方法&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>この記事によると、次のどちらかの原因かなと推測していた。&lt;/p>
&lt;ul>
&lt;li>実行 IAM ロールの権限不足&lt;/li>
&lt;li>SecretsManager エンドポイントへの不到達&lt;/li>
&lt;/ul>
&lt;p>調べども調べどもテスト環境との違いがわからなくてはまってしまった。ある検証をしていたときに、テスト環境を手動で構築した担当者から datadog の api key を設定してないんじゃない？と言われて、まさにそれが原因だった。cdk のコード上は設定済みのものとして ecs の datadog のサイドカーに設定していた。先入観で rds の credential 情報を取得できずにエラーになっていると思い込んでいたが、サイドカーの datadog の api key が原因ではまるべくしてはまったという感じ。&lt;/p>
&lt;p>一方で secrets manager に登録するサードパーティの api key をどこで管理するかというのは難しい問題でもある。cdk のコードの中に書いてしまうというのも1つのやり方ではあるが、昨今の github 関連のサードパーティから派生したセキュリティインシデントで private リポジトリのソースコードにアクセスされることも発生しているのでソースコードには書きたくない。で、手動で secrets manager に設定しないといけないから、今回みたいな初回デプロイ時に誰も気付かないみたいなことが起きる。&lt;/p></content></item><item><title>本番環境反映の監督</title><link>/diary/posts/2022/0425/</link><pubDate>Mon, 25 Apr 2022 19:39:39 +0900</pubDate><guid>/diary/posts/2022/0425/</guid><description>昨日は思いっきり昼寝してしまったせいか、夜に眠れなくて3時ぐらいまで起きてて、寝たのか寝てのかよくわからない雰囲気で7時前に起きた。
インフラ作業の本番反映 先週対応した api gateway のコード化 を本番環境に反映した。手動で作成されていた api gateway, vpc link, セキュリティグループとそれに関連するリソース群を cdk 管理のコードで置き換える。既存のリソースを削除してから cdk でデプロイするため、失敗しても切り戻しできないのでプレッシャーがかかる。とはいえ、想定通りに作業が進捗して2時間ほどで完了した。それと並行してテスト環境では、手動で作成された rds を cdk 管理のコードで置き換える移行作業をしていた。これも一筋縄ではいかなくて右往左往しながら作業してた。これを本番環境でやるのもまた億劫だなぁと思いながら発生したエラーや事象を書き綴っていた。あと本番環境で行う大きな移行作業はこれだけ。</description><content>&lt;p>昨日は思いっきり昼寝してしまったせいか、夜に眠れなくて3時ぐらいまで起きてて、寝たのか寝てのかよくわからない雰囲気で7時前に起きた。&lt;/p>
&lt;h2 id="インフラ作業の本番反映">インフラ作業の本番反映&lt;/h2>
&lt;p>先週対応した &lt;a href="/diary/diary/posts/2022/0419/">api gateway のコード化&lt;/a> を本番環境に反映した。手動で作成されていた api gateway, vpc link, セキュリティグループとそれに関連するリソース群を cdk 管理のコードで置き換える。既存のリソースを削除してから cdk でデプロイするため、失敗しても切り戻しできないのでプレッシャーがかかる。とはいえ、想定通りに作業が進捗して2時間ほどで完了した。それと並行してテスト環境では、手動で作成された rds を cdk 管理のコードで置き換える移行作業をしていた。これも一筋縄ではいかなくて右往左往しながら作業してた。これを本番環境でやるのもまた億劫だなぁと思いながら発生したエラーや事象を書き綴っていた。あと本番環境で行う大きな移行作業はこれだけ。&lt;/p></content></item><item><title>rds の再作成</title><link>/diary/posts/2022/0421/</link><pubDate>Thu, 21 Apr 2022 07:37:32 +0900</pubDate><guid>/diary/posts/2022/0421/</guid><description>23時に寝て5時半に起きた。久しぶりによく眠れた気がする。
rds を独立したスタックに分離 昨日の続き。ライフサイクルにあわせたスタックに分割し、スタック間の依存関係を適切に定義することで堅牢なインフラコードになる。最後に残った DatabaseStack を分離するところを朝からやっていた。
DatabaseStack BackendStack GatewayStack FrontendStack rds を壊して、バージョンを aurora postgresql 最新の 13.x にアップグレードして、cf のテンプレートで小細工をしながら既存の環境を移行したりしていた。pg_dump を使ってテキストに dump したデータを、psql を使ってリストアしたりもした。データ量が少なかったらこれで移行してもいいかもしれない。
$ pg_dump --host mydb.xxx.ap-northeast-1.rds.amazonaws.com --username user mydb &amp;gt; mydb.dump $ psql --host mydb.xxx.ap-northeast-1.rds.amazonaws.com --username user --dbname mydb --file ./mydb.dump</description><content>&lt;p>23時に寝て5時半に起きた。久しぶりによく眠れた気がする。&lt;/p>
&lt;h2 id="rds-を独立したスタックに分離">rds を独立したスタックに分離&lt;/h2>
&lt;p>昨日の続き。ライフサイクルにあわせたスタックに分割し、スタック間の依存関係を適切に定義することで堅牢なインフラコードになる。最後に残った DatabaseStack を分離するところを朝からやっていた。&lt;/p>
&lt;ul>
&lt;li>DatabaseStack
&lt;ul>
&lt;li>BackendStack
&lt;ul>
&lt;li>GatewayStack
&lt;ul>
&lt;li>FrontendStack&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>rds を壊して、バージョンを aurora postgresql 最新の 13.x にアップグレードして、cf のテンプレートで小細工をしながら既存の環境を移行したりしていた。&lt;a href="https://www.postgresql.org/docs/13/app-pgdump.html">pg_dump&lt;/a> を使ってテキストに dump したデータを、psql を使ってリストアしたりもした。データ量が少なかったらこれで移行してもいいかもしれない。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ pg_dump --host mydb.xxx.ap-northeast-1.rds.amazonaws.com --username user mydb &amp;gt; mydb.dump
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ psql --host mydb.xxx.ap-northeast-1.rds.amazonaws.com --username user --dbname mydb --file ./mydb.dump
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>堅牢なインフラコード</title><link>/diary/posts/2022/0420/</link><pubDate>Wed, 20 Apr 2022 07:38:23 +0900</pubDate><guid>/diary/posts/2022/0420/</guid><description>23時に寝て5時に起きた。
駐輪場の定期更新 3ヶ月ごとの更新。ちょうどいまのお仕事の契約と同じ更新月になっている。前回と同じ金額だったのでまだ駐輪場の料金は値上げされていない。世界的にインフレしているのに日本が全然インフレしていない理由の1つとして不動産が値上げしていないからというのを日銀の記事で見かけた気がする。どこかのタイミングで不動産関連の値上げも始まるのかもしれない。
インフラコードの抜本的リファクタリング 約2週間かけて、新規インフラ環境の構築、既存インフラの cdk/cf と同期されていなかったインフラ (rds, security group, croudfront, api gateway, waf) の同期など、インフラコードの大きな変更をやり終えた。一部 fromLookup でインポートしているリソースもあるが、いま完全に cdk/cf 管理なインフラとなった。ここからはせっかく cdk でコードを書いているので、モジュール化や共通化など、再利用可能なリソースとして定義して、複数の Stack でコードを再利用するといったリファクタリングをしていく。ひとまずこのことをボーナスステージと呼ぼう。いま完全に同期されたインフラがあるため、インフラ上のリソースの差分が出なければリファクタリングは正しいことが保証される。cdk は Stack 間の依存関係 も定義できるため、適切な依存関係を定義することでより堅牢なインフラコードとなるはずである。具体的には次のような依存関係になる。然るべき堅牢なインフラコードに書き換えていく。
DatabaseStack BackendStack GatewayStack FrontendStack</description><content>&lt;p>23時に寝て5時に起きた。&lt;/p>
&lt;h2 id="駐輪場の定期更新">駐輪場の定期更新&lt;/h2>
&lt;p>3ヶ月ごとの更新。ちょうどいまのお仕事の契約と同じ更新月になっている。前回と同じ金額だったのでまだ駐輪場の料金は値上げされていない。世界的にインフレしているのに日本が全然インフレしていない理由の1つとして不動産が値上げしていないからというのを日銀の記事で見かけた気がする。どこかのタイミングで不動産関連の値上げも始まるのかもしれない。&lt;/p>
&lt;h2 id="インフラコードの抜本的リファクタリング">インフラコードの抜本的リファクタリング&lt;/h2>
&lt;p>約2週間かけて、新規インフラ環境の構築、既存インフラの cdk/cf と同期されていなかったインフラ (rds, security group, croudfront, api gateway, waf) の同期など、インフラコードの大きな変更をやり終えた。一部 &lt;code>fromLookup&lt;/code> でインポートしているリソースもあるが、いま完全に cdk/cf 管理なインフラとなった。ここからはせっかく cdk でコードを書いているので、モジュール化や共通化など、再利用可能なリソースとして定義して、複数の Stack でコードを再利用するといったリファクタリングをしていく。ひとまずこのことをボーナスステージと呼ぼう。いま完全に同期されたインフラがあるため、インフラ上のリソースの差分が出なければリファクタリングは正しいことが保証される。cdk は &lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib-readme.html#dependencies">Stack 間の依存関係&lt;/a> も定義できるため、適切な依存関係を定義することでより堅牢なインフラコードとなるはずである。具体的には次のような依存関係になる。然るべき堅牢なインフラコードに書き換えていく。&lt;/p>
&lt;ul>
&lt;li>DatabaseStack
&lt;ul>
&lt;li>BackendStack
&lt;ul>
&lt;li>GatewayStack
&lt;ul>
&lt;li>FrontendStack&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></content></item><item><title>api gateway のデプロイ検証</title><link>/diary/posts/2022/0419/</link><pubDate>Tue, 19 Apr 2022 07:38:47 +0900</pubDate><guid>/diary/posts/2022/0419/</guid><description>0時に寝て6時に起きた。
api gateway のデプロイ検証 昨日の続き。api gateway を再作成すると、当然、既存のテスト環境は疎通できなくなってしまう。他の開発者を妨害しないようにまた21時から深夜にかけて作業しようと考えていたら、いまは閑散期でテスト環境を使わないといけないような開発の状況にはないと他の開発者から教えてもらった。PO の人たちも検証は一段落しているとのこと。午後からテスト環境壊してもよいという確認が取れたので13時からインフラ作業をしていた。早ければ1時間ぐらい、遅ければ4時間と見積もって、実際にデプロイしてみると、あれやこれやの抜け・漏れ、手動で設定されていたリソースの弊害などもあって、3時間ぐらいかかった。もう2週間ほどインフラの作業ばかりやっているせいか、何が起こっても4時間ぐらいあれば調査とリストアを完了できるぐらいの自信がついてきた。最終的に cdk で再作成した api gateway を使って cloudfront の distribution 経由で web api 呼び出しが繋がった。当たり前の話なんだけど、繋がる瞬間、ローカルで web api のエンドポイントを叩いてレスポンスが返ってくるときが嬉しい。最後の大物だったインフラもたった2日でやっつけることができた。これからはボーナスステージ。</description><content>&lt;p>0時に寝て6時に起きた。&lt;/p>
&lt;h2 id="api-gateway-のデプロイ検証">api gateway のデプロイ検証&lt;/h2>
&lt;p>昨日の続き。api gateway を再作成すると、当然、既存のテスト環境は疎通できなくなってしまう。他の開発者を妨害しないようにまた21時から深夜にかけて作業しようと考えていたら、いまは閑散期でテスト環境を使わないといけないような開発の状況にはないと他の開発者から教えてもらった。PO の人たちも検証は一段落しているとのこと。午後からテスト環境壊してもよいという確認が取れたので13時からインフラ作業をしていた。早ければ1時間ぐらい、遅ければ4時間と見積もって、実際にデプロイしてみると、あれやこれやの抜け・漏れ、手動で設定されていたリソースの弊害などもあって、3時間ぐらいかかった。もう2週間ほどインフラの作業ばかりやっているせいか、何が起こっても4時間ぐらいあれば調査とリストアを完了できるぐらいの自信がついてきた。最終的に cdk で再作成した api gateway を使って cloudfront の distribution 経由で web api 呼び出しが繋がった。当たり前の話なんだけど、繋がる瞬間、ローカルで web api のエンドポイントを叩いてレスポンスが返ってくるときが嬉しい。最後の大物だったインフラもたった2日でやっつけることができた。これからはボーナスステージ。&lt;/p></content></item><item><title>api gateway のコード化</title><link>/diary/posts/2022/0418/</link><pubDate>Mon, 18 Apr 2022 07:39:00 +0900</pubDate><guid>/diary/posts/2022/0418/</guid><description>1時に寝て7時に起きた。
api gateway のコード化 いま cdk で管理していない大きなインフラとして api gateway がある。さらに restful api ではなく http api という新しい仕組みを使っているため、cdk 側も experimental な機能として提供されている。experimental と言っても本番環境で使われいるので十分に production ready と言える。弊害としては cdk のサンプルコードが少なく、公式の api reference に付いているサンプルコードぐらいしか参考になるものがなく、適当にコードを書いてでデプロイし、管理画面で意図した設定になっているかを確認するといったトライ&amp;amp;エラーみたいなやり方しかない。今日の時点では pr まで作ってデプロイと検証は翌日にまわることにした。
さらに cdk の v2 系では -alpha というパッケージが experimental 向けの機能を提供している。バージョン番号ではなく、パッケージ名に alpha が付くという分かりにくさに拍車をかけている。お手伝い先で使っている設定は次の3つのパッケージを使うことで実現できた。api gateway, vpc link, integration, route, authorization, stage など、複数のリソースを連携して設定しないといけないのでちょっとややこしい。
https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-alpha https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-authorizers-alpha https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-integrations-alpha</description><content>&lt;p>1時に寝て7時に起きた。&lt;/p>
&lt;h2 id="api-gateway-のコード化">api gateway のコード化&lt;/h2>
&lt;p>いま cdk で管理していない大きなインフラとして &lt;a href="https://aws.amazon.com/jp/api-gateway/">api gateway&lt;/a> がある。さらに restful api ではなく http api という新しい仕組みを使っているため、cdk 側も experimental な機能として提供されている。experimental と言っても本番環境で使われいるので十分に production ready と言える。弊害としては cdk のサンプルコードが少なく、公式の api reference に付いているサンプルコードぐらいしか参考になるものがなく、適当にコードを書いてでデプロイし、管理画面で意図した設定になっているかを確認するといったトライ&amp;amp;エラーみたいなやり方しかない。今日の時点では pr まで作ってデプロイと検証は翌日にまわることにした。&lt;/p>
&lt;p>さらに cdk の v2 系では &lt;code>-alpha&lt;/code> というパッケージが experimental 向けの機能を提供している。バージョン番号ではなく、パッケージ名に alpha が付くという分かりにくさに拍車をかけている。お手伝い先で使っている設定は次の3つのパッケージを使うことで実現できた。api gateway, vpc link, integration, route, authorization, stage など、複数のリソースを連携して設定しないといけないのでちょっとややこしい。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-alpha">https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-alpha&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-authorizers-alpha">https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-authorizers-alpha&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-integrations-alpha">https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-integrations-alpha&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>コロナワクチン3回目</title><link>/diary/posts/2022/0414/</link><pubDate>Thu, 14 Apr 2022 07:11:44 +0900</pubDate><guid>/diary/posts/2022/0414/</guid><description>22時に寝て3時に起きた6時に起きた。季節の変わり目のせいか、いつも眠い。
ワクチン3回目接種 2回目は昨年の9月27日 に接種した。6ヶ月以上経たないといけないので3月27日以降に接種資格を得て、実際に接種券が届いたのが4月7日だった。神戸市は初回のときにまごまごしたので他の自治体よりも遅れている。これまでファイザーを2回接種したので今回はモデルナを受けてみることにした。モデルナを扱っていてネット予約できるもっとも近くの診療所を予約した。オフィスから自転車で15分ぐらいのところ。16:30の予約なのに16時過ぎぐらいに行ったら普通に受け付けしてくれてすぐに接種もできた。16:05に接種終えて16:20まで待機してオフィスに戻ってきて普通にお仕事してた。その1時間半後に熱を測ったら37.0℃だった。その後もやや熱っぽいけど、平気とは言えば平気。
インフラ移行作業 昨日の続き。フロントエンドのテスト環境が壊れる可能性があるため、他メンバーが使っていない夜に移行作業を行う。POの人たちがQA検証を19-21時ぐらいにやっていることが多いので21時以降に作業すると連絡しておいた。なかなか大変だった。最悪の場合、数時間テスト環境を使えませんと伝えていたものの、その通りで4時間ぐらい検証作業をしていた。cloudfront の distribution 設定を CloudFrontWebDistribution から Distribution へ移行して、新しいやり方であるマネージドポリシーを使うようにした。この設定が意図した振る舞いにならなくて検証作業に時間を割いた。
cloudfront 経由で web api を呼び出すルートがあってキャッシュを無効にしたいのだが、無効にしたキャッシュポリシー (ttl をゼロにする) を作るとヘッダーの設定ができない。次のようなエラーになる。
The parameter HeaderBehavior is invalid for policy with caching disabled. (cloudfront): Cache Policy cannot forward Authorization header. #13441 によると、maxTTL を1秒にして Authorization ヘッダーをオリジンに転送するようには設定できる。キャッシュメソッドは GET と HEAD なので実運用上は問題ないとは思うが、この回避策がないかどうかを調べて検証していた。結果としてはなかった。Configuring CloudFront to forward the Authorization header には Authorization ヘッダーを転送する方法は次の2通りとある。
cache key に含める Managed-AllViewer という origin request policy をすべての viewer requests に含める 最終的には1番目のやり方で対応はしたが、2番目のオリジンリクエストポリシーを設定する方法も検証してみた。オリジンリクエストポリシーを単体で設定することはできなくて、キャッシュポリシーも一緒に設定しないといけないことからキャッシュポリシーの設定の影響を受けて意図したように Authorization ヘッダーの転送はできなかった。</description><content>&lt;p>22時に寝て3時に起きた6時に起きた。季節の変わり目のせいか、いつも眠い。&lt;/p>
&lt;h2 id="ワクチン3回目接種">ワクチン3回目接種&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2021/0927/#コロナワクチン2回目摂取">2回目は昨年の9月27日&lt;/a> に接種した。6ヶ月以上経たないといけないので3月27日以降に接種資格を得て、実際に接種券が届いたのが4月7日だった。神戸市は初回のときにまごまごしたので他の自治体よりも遅れている。これまでファイザーを2回接種したので今回はモデルナを受けてみることにした。モデルナを扱っていてネット予約できるもっとも近くの診療所を予約した。オフィスから自転車で15分ぐらいのところ。16:30の予約なのに16時過ぎぐらいに行ったら普通に受け付けしてくれてすぐに接種もできた。16:05に接種終えて16:20まで待機してオフィスに戻ってきて普通にお仕事してた。その1時間半後に熱を測ったら37.0℃だった。その後もやや熱っぽいけど、平気とは言えば平気。&lt;/p>
&lt;h2 id="インフラ移行作業">インフラ移行作業&lt;/h2>
&lt;p>昨日の続き。フロントエンドのテスト環境が壊れる可能性があるため、他メンバーが使っていない夜に移行作業を行う。POの人たちがQA検証を19-21時ぐらいにやっていることが多いので21時以降に作業すると連絡しておいた。なかなか大変だった。最悪の場合、数時間テスト環境を使えませんと伝えていたものの、その通りで4時間ぐらい検証作業をしていた。cloudfront の distribution 設定を CloudFrontWebDistribution から Distribution へ移行して、新しいやり方であるマネージドポリシーを使うようにした。この設定が意図した振る舞いにならなくて検証作業に時間を割いた。&lt;/p>
&lt;p>cloudfront 経由で web api を呼び出すルートがあってキャッシュを無効にしたいのだが、無効にしたキャッシュポリシー (ttl をゼロにする) を作るとヘッダーの設定ができない。次のようなエラーになる。&lt;/p>
&lt;pre tabindex="0">&lt;code>The parameter HeaderBehavior is invalid for policy with caching disabled.
&lt;/code>&lt;/pre>&lt;p>&lt;a href="https://github.com/aws/aws-cdk/issues/13441">(cloudfront): Cache Policy cannot forward Authorization header. #13441&lt;/a> によると、maxTTL を1秒にして &lt;code>Authorization&lt;/code> ヘッダーをオリジンに転送するようには設定できる。キャッシュメソッドは GET と HEAD なので実運用上は問題ないとは思うが、この回避策がないかどうかを調べて検証していた。結果としてはなかった。&lt;a href="https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/add-origin-custom-headers.html#add-origin-custom-headers-forward-authorization">Configuring CloudFront to forward the Authorization header&lt;/a> には &lt;code>Authorization&lt;/code> ヘッダーを転送する方法は次の2通りとある。&lt;/p>
&lt;ol>
&lt;li>cache key に含める&lt;/li>
&lt;li>Managed-AllViewer という origin request policy をすべての viewer requests に含める&lt;/li>
&lt;/ol>
&lt;p>最終的には1番目のやり方で対応はしたが、2番目のオリジンリクエストポリシーを設定する方法も検証してみた。オリジンリクエストポリシーを単体で設定することはできなくて、キャッシュポリシーも一緒に設定しないといけないことからキャッシュポリシーの設定の影響を受けて意図したように &lt;code>Authorization&lt;/code> ヘッダーの転送はできなかった。&lt;/p>
&lt;ul>
&lt;li>cache policy: Managed-CachingDisabled&lt;/li>
&lt;li>origin request policy: Managed-AllViewer&lt;/li>
&lt;/ul></content></item><item><title>cdk の cloudfront の distribution 設定の移行</title><link>/diary/posts/2022/0413/</link><pubDate>Wed, 13 Apr 2022 15:22:21 +0900</pubDate><guid>/diary/posts/2022/0413/</guid><description>0時に寝て5時過ぎに起きた。
フロントエンドのインフラ作業の続き 昨日、cloudfront の distribution 設定のビヘイビアのキャッシュ設定が誤っていることに気付いたので cdk のコードを修正していく。Amazon CloudFront キャッシュポリシーとオリジンリクエストポリシーを発表 によると、2020年ぐらいに distribution ごとに個別設定していたのをマネージドポリシーというリソースを参照することで一元管理できるようになったらしい。cdk のコードで言えば、次の issue で対応されているが、これらの機能は aws_cloudfront.Distribution のみに追加されている。従来の CloudFrontWebDistribution では使えないので distribution 設定そのものを新しいやり方に移行する必要がある。
https://github.com/aws/aws-cdk/issues/9644 https://github.com/aws/aws-cdk/issues/9647 基本的には同じ設定を行うので新しいクラスのメンバーや構造にあわせて移行するだけなので難しくはないけれど、1つずつ設定内容の移行方法を確認していかないといけないから手間暇はかかる。cdk のドキュメントをみると、型に対してどういった設定をすればよいかが書いてあって、あとはメソッドの定義などもみながら自分たちの設定に近いものを選択していくといった作業になる。難しくはないけど時間はかかる。半日ほどやって移行のための pr を作成した。</description><content>&lt;p>0時に寝て5時過ぎに起きた。&lt;/p>
&lt;h2 id="フロントエンドのインフラ作業の続き">フロントエンドのインフラ作業の続き&lt;/h2>
&lt;p>昨日、cloudfront の distribution 設定のビヘイビアのキャッシュ設定が誤っていることに気付いたので cdk のコードを修正していく。&lt;a href="https://aws.amazon.com/jp/blogs/news/amazon-cloudfront-announces-cache-and-origin-request-policies/">Amazon CloudFront キャッシュポリシーとオリジンリクエストポリシーを発表&lt;/a> によると、2020年ぐらいに distribution ごとに個別設定していたのをマネージドポリシーというリソースを参照することで一元管理できるようになったらしい。cdk のコードで言えば、次の issue で対応されているが、これらの機能は &lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_cloudfront.Distribution.html">aws_cloudfront.Distribution&lt;/a> のみに追加されている。従来の CloudFrontWebDistribution では使えないので distribution 設定そのものを新しいやり方に移行する必要がある。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/9644">https://github.com/aws/aws-cdk/issues/9644&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/9647">https://github.com/aws/aws-cdk/issues/9647&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>基本的には同じ設定を行うので新しいクラスのメンバーや構造にあわせて移行するだけなので難しくはないけれど、1つずつ設定内容の移行方法を確認していかないといけないから手間暇はかかる。cdk のドキュメントをみると、型に対してどういった設定をすればよいかが書いてあって、あとはメソッドの定義などもみながら自分たちの設定に近いものを選択していくといった作業になる。難しくはないけど時間はかかる。半日ほどやって移行のための pr を作成した。&lt;/p></content></item><item><title>cloudfront のキャッシュ設定</title><link>/diary/posts/2022/0412/</link><pubDate>Tue, 12 Apr 2022 07:31:19 +0900</pubDate><guid>/diary/posts/2022/0412/</guid><description>1時に寝て5時に起きた。
フロントエンドのインフラ作業 これまでバックエンドのインフラ作業をしてきたが、そちらの移行作業は完了した。フロントエンドのインフラも積み残しが多々あるのでこの機にリファクタリングする。差分が出ないことを確認して cf のテンプレートのドリフト結果を解消したものをデプロイしたらフロントエンドが壊れた。具体的には xhr の web api 呼び出しに対して認可エラーが返るようになった。数時間ほどの調査の結果、cloudfront の distribution 設定のビヘイビアのキャッシュ設定で web api 呼び出しのときに authorization ヘッダーを転送するための設定が漏れていることがわかった。cdk のコードにはそんな設定がどこにもなく差分も表示されないことから、フロントエンドの cdk コードも全く保守されていないことがわかった。デプロイするときに前回のデプロイが半年前だったのでなんか悪い予感はしたんよね。インフラ担当者の怠慢が度を越し過ぎてもう何が起きても私は驚かないけど。設定に漏れがある前提でフロントエンドのインフラをリファクタリングしていかないといけない。</description><content>&lt;p>1時に寝て5時に起きた。&lt;/p>
&lt;h2 id="フロントエンドのインフラ作業">フロントエンドのインフラ作業&lt;/h2>
&lt;p>これまでバックエンドのインフラ作業をしてきたが、そちらの移行作業は完了した。フロントエンドのインフラも積み残しが多々あるのでこの機にリファクタリングする。差分が出ないことを確認して cf のテンプレートのドリフト結果を解消したものをデプロイしたらフロントエンドが壊れた。具体的には xhr の web api 呼び出しに対して認可エラーが返るようになった。数時間ほどの調査の結果、cloudfront の distribution 設定のビヘイビアのキャッシュ設定で web api 呼び出しのときに authorization ヘッダーを転送するための設定が漏れていることがわかった。cdk のコードにはそんな設定がどこにもなく差分も表示されないことから、フロントエンドの cdk コードも全く保守されていないことがわかった。デプロイするときに前回のデプロイが半年前だったのでなんか悪い予感はしたんよね。インフラ担当者の怠慢が度を越し過ぎてもう何が起きても私は驚かないけど。設定に漏れがある前提でフロントエンドのインフラをリファクタリングしていかないといけない。&lt;/p></content></item><item><title>cdk/cf の Stack とライフサイクル</title><link>/diary/posts/2022/0411/</link><pubDate>Mon, 11 Apr 2022 07:26:57 +0900</pubDate><guid>/diary/posts/2022/0411/</guid><description>0時に寝て5時に起きた。
インフラ変更の本番作業 先週やっていた様々なインフラ構築の改善を本番環境に適用する。cf の changeset は23とかになっていた気がする。大きな括りで次の3つの移行作業をした。
rds をスタックから切り離す cdk の v1 から v2 へのアップグレード ポリシーとセキュリティグループのドリフト解消 私は本番環境へのアクセス権限をもっていないので社員さんの作業内容を伝えてやってもらう。cf のテンプレートの更新やドリフトの解消など、テスト環境で10時間以上は費やした検証結果が功を奏して、本番作業は想定外の事象も発生せず1.5時間で完了した。cdk のコードも意図した設定になるように修正済みだし、なにも問題は発生しない。cdk のコードを書くときは cf のイベントログやドリフト結果と実際のインフラの振る舞いを確認するといった検証には時間がかかるが、それができてしまえば本番環境の構築はうまくできる。それがわかっているインフラ担当者がいなくなると、また1から担当者が検証する必要があって保守は大変かもしれないけど。
同じ Stack でどういったリソースを管理するかというライフサイクルは難しい問題かもしれない。今回 rds を削除したのは、なんらかの理由で手動運用で rds を変更することがあって、アプリケーション Stack から外してしまった方が保守しやすいのではないかという意見が出たため。それが正しいかどうかはわからないが、一度作ったリソースを削除しないもの (vpc や s3 bucket など) をアプリケーション Stack で管理すると、再作成できなくて面倒くさいことがある。というのは、再作成は新規に作成 → 古いリソースを削除の順番で処理されるため、一意な名前をもつリソースは基本的に再作成できない。</description><content>&lt;p>0時に寝て5時に起きた。&lt;/p>
&lt;h2 id="インフラ変更の本番作業">インフラ変更の本番作業&lt;/h2>
&lt;p>先週やっていた様々なインフラ構築の改善を本番環境に適用する。cf の changeset は23とかになっていた気がする。大きな括りで次の3つの移行作業をした。&lt;/p>
&lt;ul>
&lt;li>rds をスタックから切り離す&lt;/li>
&lt;li>cdk の v1 から v2 へのアップグレード&lt;/li>
&lt;li>ポリシーとセキュリティグループのドリフト解消&lt;/li>
&lt;/ul>
&lt;p>私は本番環境へのアクセス権限をもっていないので社員さんの作業内容を伝えてやってもらう。cf のテンプレートの更新やドリフトの解消など、テスト環境で10時間以上は費やした検証結果が功を奏して、本番作業は想定外の事象も発生せず1.5時間で完了した。cdk のコードも意図した設定になるように修正済みだし、なにも問題は発生しない。cdk のコードを書くときは cf のイベントログやドリフト結果と実際のインフラの振る舞いを確認するといった検証には時間がかかるが、それができてしまえば本番環境の構築はうまくできる。それがわかっているインフラ担当者がいなくなると、また1から担当者が検証する必要があって保守は大変かもしれないけど。&lt;/p>
&lt;p>同じ Stack でどういったリソースを管理するかというライフサイクルは難しい問題かもしれない。今回 rds を削除したのは、なんらかの理由で手動運用で rds を変更することがあって、アプリケーション Stack から外してしまった方が保守しやすいのではないかという意見が出たため。それが正しいかどうかはわからないが、一度作ったリソースを削除しないもの (vpc や s3 bucket など) をアプリケーション Stack で管理すると、再作成できなくて面倒くさいことがある。というのは、再作成は新規に作成 → 古いリソースを削除の順番で処理されるため、一意な名前をもつリソースは基本的に再作成できない。&lt;/p></content></item><item><title>壊れた cf スタックのリストアと cdk の再同期</title><link>/diary/posts/2022/0408/</link><pubDate>Fri, 08 Apr 2022 10:13:19 +0900</pubDate><guid>/diary/posts/2022/0408/</guid><description>2時に寝て6時半に起きた。インフラエンジニアになったのでみんなが作業していない時間にインフラの保守作業をするようにしている。昼はアプリケーションエンジニア、夜はインフラエンジニアみたいな生活になっていてしんどい。
壊れた cf スタックの更新 テスト環境の cf スタックを手動で更新して壊れているのを cdk で管理できるように直した。壊れていたのは次の3つ。
rds をスナップショットからリストアしたので cf が管理している rds リソースが存在しない iam の acl 設定が異なる セキュリティグループのインバウンドルールが異なる aws 的にもそういった状況は認識していて cdk で同期できなくなった cf スタックを更新する手順を提供している。
AWS CloudFormation スタックを削除したときにリソースの一部を保持する方法を教えてください。 ざっくり手順をまとめると次になる。
対象のリソースに DeletetionPolicy=Retain にセットする テンプレートからリソースを削除して、スタックの更新を実行する テンプレート内のリソースの実際の状態を describe して、スタック内に既存のリソースをインポートする リソースの設定ぐらいなら既存のリソースからインポートしなくても cf のテンプレートを直接書き換えたものをアップロードしてスタックを更新するのでも大丈夫だったりする。しかし、cdk もそのテンプレートにあうように修正しないといけないため、cdk のコードとテンプレートのコードの両方をチェックしながら検証する必要がある。cdk でリソース管理ができるようになったからといって、それが変更前の既存のリソースの設定と同じかどうかは人間が目でみて検証しないといけない。これがあちこちで参照されているリソースだと追跡するのが面倒くさいといった手間暇がかかる。
cdk がよいものかどうか、私はまだ判断がつかないけど、cf を抽象化して便利になっているところは認めるものの、cf のスタックが壊れたときのトラブルシューティングが必要以上に複雑で厄介というのも事実ではある。一方で壊れた cf スタックを5時間ぐらいかけて直したのではまりポイントはいくつかも学ぶことができた。しんどかったけど。例えば、あるセキュリティグループのインバウンドルールに別のセキュリティグループを関連付けるとき、1つの設定ではうまくいかなくて次の2つの設定を追加した。これが適切かどうかわからないが、この設定で cdk でデプロイしたスタックの環境と既存リソースとの環境が整合した状態 (ドリフトが解消される) になった。こういうのが cdk の抽象化による訳のわからないところの1つ。
otherSecurityGroup.addIngressRule( ec2.SecurityGroup.fromSecurityGroupId(this, &amp;#39;my security group&amp;#39;, mySgId), ec2.Port.tcp(80), &amp;#34;my inboud rule&amp;#34;, ) otherSecurityGroup.addIngressRule( ec2.Peer.securityGroupId(mySgId), ec2.Port.tcp(80), &amp;#34;my inboud rule&amp;#34;, )</description><content>&lt;p>2時に寝て6時半に起きた。インフラエンジニアになったのでみんなが作業していない時間にインフラの保守作業をするようにしている。昼はアプリケーションエンジニア、夜はインフラエンジニアみたいな生活になっていてしんどい。&lt;/p>
&lt;h2 id="壊れた-cf-スタックの更新">壊れた cf スタックの更新&lt;/h2>
&lt;p>テスト環境の cf スタックを手動で更新して壊れているのを cdk で管理できるように直した。壊れていたのは次の3つ。&lt;/p>
&lt;ul>
&lt;li>rds をスナップショットからリストアしたので cf が管理している rds リソースが存在しない&lt;/li>
&lt;li>iam の acl 設定が異なる&lt;/li>
&lt;li>セキュリティグループのインバウンドルールが異なる&lt;/li>
&lt;/ul>
&lt;p>aws 的にもそういった状況は認識していて cdk で同期できなくなった cf スタックを更新する手順を提供している。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aws.amazon.com/jp/premiumsupport/knowledge-center/delete-cf-stack-retain-resources/">AWS CloudFormation スタックを削除したときにリソースの一部を保持する方法を教えてください。&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>ざっくり手順をまとめると次になる。&lt;/p>
&lt;ol>
&lt;li>対象のリソースに DeletetionPolicy=Retain にセットする&lt;/li>
&lt;li>テンプレートからリソースを削除して、スタックの更新を実行する&lt;/li>
&lt;li>テンプレート内のリソースの実際の状態を describe して、スタック内に既存のリソースをインポートする&lt;/li>
&lt;/ol>
&lt;p>リソースの設定ぐらいなら既存のリソースからインポートしなくても cf のテンプレートを直接書き換えたものをアップロードしてスタックを更新するのでも大丈夫だったりする。しかし、cdk もそのテンプレートにあうように修正しないといけないため、cdk のコードとテンプレートのコードの両方をチェックしながら検証する必要がある。cdk でリソース管理ができるようになったからといって、それが変更前の既存のリソースの設定と同じかどうかは人間が目でみて検証しないといけない。これがあちこちで参照されているリソースだと追跡するのが面倒くさいといった手間暇がかかる。&lt;/p>
&lt;p>cdk がよいものかどうか、私はまだ判断がつかないけど、cf を抽象化して便利になっているところは認めるものの、cf のスタックが壊れたときのトラブルシューティングが必要以上に複雑で厄介というのも事実ではある。一方で壊れた cf スタックを5時間ぐらいかけて直したのではまりポイントはいくつかも学ぶことができた。しんどかったけど。例えば、あるセキュリティグループのインバウンドルールに別のセキュリティグループを関連付けるとき、1つの設定ではうまくいかなくて次の2つの設定を追加した。これが適切かどうかわからないが、この設定で cdk でデプロイしたスタックの環境と既存リソースとの環境が整合した状態 (ドリフトが解消される) になった。こういうのが cdk の抽象化による訳のわからないところの1つ。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-typescript" data-lang="typescript">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">otherSecurityGroup&lt;/span>.&lt;span style="color:#a6e22e">addIngressRule&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ec2&lt;/span>.&lt;span style="color:#a6e22e">SecurityGroup&lt;/span>.&lt;span style="color:#a6e22e">fromSecurityGroupId&lt;/span>(&lt;span style="color:#66d9ef">this&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;my security group&amp;#39;&lt;/span>, &lt;span style="color:#a6e22e">mySgId&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ec2&lt;/span>.&lt;span style="color:#a6e22e">Port&lt;/span>.&lt;span style="color:#a6e22e">tcp&lt;/span>(&lt;span style="color:#ae81ff">80&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;my inboud rule&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">otherSecurityGroup&lt;/span>.&lt;span style="color:#a6e22e">addIngressRule&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ec2&lt;/span>.&lt;span style="color:#a6e22e">Peer&lt;/span>.&lt;span style="color:#a6e22e">securityGroupId&lt;/span>(&lt;span style="color:#a6e22e">mySgId&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ec2&lt;/span>.&lt;span style="color:#a6e22e">Port&lt;/span>.&lt;span style="color:#a6e22e">tcp&lt;/span>(&lt;span style="color:#ae81ff">80&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;my inboud rule&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>cdk のメジャーバージョンのマイグレーション</title><link>/diary/posts/2022/0407/</link><pubDate>Thu, 07 Apr 2022 06:10:00 +0900</pubDate><guid>/diary/posts/2022/0407/</guid><description>0時に寝て5時に起きた。開発者にインフラ変更の影響を出さないように6時半からインフラのお仕事してた。
cdk v1 と v2 の違い AWS CDK Versions には v1 と v2 の2つがある。新規で作るものは v2 を選択すればよいけど、既存のスタックが v1 だとマイグレーションが必要になる。cdk は bootstrap したときに CDKToolkit というスタックを生成する。cdk をアップグレードするというのはこのスタックの設定も更新する必要がある。デフォルト設定をそのまま使っていればマイグレーションはそんなに難しくはないはずだけど、設定をカスタマイズしていたりするといくつかパラメーターを調整したりしなかったりしてややこしいかもしれない。
Migrating to AWS CDK v2 Bootstrapping また v2 は v1 の experimental な機能は移行されていないため、v1 のライブラリを直接使うか、自前でその機能を実装するといったことも必要になる可能性がある。
cdk v2 を使っていて v1 にある機能が v2 になくてあれー？って感じで調べてたら experimental な機能はまだ移行されてないらしい。https://t.co/SbRZ5ddrTj
&amp;mdash; Tetsuya Morimoto (@t2y) April 7, 2022 例えば、v1 の apigwv2.VpcLink というメソッドは experimental で v2 に移行されていないため、v2 に移行されている stable な CfnVpcLink という機能を使って次のように実装した。これは v1 の cdk の実装をみて同じように実装しただけ。</description><content>&lt;p>0時に寝て5時に起きた。開発者にインフラ変更の影響を出さないように6時半からインフラのお仕事してた。&lt;/p>
&lt;h2 id="cdk-v1-と-v2-の違い">cdk v1 と v2 の違い&lt;/h2>
&lt;p>&lt;a href="https://docs.aws.amazon.com/cdk/api/versions.html">AWS CDK Versions&lt;/a> には v1 と v2 の2つがある。新規で作るものは v2 を選択すればよいけど、既存のスタックが v1 だとマイグレーションが必要になる。cdk は bootstrap したときに CDKToolkit というスタックを生成する。cdk をアップグレードするというのはこのスタックの設定も更新する必要がある。デフォルト設定をそのまま使っていればマイグレーションはそんなに難しくはないはずだけど、設定をカスタマイズしていたりするといくつかパラメーターを調整したりしなかったりしてややこしいかもしれない。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.aws.amazon.com/cdk/v2/guide/migrating-v2.html">Migrating to AWS CDK v2&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/cdk/v2/guide/bootstrapping.html">Bootstrapping&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>また v2 は v1 の experimental な機能は移行されていないため、v1 のライブラリを直接使うか、自前でその機能を実装するといったことも必要になる可能性がある。&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="ja" dir="ltr">cdk v2 を使っていて v1 にある機能が v2 になくてあれー？って感じで調べてたら experimental な機能はまだ移行されてないらしい。&lt;a href="https://t.co/SbRZ5ddrTj">https://t.co/SbRZ5ddrTj&lt;/a>&lt;/p>&amp;mdash; Tetsuya Morimoto (@t2y) &lt;a href="https://twitter.com/t2y/status/1511924087450640386?ref_src=twsrc%5Etfw">April 7, 2022&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>例えば、v1 の apigwv2.VpcLink というメソッドは experimental で v2 に移行されていないため、v2 に移行されている stable な CfnVpcLink という機能を使って次のように実装した。これは v1 の cdk の実装をみて同じように実装しただけ。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/pull/10531/files#diff-1cf3aaf7b2b6b2e72123b93cb3108eb9e9a3291e588d62eb2cd34dd0509d3045">https://github.com/aws/aws-cdk/pull/10531/files#diff-1cf3aaf7b2b6b2e72123b93cb3108eb9e9a3291e588d62eb2cd34dd0509d3045&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-diff" data-lang="diff">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- const apiGwVpcLink = new apigwv2.VpcLink(this, &amp;#39;ApiGwVpcLink&amp;#39;, {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- vpc: vpc,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- vpcLinkName: &amp;#39;my-vpc-link&amp;#39;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- securityGroups: [mySecurityGroup]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&lt;/span>&lt;span style="color:#a6e22e">+ const apiGwVpcLink = new apigwv2.CfnVpcLink(this, &amp;#39;ApiGwVpcLink&amp;#39;, {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">+ name: &amp;#39;my-vpc-link&amp;#39;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">+ subnetIds: vpc.privateSubnets.map(sb =&amp;gt; sb.subnetId),
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">+ securityGroupIds: [mySecurityGroup.securityGroupId]
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>ecs の draining とタスクの停止時間</title><link>/diary/posts/2022/0406/</link><pubDate>Wed, 06 Apr 2022 06:30:16 +0900</pubDate><guid>/diary/posts/2022/0406/</guid><description>0時に寝て4時に起きた。なんか起きてから sns のタイムラインを眺めてた。6時半にはオフィスについて cdk のコードを読み始めた。
ecs の draining に時間がかかる？ cdk でインフラのデプロイをしていて、ecs のタスクの置き換えにやたら時間がかかっているのに気付いた。調べてみると、aws のドキュメントがすぐにヒットした。デフォルトでは停止するまでに5分ぐらいかかってしまうようだけど、それを調整したかったらいくつかパラメーターがある。
コンテナインスタンスが DRAINING に設定されているときに、Amazon ECS タスクが停止するのに時間がかかるトラブルシューティング方法を教えてください。 ecs サービスの deployment configuration minimumHealthyPercent: 同時に停止できるタスクの割合設定 maximumPercent: draining されるタスクが停止するまで置き換えるタスクを開始するかどうかの設定？ ロードバランサーの deregistration delay deregistrationDelay: elb(nlb) が登録解除処理が完了するまでに待つ時間。タスクが draining の状態になってこの時間が過ぎた後に登録解除して target が未使用になる ecs タスク定義の stop timeout stopTimeout: コンテナーが正常終了しないときに ecs が強制的にプロセスを kill するまでの待ち時間 それぞれのインフラの状況にあわせて適切なパラメーターを変更すればよい。私が管理しているのは次の2つを変更した。
maximumPercent: 100 -&amp;gt; 200 (%) deregistrationDelay: 300 -&amp;gt; 30 (秒) これで18分ほどかかっていたデプロイ時間を8分ぐらいまで短縮できた。テスト環境の設定なので多少のエラーが発生したとしても速い方がよい。</description><content>&lt;p>0時に寝て4時に起きた。なんか起きてから sns のタイムラインを眺めてた。6時半にはオフィスについて cdk のコードを読み始めた。&lt;/p>
&lt;h2 id="ecs-の-draining-に時間がかかる">ecs の draining に時間がかかる？&lt;/h2>
&lt;p>cdk でインフラのデプロイをしていて、ecs のタスクの置き換えにやたら時間がかかっているのに気付いた。調べてみると、aws のドキュメントがすぐにヒットした。デフォルトでは停止するまでに5分ぐらいかかってしまうようだけど、それを調整したかったらいくつかパラメーターがある。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aws.amazon.com/jp/premiumsupport/knowledge-center/ecs-tasks-stop-delayed-draining/">コンテナインスタンスが DRAINING に設定されているときに、Amazon ECS タスクが停止するのに時間がかかるトラブルシューティング方法を教えてください。&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="ecs-サービスの-deployment-configuration">ecs サービスの deployment configuration&lt;/h4>
&lt;ul>
&lt;li>minimumHealthyPercent: 同時に停止できるタスクの割合設定&lt;/li>
&lt;li>maximumPercent: draining されるタスクが停止するまで置き換えるタスクを開始するかどうかの設定？&lt;/li>
&lt;/ul>
&lt;h4 id="ロードバランサーの-deregistration-delay">ロードバランサーの deregistration delay&lt;/h4>
&lt;ul>
&lt;li>deregistrationDelay: elb(nlb) が登録解除処理が完了するまでに待つ時間。タスクが draining の状態になってこの時間が過ぎた後に登録解除して target が未使用になる&lt;/li>
&lt;/ul>
&lt;h4 id="ecs-タスク定義の-stop-timeout">ecs タスク定義の stop timeout&lt;/h4>
&lt;ul>
&lt;li>stopTimeout: コンテナーが正常終了しないときに ecs が強制的にプロセスを kill するまでの待ち時間&lt;/li>
&lt;/ul>
&lt;p>それぞれのインフラの状況にあわせて適切なパラメーターを変更すればよい。私が管理しているのは次の2つを変更した。&lt;/p>
&lt;ul>
&lt;li>maximumPercent: 100 -&amp;gt; 200 (%)&lt;/li>
&lt;li>deregistrationDelay: 300 -&amp;gt; 30 (秒)&lt;/li>
&lt;/ul>
&lt;p>これで18分ほどかかっていたデプロイ時間を8分ぐらいまで短縮できた。テスト環境の設定なので多少のエラーが発生したとしても速い方がよい。&lt;/p></content></item><item><title>再びのインフラエンジニア</title><link>/diary/posts/2022/0405/</link><pubDate>Tue, 05 Apr 2022 06:30:11 +0900</pubDate><guid>/diary/posts/2022/0405/</guid><description>0時に寝て7時に起きた。
インフラタスクに専念 本当はインフラ担当者が別途いるのだけど、多忙過ぎて、インフラタスクが1ヶ月近く遅延していて、プロジェクト内で合意を得て私がすべて巻き取ることにした。内容の如何に依らず、その一切合切をすべて巻き取ると宣言した。過去に働いた会社でも他の担当者ができなかった業務を後からリカバリするのはよくやってたのでそれ自体は構わない。ただ他人のタスクを肩代わりしても評価されないことも多くて、もともと私のタスクではないから誰がやったかなんか忘れてしまうんよね。私もとくにアピールしないからそう認識されても構わないのだけど、そういう業務が増えてくるとその職場を辞めるきっかけにもなってた。
インフラ担当者や他の社員さんにヒアリングしながら現時点でも十数個のタスクがある。過去のインフラの負債も含めて2-3週間ぐらい、私が集中的にやればすべて片がつくのではないかと考えている。今日たまたまスクラムのリファインメントやってて、業務の人から他の機能開発が遅れているのに2-3週間もインフラ作業に専念するってどういうこと？インフラタスクってインフラ担当者にやってもらえないんですか？と質問を受けて、できるんならその方が望ましいけど、過去の実績からまったく進捗しないのでこちらでやるざるを得ない状況というのを説明した。業務の人からみたらインフラなんか何をやっているかわからないからそんなもんよね。これから2-3週間経って蓄積したインフラタスクをすべて解決した後で少し時間が経つとインフラ担当者が全部やったように外部からはみえてしまうというのを、過去に何度も経験した。</description><content>&lt;p>0時に寝て7時に起きた。&lt;/p>
&lt;h2 id="インフラタスクに専念">インフラタスクに専念&lt;/h2>
&lt;p>本当はインフラ担当者が別途いるのだけど、多忙過ぎて、インフラタスクが1ヶ月近く遅延していて、プロジェクト内で合意を得て私がすべて巻き取ることにした。内容の如何に依らず、その一切合切をすべて巻き取ると宣言した。過去に働いた会社でも他の担当者ができなかった業務を後からリカバリするのはよくやってたのでそれ自体は構わない。ただ他人のタスクを肩代わりしても評価されないことも多くて、もともと私のタスクではないから誰がやったかなんか忘れてしまうんよね。私もとくにアピールしないからそう認識されても構わないのだけど、そういう業務が増えてくるとその職場を辞めるきっかけにもなってた。&lt;/p>
&lt;p>インフラ担当者や他の社員さんにヒアリングしながら現時点でも十数個のタスクがある。過去のインフラの負債も含めて2-3週間ぐらい、私が集中的にやればすべて片がつくのではないかと考えている。今日たまたまスクラムのリファインメントやってて、業務の人から他の機能開発が遅れているのに2-3週間もインフラ作業に専念するってどういうこと？インフラタスクってインフラ担当者にやってもらえないんですか？と質問を受けて、できるんならその方が望ましいけど、過去の実績からまったく進捗しないのでこちらでやるざるを得ない状況というのを説明した。業務の人からみたらインフラなんか何をやっているかわからないからそんなもんよね。これから2-3週間経って蓄積したインフラタスクをすべて解決した後で少し時間が経つとインフラ担当者が全部やったように外部からはみえてしまうというのを、過去に何度も経験した。&lt;/p></content></item></channel></rss>