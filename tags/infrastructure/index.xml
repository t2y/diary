<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Infrastructure on</title><link>/diary/tags/infrastructure/</link><description>Recent content in Infrastructure on</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 21 Nov 2024 14:23:04 +0900</lastBuildDate><atom:icon>/diary/favicon.ico</atom:icon><icon>/diary/favicon.ico</icon><atom:link href="/diary/tags/infrastructure/index.xml" rel="self" type="application/rss+xml"/><item><title>ipv6 による疎通検証</title><link>/diary/posts/2024/1121/</link><pubDate>Thu, 21 Nov 2024 14:23:04 +0900</pubDate><guid>/diary/posts/2024/1121/</guid><description>今日は神戸へ帰ってきてバテバテでしんどかったのでバドミントン練習はおやすみ。出張で体調を崩してしんどい。
ipv6 とリバースプロキシと xff ヘッダーの扱い 要件の1つに ipv6 での通信ができることという項目がある。OSI参照モデル の概念から言うと ipv6 は第3層であるネットワーク層の話しになる。実際に世の中で運用されている tcp/ip のプロトコルスタックにおいてもネットワーク層の話しであり、レイヤーが異なることからアプリケーション層では影響を受けないはずではある。アプリケーション層からみたら ipv4 であろうと ipv6 であろうと、ネットワーク周りのライブラリやフレームワークが対応していれば問題ないだろうと考えていた。それ自体の認識は誤っていない。
プロキシを経由するときに X-Forwarded-For (以下 xff) ヘッダーをセットすると、そのプロキシへアクセスしてきたリクエスト元の ip アドレスを保持できる。api サーバーでは xff ヘッダーを参照すれば ipv4 または ipv6 でアクセスしてきたクライアントの ip アドレスがわかる。フレームワークの echo における対応 も過去に行っていた。1つ対応漏れがあって xff ヘッダーはプロキシを経由するごとに途中の ip アドレスを追加していく。原則として信頼できるネットワークのアクセス元の ip アドレスを使う。例えば、次のような xff ヘッダーを考える。
X-Forwarded-For: 203.0.113.3, 192.0.2.5, 198.51.100.7&amp;#34; このとき信頼できるネットワークが 198.51.100.0/24 である場合は xff ヘッダーによるクライアントの ip アドレスは 192.0.2.5 となる。信頼できるネットワークが 192.0.2.0/24 と 198.51.100.0/24 の2つである場合は 203.0.113.3 の ip アドレスを使う。基本的には信頼できるネットワークの左側にある ip アドレスを使うと考えればよい。一方で途中経路のネットワークを知っていて信頼できるネットワークであることを設定しないと、意図したクライアントの ip アドレスを取得することはできない。
さらにリバースプロキシでアクセス制限をしたいという要件がある。docker compose を使うと通常は NAT の構成となり、コンテナネットワークのリバースプロキシ (nginx) からホスト os のリクエスト元の ip アドレスを参照できない。コンテナネットワークのゲートウェイ (172.</description><content>&lt;p>今日は神戸へ帰ってきてバテバテでしんどかったのでバドミントン練習はおやすみ。出張で体調を崩してしんどい。&lt;/p>
&lt;h2 id="ipv6-とリバースプロキシと-xff-ヘッダーの扱い">ipv6 とリバースプロキシと xff ヘッダーの扱い&lt;/h2>
&lt;p>要件の1つに ipv6 での通信ができることという項目がある。&lt;a href="https://ja.wikipedia.org/wiki/OSI%E5%8F%82%E7%85%A7%E3%83%A2%E3%83%87%E3%83%AB">OSI参照モデル&lt;/a> の概念から言うと ipv6 は第3層であるネットワーク層の話しになる。実際に世の中で運用されている tcp/ip のプロトコルスタックにおいてもネットワーク層の話しであり、レイヤーが異なることからアプリケーション層では影響を受けないはずではある。アプリケーション層からみたら ipv4 であろうと ipv6 であろうと、ネットワーク周りのライブラリやフレームワークが対応していれば問題ないだろうと考えていた。それ自体の認識は誤っていない。&lt;/p>
&lt;p>プロキシを経由するときに &lt;a href="https://developer.mozilla.org/ja/docs/Web/HTTP/Headers/X-Forwarded-For">X-Forwarded-For&lt;/a> (以下 xff) ヘッダーをセットすると、そのプロキシへアクセスしてきたリクエスト元の ip アドレスを保持できる。api サーバーでは xff ヘッダーを参照すれば ipv4 または ipv6 でアクセスしてきたクライアントの ip アドレスがわかる。&lt;a href="/diary/diary/posts/2024/0901/#x-forwarded-for-ヘッダーの制御">フレームワークの echo における対応&lt;/a> も過去に行っていた。1つ対応漏れがあって xff ヘッダーはプロキシを経由するごとに途中の ip アドレスを追加していく。原則として信頼できるネットワークのアクセス元の ip アドレスを使う。例えば、次のような xff ヘッダーを考える。&lt;/p>
&lt;pre tabindex="0">&lt;code>X-Forwarded-For: 203.0.113.3, 192.0.2.5, 198.51.100.7&amp;#34;
&lt;/code>&lt;/pre>&lt;p>このとき信頼できるネットワークが 198.51.100.0/24 である場合は xff ヘッダーによるクライアントの ip アドレスは 192.0.2.5 となる。信頼できるネットワークが 192.0.2.0/24 と 198.51.100.0/24 の2つである場合は 203.0.113.3 の ip アドレスを使う。基本的には信頼できるネットワークの左側にある ip アドレスを使うと考えればよい。一方で途中経路のネットワークを知っていて信頼できるネットワークであることを設定しないと、意図したクライアントの ip アドレスを取得することはできない。&lt;/p>
&lt;p>さらにリバースプロキシでアクセス制限をしたいという要件がある。docker compose を使うと通常は NAT の構成となり、コンテナネットワークのリバースプロキシ (nginx) からホスト os のリクエスト元の ip アドレスを参照できない。コンテナネットワークのゲートウェイ (172.18.0.1) からアクセスを受けたようにみえる。この振る舞い自体も正しくはあるが、調査したところ、docker の &lt;a href="https://docs.docker.com/engine/security/rootless/">rootless&lt;/a>　モードだとリクエスト元の ip アドレスを参照できないということがわかった。次の issue によると port forwarder と呼ばれるモジュールがあり、デフォルトものから &lt;a href="https://github.com/rootless-containers/slirp4netns">slirp4netns&lt;/a> に変更すれば参照できると次の issue で紹介されていた。&lt;/p>
&lt;pre tabindex="0">&lt;code>Environment=&amp;#34;DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=slirp4netns&amp;#34;
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>&lt;a href="https://github.com/moby/moby/issues/41789">Rootless mode overwrites public IP to Docker&amp;rsquo;s IP #41789&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://rootlesscontaine.rs/getting-started/docker/#changing-the-port-forwarder">Changing the port forwarder&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>実際にやってみて ipv4 の ip アドレスを参照することはできたものの ipv6 は未対応らしい。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/rootless-containers/slirp4netns/issues/253">Support IPv6 port forwarding (libslirp 4.5) #253&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>他のやり方も調査してコンテナネットワーク内の nginx から ipv6 の ip アドレスを参照することができなかった。要件を満たせないことからリバースプロキシをコンテナネットワークから外出しして構築することに決めた。rootless モードでなければ ipv4/ipv6 の両方を取得できるという話しもお手伝い先の同僚から聞いた。こんなところではまるとは思わなかった。&lt;/p>
&lt;p>ping や curl コマンドも ipv6 対応されていてオプションを付けなくてもよいけど、ipv6 であることを明示する上では &lt;code>-6&lt;/code> とオプションを付けてもよいかもしれない。それにしても検証するときに ipv6 アドレスを手打ちするのは煩雑なのでいちいちアドレスをコピペすることになって面倒だなと感じた。&lt;/p>
&lt;pre tabindex="0">&lt;code>$ ping -6 2001:DB8:0:0:8:800:200C:417A
$ curl -s -6 &amp;#39;http://[2001:DB8:0:0:8:800:200C:417A]:8080/api&amp;#39;
&lt;/code>&lt;/pre></content></item><item><title>ミドルウェアのコンテナの振る舞い検証</title><link>/diary/posts/2024/1107/</link><pubDate>Thu, 07 Nov 2024 12:28:46 +0900</pubDate><guid>/diary/posts/2024/1107/</guid><description>今日もバドミントン練習はお休み。
mongodb の healthcheck bitnami/mongodb というサードパーティのコンテナ を使って mongodb サービスを設定している。docker compose でコンテナサービスの依存関係を記述できるが、特別な設定をしないとコンテナサービスの起動をトリガーに依存関係を制御する。実際はコンテナが起動して内部のサーバー／デーモンが正常に起動するまで少し時間がかかる。たとえば mongodb のコンテナであれば mongod デーモンに初期設定をして再起動したりといった処理を内部的に行っている。そんなときに healthcheck を使うことで実際に mongod デーモンに接続できるかどうかでコンテナのサービス間の依存関係を制御できる。
これまで mongodb には healthcheck の設定をしていなかったので調査して次の設定を追加した。
healthcheck: test: mongosh &amp;#34;mongodb://localhost:37017/test?directConnection=false&amp;amp;replicaSet=${MONGO_REPLICA_SET}&amp;#34; --eval &amp;#39;db.runCommand(&amp;#34;ping&amp;#34;).ok&amp;#39; --quiet interval: 60s timeout: 5s retries: 3 start_period: 30s start_interval: 3s mongosh で db に接続して ping を実行するだけなら認証は必要ない。mongosh でなにもパラメーターを指定せずに接続すると direct 接続になってしまう。replica set の設定が完了していることを検証するために replica set 接続にしている。また interval は起動中もずっと死活監視に test コマンドを実行している。それとは別に start_interval を指定することでサービス開始時と通常の運用時の test コマンドによる制御をわけて管理できる。
rabbitmq のアップグレード 19時過ぎに業務終了報告をして、帰ろうと思ったときにふと rabbitmq のバージョンを最近あまり確認していないことに気付いた。いま 3.12.14 を使っているが、Release Information をみるとコミュニティサポートは切れていて、現行バージョンは 4.</description><content>&lt;p>今日もバドミントン練習はお休み。&lt;/p>
&lt;h2 id="mongodb-の-healthcheck">mongodb の healthcheck&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2023/1214/#コンテナを使ったレプリカセットの初期設定">bitnami/mongodb というサードパーティのコンテナ&lt;/a> を使って mongodb サービスを設定している。docker compose でコンテナサービスの依存関係を記述できるが、特別な設定をしないとコンテナサービスの起動をトリガーに依存関係を制御する。実際はコンテナが起動して内部のサーバー／デーモンが正常に起動するまで少し時間がかかる。たとえば mongodb のコンテナであれば mongod デーモンに初期設定をして再起動したりといった処理を内部的に行っている。そんなときに &lt;a href="https://docs.docker.com/reference/dockerfile/#healthcheck">healthcheck&lt;/a> を使うことで実際に mongod デーモンに接続できるかどうかでコンテナのサービス間の依存関係を制御できる。&lt;/p>
&lt;p>これまで mongodb には healthcheck の設定をしていなかったので調査して次の設定を追加した。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yml" data-lang="yml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">healthcheck&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">test&lt;/span>: &lt;span style="color:#ae81ff">mongosh &amp;#34;mongodb://localhost:37017/test?directConnection=false&amp;amp;replicaSet=${MONGO_REPLICA_SET}&amp;#34; --eval &amp;#39;db.runCommand(&amp;#34;ping&amp;#34;).ok&amp;#39; --quiet&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">interval&lt;/span>: &lt;span style="color:#ae81ff">60s&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">timeout&lt;/span>: &lt;span style="color:#ae81ff">5s&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">retries&lt;/span>: &lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">start_period&lt;/span>: &lt;span style="color:#ae81ff">30s&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">start_interval&lt;/span>: &lt;span style="color:#ae81ff">3s&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>mongosh で db に接続して ping を実行するだけなら認証は必要ない。mongosh でなにもパラメーターを指定せずに接続すると direct 接続になってしまう。replica set の設定が完了していることを検証するために replica set 接続にしている。また interval は起動中もずっと死活監視に test コマンドを実行している。それとは別に start_interval を指定することでサービス開始時と通常の運用時の test コマンドによる制御をわけて管理できる。&lt;/p>
&lt;h2 id="rabbitmq-のアップグレード">rabbitmq のアップグレード&lt;/h2>
&lt;p>19時過ぎに業務終了報告をして、帰ろうと思ったときにふと rabbitmq のバージョンを最近あまり確認していないことに気付いた。いま 3.12.14 を使っているが、&lt;a href="https://www.rabbitmq.com/release-information">Release Information&lt;/a> をみるとコミュニティサポートは切れていて、現行バージョンは 4.0 になっていることに気付いた。試しに結合テストの rabbitmq のバージョンを 4.0.3 に上げてみたところ、問題なく動作している。テスト環境の移行は他のメンバーが使っていない夜にやった方がいいかと帰ることをやめて普通に移行作業をやり始めてしまった。メッセージキューは永続化したデータを基本的には保持しないため、メジャーバージョンアップで互換性がなかったとしても volume 配下のデータを削除して exchange/queue を移行すればよい。&lt;/p>
&lt;p>rabbitmq の http api client として rabbit-hole というツールを使っている。それも v2 から v3 へアップグレードしていて &lt;a href="https://github.com/michaelklishin/rabbit-hole/releases/tag/v3.1.0">Changes Between 2.16.0 and 3.1.0 (Oct 31, 2024)&lt;/a> に書いてあるが、機能的な変更も非互換の変更もいまのところはないが、4.0 にあわせて将来的に非互換な変更をやりやすいよう、メジャーバージョンを上げると書いてある。go.mod の依存関係も更新したりした。&lt;/p>
&lt;p>19時過ぎに帰ろうと思ってから、なんやらかんやらしているうちに最終的には21時半まで作業していた。&lt;/p></content></item><item><title>sveltekit の base path 設定</title><link>/diary/posts/2024/1106/</link><pubDate>Wed, 06 Nov 2024 08:56:02 +0900</pubDate><guid>/diary/posts/2024/1106/</guid><description>昨日遅くまで作業していたせいか、体調があまりよくなくて19時でお仕事を終えて帰って休んでいた。21時過ぎにはベッドに入って寝てた。
base path 移行とビルド リバースプロキシのルーティング検証 を終えて path based routing を採用することに決まった。そのため、nginx の設定にあわせて sveltekit の ui の base path を設定し直し、テスト環境にデプロイして検証していた。デプロイ先の制約によって base path が変わるというのはよくある状況なので sveltekit も Configuration paths で svelte.config.js に base path が設定できるようになっている。当初は環境変数で切り替えできるように設定して修正したものの、実際にビルドしてコンテナでテスト環境へデプロイしてみると有効にならない。adapter-node を使ってビルドしたソースコードを調べてみると svelte.config.js に設定した値がリテラルで埋め込まれていることがわかった。node.js サーバーの起動時に環境変数などを参照して動的に設定することはできない。次の issue が登録されている。
make adapter-node base path configurable via environment variable #7242 ビルド時にパスを固定にしないと、他のスクリプトソースやアセットの管理で煩雑になるところがあるのだろうと推測する。本当は環境変数で起動時に動的に変更できると、お客さんの環境にあわせて base path の値を変えたりもできるが、現状ではこちらが決めた値を固定で使ってもらうしかないことがわかった。</description><content>&lt;p>昨日遅くまで作業していたせいか、体調があまりよくなくて19時でお仕事を終えて帰って休んでいた。21時過ぎにはベッドに入って寝てた。&lt;/p>
&lt;h2 id="base-path-移行とビルド">base path 移行とビルド&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2024/1030/#nginx-のリバースプロキシ">リバースプロキシのルーティング検証&lt;/a> を終えて path based routing を採用することに決まった。そのため、nginx の設定にあわせて sveltekit の ui の base path を設定し直し、テスト環境にデプロイして検証していた。デプロイ先の制約によって base path が変わるというのはよくある状況なので sveltekit も &lt;a href="https://svelte.dev/docs/kit/configuration#paths">Configuration paths&lt;/a> で &lt;code>svelte.config.js&lt;/code> に base path が設定できるようになっている。当初は環境変数で切り替えできるように設定して修正したものの、実際にビルドしてコンテナでテスト環境へデプロイしてみると有効にならない。adapter-node を使ってビルドしたソースコードを調べてみると &lt;code>svelte.config.js&lt;/code> に設定した値がリテラルで埋め込まれていることがわかった。node.js サーバーの起動時に環境変数などを参照して動的に設定することはできない。次の issue が登録されている。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/sveltejs/kit/issues/7242">make adapter-node base path configurable via environment variable #7242&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>ビルド時にパスを固定にしないと、他のスクリプトソースやアセットの管理で煩雑になるところがあるのだろうと推測する。本当は環境変数で起動時に動的に変更できると、お客さんの環境にあわせて base path の値を変えたりもできるが、現状ではこちらが決めた値を固定で使ってもらうしかないことがわかった。&lt;/p></content></item><item><title>nginx の設定調査</title><link>/diary/posts/2024/1030/</link><pubDate>Wed, 30 Oct 2024 08:14:16 +0900</pubDate><guid>/diary/posts/2024/1030/</guid><description>開発合宿の準備をしていて今日のバドミントン練習はお休み。
nginx のリバースプロキシ compose に起動している2つのサービスを同じポート番号で共有したい。nginx を tls 終端にしていてリバースプロキシとして構築している。ルーティングするには2つの方法がある。次のような compose.yml を作ってローカルのファイルシステムをマウントして設定変更しながら検証する。
services: proxy: container_name: proxy image: docker.io/library/nginx:stable ports: - 8443:8443 network_mode: &amp;#34;host&amp;#34; restart: unless-stopped volumes: - ./nginx:/etc/nginx sub-domain based routing クラウドでは普通のやり方がサブドメインのホスト名でルーティングを行う。設定もシンプルでファイルも管理しやすいように分割できてよいと思える。システム変更時の移行も名前を切り替えればよいので移行しやすい。
nginx.conf http { sendfile on; upstream web-api1 { server localhost:8801; } upstream web-api2 { server localhost:8802; } ssl_certificate /etc/nginx/ssl/sample.crt; ssl_certificate_key /etc/nginx/ssl/sample.key; include /etc/nginx/sites-enabled/*; } nginx/sites-enabled/www.sub1.example.com server { listen 8443 ssl; server_name sub1.example.com; location / { include /etc/nginx/common_proxy.conf; proxy_pass http://web-api1; } } path based routing サブドメインの方が私は好みではあるが、サブドメインをネームサーバーに登録したり、tls の証明書にも複数ホストの考慮が必要になってくる。パスでルーティングするなら1台のマシンのように仮想的にみせられるというメリットはある。</description><content>&lt;p>開発合宿の準備をしていて今日のバドミントン練習はお休み。&lt;/p>
&lt;h2 id="nginx-のリバースプロキシ">nginx のリバースプロキシ&lt;/h2>
&lt;p>compose に起動している2つのサービスを同じポート番号で共有したい。nginx を tls 終端にしていてリバースプロキシとして構築している。ルーティングするには2つの方法がある。次のような compose.yml を作ってローカルのファイルシステムをマウントして設定変更しながら検証する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yml" data-lang="yml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">services&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">proxy&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">container_name&lt;/span>: &lt;span style="color:#ae81ff">proxy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">docker.io/library/nginx:stable&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">8443&lt;/span>:&lt;span style="color:#ae81ff">8443&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">network_mode&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;host&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">restart&lt;/span>: &lt;span style="color:#ae81ff">unless-stopped&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumes&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">./nginx:/etc/nginx&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="sub-domain-based-routing">sub-domain based routing&lt;/h3>
&lt;p>クラウドでは普通のやり方がサブドメインのホスト名でルーティングを行う。設定もシンプルでファイルも管理しやすいように分割できてよいと思える。システム変更時の移行も名前を切り替えればよいので移行しやすい。&lt;/p>
&lt;ul>
&lt;li>nginx.conf&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>http {
sendfile on;
upstream web-api1 {
server localhost:8801;
}
upstream web-api2 {
server localhost:8802;
}
ssl_certificate /etc/nginx/ssl/sample.crt;
ssl_certificate_key /etc/nginx/ssl/sample.key;
include /etc/nginx/sites-enabled/*;
}
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>nginx/sites-enabled/www.sub1.example.com&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>server {
listen 8443 ssl;
server_name sub1.example.com;
location / {
include /etc/nginx/common_proxy.conf;
proxy_pass http://web-api1;
}
}
&lt;/code>&lt;/pre>&lt;h3 id="path-based-routing">path based routing&lt;/h3>
&lt;p>サブドメインの方が私は好みではあるが、サブドメインをネームサーバーに登録したり、tls の証明書にも複数ホストの考慮が必要になってくる。パスでルーティングするなら1台のマシンのように仮想的にみせられるというメリットはある。&lt;/p>
&lt;pre tabindex="0">&lt;code>http {
sendfile on;
upstream web-api1 {
server localhost:8801;
}
upstream web-api2 {
server localhost:8802;
}
ssl_certificate /etc/nginx/ssl/sample.crt;
ssl_certificate_key /etc/nginx/ssl/sample.key;
server {
listen 8443 ssl;
location /app1/ {
include /etc/nginx/common_proxy.conf;
proxy_pass http://web-api1;
}
location /app2/ {
include /etc/nginx/common_proxy.conf;
proxy_pass http://web-api2;
}
}
}
&lt;/code>&lt;/pre>&lt;h2 id="コンテナの-capability">コンテナの capability&lt;/h2>
&lt;p>docker compose を rootless mode で使っていて 443 ポートを使いたいと言われてどうしたらいいのだろう？といままで考えていないことに気付いた。調べたらすぐにやり方が書いてあった。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.docker.com/engine/security/rootless/#exposing-privileged-ports">Exposing privileged ports&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ sudo setcap cap_net_bind_service&lt;span style="color:#f92672">=&lt;/span>ep &lt;span style="color:#66d9ef">$(&lt;/span>which rootlesskit&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ systemctl --user restart docker
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>これだけで compose サービスの1つにポート設定できた。めちゃ簡単だった。いままで capability を設定したことがなかったのと、権限周りはややこしいという先入観もあって触る機会がなかった。いろいろ洗練されて抽象化されているのだと推測する。バックエンドの場合は任意のポート番号を使えばよいから capability の設定をして 1024 以下のポート番号を使わないといけない理由はあまりない気はするが、そういう設定もできるようにはみえる。そういう話題すら聞いたことがなかった。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container">Set capabilities for a Container&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>exec とスクリプト</title><link>/diary/posts/2024/1018/</link><pubDate>Fri, 18 Oct 2024 11:19:20 +0900</pubDate><guid>/diary/posts/2024/1018/</guid><description>今日のバドミントン練習はエアシャトルでリフティングを60分した。連続最大回数は191回だった。もう少しで200回だったのに残念。木曜日は睡眠をたくさんとって疲れは少し取れたし、安定的に50回前後は続くようになりつつも、100回までに失敗してしまう。今日は100回を超えたのが2回だけだった。ラケットのスィートスポットでとらえたときにきれいに真上にあがる感覚が楽しい。うまくいくときは数回は続く。それが自然にできるときとそうじゃないときの違いを私は制御できてなくて言語化もできない。
エアシャトルとメイビスにおけるリフティングの違いを比べてみると、メイビスの方が打ち上げて落ちてくるときにあまり回転せずコルクが下を向く傾向が多いようにみえる。エアシャトルの方がコルクが重い分、縦方向に回転し始めるとその回転が止まらず、回転しているからラケット面でとらえるのが難しくなる。だからエアシャトルの方がメイビスよりもリフティングが難しいといえる。シャトルを高く打ち上げると、落下してくる距離が長くなりその回転が落ち着く傾向があるからリフティングしやすくなるのではないかと仮説を考えた。伸び悩みかもしれないし、地道に練習を継続するときかもしれない。
exec とエントリーポイントのスクリプト コンテナを起動して stop すると SIGTERM が送られる。そのときに api サーバーでシグナルの処理をしているのに、気付いたらシグナル処理が行われずタイムアウトするようになっていた。デフォルトでは10秒でタイムアウトして強制終了となる。なぜシグナルを捕捉しなくなったかを調査したら、あるときサーバーの起動前に前処理が必要になってエントリーポイントをシェルスクリプトにしていた。そのときに exec しないと、シェルスクリプトのプロセスに対してシグナルが送られるため、api サーバーがシグナルを検知できなくなるという副作用があることに気付いた。これまでも exec を使うとプロセス ID は変更されないという知識を知っていたが、それがどういう状況で役に立つかを理解できていなかった。シグナルを用いた同期処理に exec が役に立つ状況があることを学んだ。修正は次の1行のみ。
--- a/docker/entrypoint.sh +++ b/docker/entrypoint.sh @@ -2,4 +2,4 @@ ... ... (pre process) ... -./bin/api &amp;#34;$@&amp;#34; +exec ./bin/api &amp;#34;$@&amp;#34; go test からバイナリをビルドしてサーバーを起動する 先日 結合テスト向けカバレッジ計測の調査 をした成果を使って実際に go test からカバレッジ計測のカスタマイズを施したバイナリをビルドしてサーバー起動するコードを書いてみた。やや手間取ったが、一通り動いてカバレッジを計測できた。例えば、単体テストのカバレッジを計測するための makefile のターゲットは次のようになる。
GO_COVER_DIR:=$(CURDIR)/tests/coverage coverage: @mkdir -p $(GO_COVER_DIR) go test -tags=integration -race -cover ./... -covermode atomic -args -test.gocoverdir=$(GO_COVER_DIR) go は fork ができない。fork の代わりに exec を使う。How do I fork a go process?</description><content>&lt;p>今日のバドミントン練習はエアシャトルでリフティングを60分した。連続最大回数は191回だった。もう少しで200回だったのに残念。木曜日は睡眠をたくさんとって疲れは少し取れたし、安定的に50回前後は続くようになりつつも、100回までに失敗してしまう。今日は100回を超えたのが2回だけだった。ラケットのスィートスポットでとらえたときにきれいに真上にあがる感覚が楽しい。うまくいくときは数回は続く。それが自然にできるときとそうじゃないときの違いを私は制御できてなくて言語化もできない。&lt;/p>
&lt;p>エアシャトルとメイビスにおけるリフティングの違いを比べてみると、メイビスの方が打ち上げて落ちてくるときにあまり回転せずコルクが下を向く傾向が多いようにみえる。エアシャトルの方がコルクが重い分、縦方向に回転し始めるとその回転が止まらず、回転しているからラケット面でとらえるのが難しくなる。だからエアシャトルの方がメイビスよりもリフティングが難しいといえる。シャトルを高く打ち上げると、落下してくる距離が長くなりその回転が落ち着く傾向があるからリフティングしやすくなるのではないかと仮説を考えた。伸び悩みかもしれないし、地道に練習を継続するときかもしれない。&lt;/p>
&lt;h2 id="exec-とエントリーポイントのスクリプト">exec とエントリーポイントのスクリプト&lt;/h2>
&lt;p>コンテナを起動して stop すると SIGTERM が送られる。そのときに api サーバーでシグナルの処理をしているのに、気付いたらシグナル処理が行われずタイムアウトするようになっていた。デフォルトでは10秒でタイムアウトして強制終了となる。なぜシグナルを捕捉しなくなったかを調査したら、あるときサーバーの起動前に前処理が必要になってエントリーポイントをシェルスクリプトにしていた。そのときに exec しないと、シェルスクリプトのプロセスに対してシグナルが送られるため、api サーバーがシグナルを検知できなくなるという副作用があることに気付いた。これまでも &lt;a href="https://ja.wikipedia.org/wiki/Exec">exec&lt;/a> を使うとプロセス ID は変更されないという知識を知っていたが、それがどういう状況で役に立つかを理解できていなかった。シグナルを用いた同期処理に exec が役に立つ状況があることを学んだ。修正は次の1行のみ。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-diff" data-lang="diff">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">--- a/docker/entrypoint.sh
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&lt;/span>&lt;span style="color:#a6e22e">+++ b/docker/entrypoint.sh
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">&lt;/span>&lt;span style="color:#75715e">@@ -2,4 +2,4 @@
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ... (pre process)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">-./bin/api &amp;#34;$@&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&lt;/span>&lt;span style="color:#a6e22e">+exec ./bin/api &amp;#34;$@&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="go-test-からバイナリをビルドしてサーバーを起動する">go test からバイナリをビルドしてサーバーを起動する&lt;/h2>
&lt;p>先日 &lt;a href="/diary/diary/posts/2024/1011/#go-の結合テスト向けカバレッジ計測の考察">結合テスト向けカバレッジ計測の調査&lt;/a> をした成果を使って実際に go test からカバレッジ計測のカスタマイズを施したバイナリをビルドしてサーバー起動するコードを書いてみた。やや手間取ったが、一通り動いてカバレッジを計測できた。例えば、単体テストのカバレッジを計測するための makefile のターゲットは次のようになる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-makefile" data-lang="makefile">&lt;span style="display:flex;">&lt;span>GO_COVER_DIR&lt;span style="color:#f92672">:=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>CURDIR&lt;span style="color:#66d9ef">)&lt;/span>/tests/coverage
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">coverage&lt;/span>&lt;span style="color:#f92672">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> @mkdir -p &lt;span style="color:#66d9ef">$(&lt;/span>GO_COVER_DIR&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> go test -tags&lt;span style="color:#f92672">=&lt;/span>integration -race -cover ./... -covermode atomic -args -test.gocoverdir&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>GO_COVER_DIR&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>go は &lt;a href="https://ja.wikipedia.org/wiki/Fork">fork&lt;/a> ができない。fork の代わりに exec を使う。&lt;a href="https://stackoverflow.com/a/28371586">How do I fork a go process?&lt;/a> に go の goroutine のスケジューリングと fork は相性が悪くてうまく動かないということが背景だと説明されている。それはともかく exec を使ってもサーバープロセスを非同期に起動できたのでそのスニペットを書いておく。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">binaryPath&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">buildBinary&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">args&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> []&lt;span style="color:#66d9ef">string&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;-verbose&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;-port&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">strconv&lt;/span>.&lt;span style="color:#a6e22e">Itoa&lt;/span>(&lt;span style="color:#a6e22e">ServerPort&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">r&lt;/span>, &lt;span style="color:#a6e22e">w&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">io&lt;/span>.&lt;span style="color:#a6e22e">Pipe&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">go&lt;/span> &lt;span style="color:#66d9ef">func&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">s&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">bufio&lt;/span>.&lt;span style="color:#a6e22e">NewScanner&lt;/span>(&lt;span style="color:#a6e22e">r&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> &lt;span style="color:#a6e22e">s&lt;/span>.&lt;span style="color:#a6e22e">Scan&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">fmt&lt;/span>.&lt;span style="color:#a6e22e">Println&lt;/span>(&lt;span style="color:#a6e22e">s&lt;/span>.&lt;span style="color:#a6e22e">Text&lt;/span>())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">cmd&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">exec&lt;/span>.&lt;span style="color:#a6e22e">Command&lt;/span>(&lt;span style="color:#a6e22e">binaryPath&lt;/span>, &lt;span style="color:#a6e22e">args&lt;/span>&lt;span style="color:#f92672">...&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">cmd&lt;/span>.&lt;span style="color:#a6e22e">Stdout&lt;/span> = &lt;span style="color:#a6e22e">w&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">cmd&lt;/span>.&lt;span style="color:#a6e22e">Start&lt;/span>(); &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">slog&lt;/span>.&lt;span style="color:#a6e22e">Error&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;failed to start api server&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;err&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">defer&lt;/span> &lt;span style="color:#66d9ef">func&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">cmd&lt;/span>.&lt;span style="color:#a6e22e">Process&lt;/span>.&lt;span style="color:#a6e22e">Signal&lt;/span>(&lt;span style="color:#a6e22e">syscall&lt;/span>.&lt;span style="color:#a6e22e">SIGTERM&lt;/span>); &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">slog&lt;/span>.&lt;span style="color:#a6e22e">Error&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;failed to terminate the api process&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;err&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">s&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">cmd&lt;/span>.&lt;span style="color:#a6e22e">Process&lt;/span>.&lt;span style="color:#a6e22e">Wait&lt;/span>(); &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">slog&lt;/span>.&lt;span style="color:#a6e22e">Error&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;failed to wait terminating the api process&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;err&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> } &lt;span style="color:#66d9ef">else&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">w&lt;/span>.&lt;span style="color:#a6e22e">Close&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">slog&lt;/span>.&lt;span style="color:#a6e22e">Info&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;completed to terminate the api process&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;s&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">s&lt;/span>.&lt;span style="color:#a6e22e">String&lt;/span>())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// サーバーに対するテストを実行
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>サーバープロセスの標準出力のログを &lt;a href="/diary/diary/posts/2024/0919/">io.Pipe&lt;/a> を使って出力することもできる。exec で生成したプロセスに対してもシグナルを送ったり終了を待つこともできる。デバッグしている分にはこれで意図したように制御できた。この知見は将来的に役に立つ気がする。0時過ぎから調査を再開して4時前ぐらいまでやっていた。少しはまって時間はかかったものの、久しぶりに集中してデバッグしていた。&lt;/p></content></item><item><title>寝台特急に初乗車</title><link>/diary/posts/2024/0205/</link><pubDate>Mon, 05 Feb 2024 10:07:23 +0900</pubDate><guid>/diary/posts/2024/0205/</guid><description>2時に寝て6時過ぎに起きた。本当はもっと早く寝た方がよいのに22時半から2時前まで作業してた。格闘家に学ぶ体脂肪コントロール 本に睡眠をとることや疲労を溜めないことが代謝をよくすると書いてあって、その通りだとは思うのだけど、なぜかここ最近は2時ぐらいまで作業していることが多い。いろいろうまくいかない。
今日の運動は腕立て,スクワット,背筋をした。統計を 運動の記録 にまとめる。
テスト環境の再構築 テスト環境の見直し の都築。先週末から cockpit 上の仮想マシンに OS をインストールしたりしていた。今日はテスト対象のアプリケーションをインストールして設定をしたり、既存のテスト環境を移行したりしていた。
いまテスト環境は1つしかないが、機能が増えてきて、複数の運用要件を満たす必要があって、1つの環境ですべてを検証するには不便な状態になっていた。そこで1つに全部入りするから相反する要件が競合するのであって、いまわかっている要件を用途別に、1つを3つの環境に分割した。1つしかないものを複数にすると、その重複するところのコストもかかる。その辺りの調整もしていた。テスト環境の構築について、過去に私が作ったものではあるけど、1回作って終わりだと思っていたのか、構築の issue にはなにか書いてあるかもしれないが、wiki には残していなかった。今回3つに作り直すにあたってドキュメントがあれば思い出したり、トライアンドエラーを行うコストも省略できて、ちゃんとドキュメントを書かないとあかんなと反省した。新規に設置した仮想マシンに openldap サーバーを構築する以外の作業は完了したはず。
サンライズ出雲の乗車と遅延 1ヶ月前に予約した 0時11分発の特急サンライズ出雲で東京へ向かう。
20時半にオフィスから家に帰ってきて、洗い物して、お風呂入って、ストレッチして、荷造りして、3時間もあれば余裕で出発できる。初めて寝台特急に乗る。間違いがないよう、改札で駅員に問い合わせる。すると、動物(シカ)と接触して列車が止まってしまっているという。再開予定ではあるが、駅員さんに話しかけた時点ではいつ再開するかはわかっておらず、その情報も乗客からオンラインでは調べられず、駅員さんしか状況を知る方法はないとのこと。動物とぶつかったぐらいで運休はないやろうという話しもあってそのままホームで待つことに。
ホームのアナウンスを聞いていると「運転しています。」と言い切るので再開したのかな？と期待するものの、35分遅れ→40分遅れ→45分遅れと遅延時間がどんどん増えていくので実際には停車していて点検中でも「運転している」とアナウンスするらしいということがわかった。最終的には80分遅れて三ノ宮に到着した。待合室で半分寝てたよ。そのまま目を覚さなくなるところでしたよ。
この時点で今日はゆっくり睡眠をとろうと思っていたのに大きな誤算だった。待ち疲れていたのもあったせいか、部屋に入って消灯してそのままわりとすぐに寝てしまった。列車に乗ったのが1時半、トイレに行って、駅員さんが切符確認に来て、2時頃には寝てしまったのではないかと思う。わりとよく眠れたと思う。列車の揺れと移動音は普通にはあるものの、私はこの手の環境には強いので、もちろん身体は疲労するかもしれないが、感覚的には気にせずそのまま寝ていたと思う。この状態で眠れるかどうかによって寝台特急の是非は変わってくるだろう。カプセルホテルよりは天井も高く、スペースも一回り広くて余裕がある。今回は下の真ん中ぐらいの部屋だったのだけど、場所によって揺れや移動音の聞こえ方も変わるという。通な人は部屋の位置もこだわるのかもしれない。
あと薄い毛布しかないため、冬はちょっと寒い。暖房入れて上着もかけてぎりぎりの暖かさかな。べつに眠れないほど寒いわけではないが、冬なんだからもう一枚布団を用意してくれてもいいのになという感覚はある。朝に起きて窓から線路は延々と眺められる。線路が好きな人にもよさそう。けっこうよかったのでまた来月も予約して寝台特急で出張へ行く。</description><content>&lt;p>2時に寝て6時過ぎに起きた。本当はもっと早く寝た方がよいのに22時半から2時前まで作業してた。&lt;a href="/diary/diary/posts/2024/0204/">格闘家に学ぶ体脂肪コントロール&lt;/a> 本に睡眠をとることや疲労を溜めないことが代謝をよくすると書いてあって、その通りだとは思うのだけど、なぜかここ最近は2時ぐらいまで作業していることが多い。いろいろうまくいかない。&lt;/p>
&lt;p>今日の運動は腕立て,スクワット,背筋をした。統計を &lt;a href="https://docs.google.com/spreadsheets/d/1bg85QtM-LciUgey8I79uI7vW2PEwsP6TVdeIRVkACBg/edit?usp=sharing">運動の記録&lt;/a> にまとめる。&lt;/p>
&lt;h2 id="テスト環境の再構築">テスト環境の再構築&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2024/0126/#テスト環境の見直し">テスト環境の見直し&lt;/a> の都築。先週末から &lt;a href="https://cockpit-project.org/">cockpit&lt;/a> 上の仮想マシンに OS をインストールしたりしていた。今日はテスト対象のアプリケーションをインストールして設定をしたり、既存のテスト環境を移行したりしていた。&lt;/p>
&lt;p>いまテスト環境は1つしかないが、機能が増えてきて、複数の運用要件を満たす必要があって、1つの環境ですべてを検証するには不便な状態になっていた。そこで1つに全部入りするから相反する要件が競合するのであって、いまわかっている要件を用途別に、1つを3つの環境に分割した。1つしかないものを複数にすると、その重複するところのコストもかかる。その辺りの調整もしていた。テスト環境の構築について、過去に私が作ったものではあるけど、1回作って終わりだと思っていたのか、構築の issue にはなにか書いてあるかもしれないが、wiki には残していなかった。今回3つに作り直すにあたってドキュメントがあれば思い出したり、トライアンドエラーを行うコストも省略できて、ちゃんとドキュメントを書かないとあかんなと反省した。新規に設置した仮想マシンに openldap サーバーを構築する以外の作業は完了したはず。&lt;/p>
&lt;h2 id="サンライズ出雲の乗車と遅延">サンライズ出雲の乗車と遅延&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2024/0108/#サンライズ出雲の予約">1ヶ月前に予約した&lt;/a> 0時11分発の特急サンライズ出雲で東京へ向かう。&lt;/p>
&lt;p>20時半にオフィスから家に帰ってきて、洗い物して、お風呂入って、ストレッチして、荷造りして、3時間もあれば余裕で出発できる。初めて寝台特急に乗る。間違いがないよう、改札で駅員に問い合わせる。すると、動物(シカ)と接触して列車が止まってしまっているという。再開予定ではあるが、駅員さんに話しかけた時点ではいつ再開するかはわかっておらず、その情報も乗客からオンラインでは調べられず、駅員さんしか状況を知る方法はないとのこと。動物とぶつかったぐらいで運休はないやろうという話しもあってそのままホームで待つことに。&lt;/p>
&lt;p>ホームのアナウンスを聞いていると「運転しています。」と言い切るので再開したのかな？と期待するものの、35分遅れ→40分遅れ→45分遅れと遅延時間がどんどん増えていくので実際には停車していて点検中でも「運転している」とアナウンスするらしいということがわかった。最終的には80分遅れて三ノ宮に到着した。待合室で半分寝てたよ。そのまま目を覚さなくなるところでしたよ。&lt;/p>
&lt;p>この時点で今日はゆっくり睡眠をとろうと思っていたのに大きな誤算だった。待ち疲れていたのもあったせいか、部屋に入って消灯してそのままわりとすぐに寝てしまった。列車に乗ったのが1時半、トイレに行って、駅員さんが切符確認に来て、2時頃には寝てしまったのではないかと思う。わりとよく眠れたと思う。列車の揺れと移動音は普通にはあるものの、私はこの手の環境には強いので、もちろん身体は疲労するかもしれないが、感覚的には気にせずそのまま寝ていたと思う。この状態で眠れるかどうかによって寝台特急の是非は変わってくるだろう。カプセルホテルよりは天井も高く、スペースも一回り広くて余裕がある。今回は下の真ん中ぐらいの部屋だったのだけど、場所によって揺れや移動音の聞こえ方も変わるという。通な人は部屋の位置もこだわるのかもしれない。&lt;/p>
&lt;p>あと薄い毛布しかないため、冬はちょっと寒い。暖房入れて上着もかけてぎりぎりの暖かさかな。べつに眠れないほど寒いわけではないが、冬なんだからもう一枚布団を用意してくれてもいいのになという感覚はある。朝に起きて窓から線路は延々と眺められる。線路が好きな人にもよさそう。けっこうよかったのでまた来月も予約して寝台特急で出張へ行く。&lt;/p>
&lt;p>&lt;figure>&lt;img src="/diary/diary/img/2024/0205_train1.jpeg"/>
&lt;/figure>
&lt;figure>&lt;img src="/diary/diary/img/2024/0205_train2.jpeg"/>
&lt;/figure>
&lt;figure>&lt;img src="/diary/diary/img/2024/0205_train3.jpeg"/>
&lt;/figure>
&lt;/p></content></item><item><title>テスト環境を作り直す</title><link>/diary/posts/2024/0116/</link><pubDate>Tue, 16 Jan 2024 11:10:20 +0900</pubDate><guid>/diary/posts/2024/0116/</guid><description>21時頃から寝て何度か起きて7時半に起きた。昨日は頭が痛かったら早く帰って安静にしてた。安静にしたら直ったので大したことはなかったみたい。
今日の筋トレは腹筋:15x2,腕立て:10x1,スクワット15x2をした。
テスト環境の見直し いまどきの開発の定番としては ci/cd でコミット単位にテスト環境にデプロイする仕組みを構築している。テスト環境は1つだけになるのだけど、プロダクトの開発を1年以上やってきて機能が増えたことによって、1つのテスト環境で相容れない複数の要件や機能を混在させることで管理や運用が煩雑になってきた。気付いたタイミングがよい時期だと思うのでこの機会に1つのテスト環境を3つのテスト環境に分散させようと思う。既存データを確認するだけなら1つでもよいが、id 連携という機能の特性上、複数のシステム間でデータ連携するため、システム間での依存関係が発生する。それを整理しないといけなくなったという次第。今日のところは現状把握や移行に必要な段取りなどを設計していた。
hugo のハンズオン資料作り 昨日の続き 。23時過ぎに外に出たついでにオフィスに寄り道して書き始めたらまた熱中してしまって2時ぐらいまで書いていた。ハンズオンで説明する内容は一通り書いたつもり。せっかくの機会だから上級者向けにもう少し知っていることを書いてみようとは思う。</description><content>&lt;p>21時頃から寝て何度か起きて7時半に起きた。昨日は頭が痛かったら早く帰って安静にしてた。安静にしたら直ったので大したことはなかったみたい。&lt;/p>
&lt;p>今日の筋トレは腹筋:15x2,腕立て:10x1,スクワット15x2をした。&lt;/p>
&lt;h2 id="テスト環境の見直し">テスト環境の見直し&lt;/h2>
&lt;p>いまどきの開発の定番としては ci/cd でコミット単位にテスト環境にデプロイする仕組みを構築している。テスト環境は1つだけになるのだけど、プロダクトの開発を1年以上やってきて機能が増えたことによって、1つのテスト環境で相容れない複数の要件や機能を混在させることで管理や運用が煩雑になってきた。気付いたタイミングがよい時期だと思うのでこの機会に1つのテスト環境を3つのテスト環境に分散させようと思う。既存データを確認するだけなら1つでもよいが、id 連携という機能の特性上、複数のシステム間でデータ連携するため、システム間での依存関係が発生する。それを整理しないといけなくなったという次第。今日のところは現状把握や移行に必要な段取りなどを設計していた。&lt;/p>
&lt;h2 id="hugo-のハンズオン資料作り">hugo のハンズオン資料作り&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2024/0115/">昨日の続き&lt;/a> 。23時過ぎに外に出たついでにオフィスに寄り道して書き始めたらまた熱中してしまって2時ぐらいまで書いていた。ハンズオンで説明する内容は一通り書いたつもり。せっかくの機会だから上級者向けにもう少し知っていることを書いてみようとは思う。&lt;/p></content></item><item><title>コンテナイメージの移行</title><link>/diary/posts/2023/1214/</link><pubDate>Thu, 14 Dec 2023 08:32:58 +0900</pubDate><guid>/diary/posts/2023/1214/</guid><description>1時に寝て3時に起きて6時半に起きた。スマホで呪術廻戦のゲームを開いたまま寝てた。
サードパーティの mongodb コンテナへの移行 昨日の mongodb のサードパーティのコンテナイメージ調査 の続き。
レプリカセットの削除 基本的に一度作ったレプリカセットを削除することはないせいか、レプリカセットを削除するユーティリティは提供されていない。なんらかの理由でレプリカセットを再作成したいときは、レプリカセットの設定が保存されている local database を削除する。
またレプリカセットの稼働中に local database を削除することはできないため、mongod サーバーを --replSet を指定していない状態で起動させ、そのときに次のようにして local database を削除できる。
test&amp;gt; use admin admin&amp;gt; db.grantRolesToUser(&amp;#34;root&amp;#34;, [&amp;#34;__system&amp;#34;]); { ok: 1 } admin&amp;gt; use local switched to db local local&amp;gt; db.dropDatabase() { ok: 1, dropped: &amp;#39;local&amp;#39; } local&amp;gt; use admin switched to db admin admin&amp;gt; db.revokeRolesFromUser(&amp;#34;root&amp;#34;, [&amp;#34;__system&amp;#34;]); { ok: 1 } コンテナを使ったレプリカセットの初期設定 bitnami/mongodb を使うと、ローカルのシングルノードでレプリカセットを使うには次のような設定になる。
mongo: image: docker.io/bitnami/mongodb:7.0.1 user: root # デフォルトは非 root ユーザーで起動するのでローカルの開発環境なら root で実行した方が手間がない volumes: - .</description><content>&lt;p>1時に寝て3時に起きて6時半に起きた。スマホで呪術廻戦のゲームを開いたまま寝てた。&lt;/p>
&lt;h2 id="サードパーティの-mongodb-コンテナへの移行">サードパーティの mongodb コンテナへの移行&lt;/h2>
&lt;p>昨日の &lt;a href="/diary/diary/posts/2023/1211/">mongodb のサードパーティのコンテナイメージ調査&lt;/a> の続き。&lt;/p>
&lt;h3 id="レプリカセットの削除">レプリカセットの削除&lt;/h3>
&lt;p>基本的に一度作ったレプリカセットを削除することはないせいか、レプリカセットを削除するユーティリティは提供されていない。なんらかの理由でレプリカセットを再作成したいときは、レプリカセットの設定が保存されている local database を削除する。&lt;/p>
&lt;p>またレプリカセットの稼働中に local database を削除することはできないため、mongod サーバーを &lt;code>--replSet&lt;/code> を指定していない状態で起動させ、そのときに次のようにして local database を削除できる。&lt;/p>
&lt;pre tabindex="0">&lt;code>test&amp;gt; use admin
admin&amp;gt; db.grantRolesToUser(&amp;#34;root&amp;#34;, [&amp;#34;__system&amp;#34;]);
{ ok: 1 }
admin&amp;gt; use local
switched to db local
local&amp;gt; db.dropDatabase()
{ ok: 1, dropped: &amp;#39;local&amp;#39; }
local&amp;gt; use admin
switched to db admin
admin&amp;gt; db.revokeRolesFromUser(&amp;#34;root&amp;#34;, [&amp;#34;__system&amp;#34;]);
{ ok: 1 }
&lt;/code>&lt;/pre>&lt;h3 id="コンテナを使ったレプリカセットの初期設定">コンテナを使ったレプリカセットの初期設定&lt;/h3>
&lt;p>&lt;a href="https://hub.docker.com/r/bitnami/mongodb">bitnami/mongodb&lt;/a> を使うと、ローカルのシングルノードでレプリカセットを使うには次のような設定になる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">mongo&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">docker.io/bitnami/mongodb:7.0.1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">user&lt;/span>: &lt;span style="color:#ae81ff">root &lt;/span> &lt;span style="color:#75715e"># デフォルトは非 root ユーザーで起動するのでローカルの開発環境なら root で実行した方が手間がない&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumes&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">./volumes/mongodb:/bitnami/mongodb&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">environment&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">MONGODB_ROOT_USER&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;${MONGO_USER}&amp;#34;&lt;/span> &lt;span style="color:#75715e"># 認証ユーザー&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">MONGODB_ROOT_PASSWORD&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;${MONGO_PASSWORD}&amp;#34;&lt;/span> &lt;span style="color:#75715e"># 認証ユーザーのパスワード&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">MONGODB_ADVERTISED_HOSTNAME&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;mongo-primary&amp;#34;&lt;/span> &lt;span style="color:#75715e"># レプリカセットのノードを ip アドレスではなくホスト名で指定する&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">MONGODB_REPLICA_SET_NAME&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;myrs&amp;#34;&lt;/span> &lt;span style="color:#75715e"># レプリカセットの名前&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">MONGODB_REPLICA_SET_MODE&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;primary&amp;#34;&lt;/span> &lt;span style="color:#75715e"># プライマリノードとして設定&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">MONGODB_REPLICA_SET_KEY&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;my/replication/common/key123&amp;#34;&lt;/span> &lt;span style="color:#75715e"># キーファイルのコンテンツ (base64 でデコードできる値)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">MONGODB_SYSTEM_LOG_VERBOSITY&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#75715e"># ログレベル&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">hostname&lt;/span>: &lt;span style="color:#ae81ff">mongo-primary &lt;/span> &lt;span style="color:#75715e"># コンテナの内外から解決できるホスト名を指定&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">container_name&lt;/span>: &lt;span style="color:#ae81ff">mongo &lt;/span> &lt;span style="color:#75715e"># コンテナ名 (docker container ls で表示される名前)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">27017&lt;/span>:&lt;span style="color:#ae81ff">27017&lt;/span> &lt;span style="color:#75715e"># レプリカセットを運用する場合はポート番号のマッピングを一致させる必要がある&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">restart&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;always&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>この設定でレプリカセットを初期した場合、レプリカセットの initialize 処理は、次のような config/member をもつ。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-javascript" data-lang="javascript">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">members&lt;/span>&lt;span style="color:#f92672">:&lt;/span> [{ &lt;span style="color:#a6e22e">_id&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#a6e22e">host&lt;/span> &lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#34;mongo-primary:27017&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">priority&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span> }]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>コンテナの内部からは mongo-primary というホスト名に対して、コンテナネットワーク内のローカル ip アドレスが解決される。&lt;/p>
&lt;pre tabindex="0">&lt;code>c67a5ca94a77:/app# dig +short mongo-primary
192.168.240.3
&lt;/code>&lt;/pre>&lt;p>ここで host os 上のアプリケーションから mongo コンテナに対してレプリカセット接続をする場合 &lt;code>replicaSet=${レプリカセットの名前}&lt;/code> のパラメーターを追加する。&lt;/p>
&lt;pre tabindex="0">&lt;code>mongodb://root:password@localhost:27017/?authMechanism=DEFAULT&amp;amp;replicaSet=myrs
&lt;/code>&lt;/pre>&lt;p>これは localhost:27017 にレプリカセットの接続を試行し、接続できるとレプリカセットのメンバーが返される。&lt;/p>
&lt;p>レプリカセットのメンバーには &lt;code>mongo-primary:27017&lt;/code> という設定が行われているため、mongo-primary というホスト名に対して host os 上で名前解決できる必要がある。そのために /etc/hosts に次の設定を行う。&lt;/p>
&lt;pre tabindex="0">&lt;code>$ sudo vi /etc/hosts
...
127.0.0.1 mongo-primary
&lt;/code>&lt;/pre>&lt;p>&lt;a href="https://www.mongodb.com/products/tools/compass">compass&lt;/a> で接続した場合、レプリカセット接続であれば、レプリカセットの名前が接続情報として表示される。&lt;/p>
&lt;h2 id="ダイニングテーブル引き取り">ダイニングテーブル引き取り&lt;/h2>
&lt;p>実は火曜日にも長机を引き取りに行ってきて、今日はダイニングテーブルを引き取りに行ってきた。この3日間で2つもテーブルが手に入った。いつも目ぼしいと思ったものは、すぐに他の人と取り引きが成立してしまうのに、たまたま続けて私と取り引きが成立した。車で20分ぐらいの距離のマンションまで引き取りに行った。20時の予定を、19時10分には着いてしまって、先方も快く対応してくれた。私よりも見た目すこし年配の方で人当たりのよい感じの方だった。ジモティのやり取りはその人の性格が出るもので、受け渡しだけささっとやって余計な話しはしないパターンもあれば、愛想よく話しながら受け渡しをするパターンもある。先方によると、大事に使っていたテーブルのようにみえるので私も離れのオフィススペースで大事に使おうと思う。&lt;/p></content></item><item><title>mongodb のサードパーティのコンテナイメージ</title><link>/diary/posts/2023/1213/</link><pubDate>Wed, 13 Dec 2023 20:35:00 +0900</pubDate><guid>/diary/posts/2023/1213/</guid><description>23時に寝て3時に起きて寝たかどうか覚えていないうちに6時半になっていて7時半に起きた。
json を介した go の bool 値のバリエーション go-playground/validator のバリデータには required というバリデーションオプションがある。しかし、このオプションは go のゼロ値でないことをチェックするという仕様になっている。bool のゼロ値は false となるため、リクエストした JSON データに false を設定していたのか、未設定だったのかの違いを検出できない。これはバリデータの問題ではなく、go の json ライブラリの制約のようなもので使い勝手のよい仕様とは言えない。私もこの振る舞いに起因する不具合に遭遇したこともあるし、こういうときにどうしたらよいかも過去に3回ぐらいは調べている気がする。
How to validate bool #142 現時点での私の最適化は次のコードになる。データ構造として *bool 型にすれば、ポインタ型のゼロ値は nil となるため、true, false, nil の3値でバリデーションできる。しかし、私はこのデータ構造を好ましく思わない。というのは、内部的には true/false の2値でしか管理しないメンバーを、json のバリデーションのためだけに nil も許容する3値にすることがよい設計だと私は思えない。そこでバリデータによるバリデーションは諦めて、json の Unmarshal 処理をフックしてバリデーション相当の処理を自分で実装する。このやり方のデメリットはメンバーが追加されたときに自分で UnmarshalJSON() メソッドを保守する必要がある点になる。しかし、メリットとして内部のデータ構造の型は bool 型で扱える。一概にどちらがよいとは言いにくいかもしれないし、設計上の好みかもしれない。
type reqMyData struct { Name string `json:&amp;#34;name&amp;#34;` View *bool `json:&amp;#34;view&amp;#34;` } type MyData struct { Name string `json:&amp;#34;name&amp;#34;` View bool `json:&amp;#34;view&amp;#34;` } func (d *MyData) UnmarshalJSON(data []byte) error { var tmp reqMyData if err := json.</description><content>&lt;p>23時に寝て3時に起きて寝たかどうか覚えていないうちに6時半になっていて7時半に起きた。&lt;/p>
&lt;h2 id="json-を介した-go-の-bool-値のバリエーション">json を介した go の bool 値のバリエーション&lt;/h2>
&lt;p>&lt;a href="https://github.com/go-playground/validator">go-playground/validator&lt;/a> のバリデータには &lt;a href="https://pkg.go.dev/github.com/go-playground/validator/v10#hdr-Required">required&lt;/a> というバリデーションオプションがある。しかし、このオプションは go のゼロ値でないことをチェックするという仕様になっている。bool のゼロ値は false となるため、リクエストした JSON データに false を設定していたのか、未設定だったのかの違いを検出できない。これはバリデータの問題ではなく、go の json ライブラリの制約のようなもので使い勝手のよい仕様とは言えない。私もこの振る舞いに起因する不具合に遭遇したこともあるし、こういうときにどうしたらよいかも過去に3回ぐらいは調べている気がする。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/go-playground/validator/issues/142">How to validate bool #142&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>現時点での私の最適化は次のコードになる。データ構造として &lt;code>*bool&lt;/code> 型にすれば、ポインタ型のゼロ値は nil となるため、true, false, nil の3値でバリデーションできる。しかし、私はこのデータ構造を好ましく思わない。というのは、内部的には true/false の2値でしか管理しないメンバーを、json のバリデーションのためだけに nil も許容する3値にすることがよい設計だと私は思えない。そこでバリデータによるバリデーションは諦めて、json の Unmarshal 処理をフックしてバリデーション相当の処理を自分で実装する。このやり方のデメリットはメンバーが追加されたときに自分で UnmarshalJSON() メソッドを保守する必要がある点になる。しかし、メリットとして内部のデータ構造の型は &lt;code>bool&lt;/code> 型で扱える。一概にどちらがよいとは言いにくいかもしれないし、設計上の好みかもしれない。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">reqMyData&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Name&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;name&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">View&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#66d9ef">bool&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;view&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">MyData&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Name&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;name&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">View&lt;/span> &lt;span style="color:#66d9ef">bool&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;view&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> (&lt;span style="color:#a6e22e">d&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">MyData&lt;/span>) &lt;span style="color:#a6e22e">UnmarshalJSON&lt;/span>(&lt;span style="color:#a6e22e">data&lt;/span> []&lt;span style="color:#66d9ef">byte&lt;/span>) &lt;span style="color:#66d9ef">error&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">tmp&lt;/span> &lt;span style="color:#a6e22e">reqMyData&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">json&lt;/span>.&lt;span style="color:#a6e22e">Unmarshal&lt;/span>(&lt;span style="color:#a6e22e">data&lt;/span>, &lt;span style="color:#f92672">&amp;amp;&lt;/span>&lt;span style="color:#a6e22e">tmp&lt;/span>); &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">fmt&lt;/span>.&lt;span style="color:#a6e22e">Errorf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;failed to unmarshal as reqMyData&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">tmp&lt;/span>.&lt;span style="color:#a6e22e">View&lt;/span> &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">fmt&lt;/span>.&lt;span style="color:#a6e22e">Errorf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;required view field&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">d&lt;/span>.&lt;span style="color:#a6e22e">Name&lt;/span> = &lt;span style="color:#a6e22e">tmp&lt;/span>.&lt;span style="color:#a6e22e">Name&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">d&lt;/span>.&lt;span style="color:#a6e22e">View&lt;/span> = &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">tmp&lt;/span>.&lt;span style="color:#a6e22e">View&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="サードパーティの-mongodb-コンテナイメージ">サードパーティの mongodb コンテナイメージ&lt;/h2>
&lt;p>先日の &lt;a href="/diary/diary/posts/2023/1211/">mongodb のレプリカセット調査&lt;/a> の続き。コードレビューをしていて &lt;a href="https://hub.docker.com/r/bitnami/mongodb">bitnami/mongodb&lt;/a> というサードパーティのコンテナイメージを使った方がよいのではないか？というコメントがあったのでその調査をしてみた。VMware 社が提供しているサードパーティのコンテナイメージらしい。&lt;/p>
&lt;blockquote>
&lt;p>MongoDB(R) is run and maintained by MongoDB, which is a completely separate project from Bitnami.&lt;/p>
&lt;/blockquote>
&lt;p>まず MongoDB プロジェクトとはまったく別管理であることが書いてある。&lt;/p>
&lt;blockquote>
&lt;p>Bitnami イメージを使用する理由&lt;/p>
&lt;ul>
&lt;li>Bitnamiはアップストリームソースの変更を綿密に追跡し、自動化されたシステムを使用してこのイメージの新しいバージョンを迅速に公開します。&lt;/li>
&lt;li>Bitnami イメージでは、最新のバグ修正と機能をできるだけ早く利用できます。&lt;/li>
&lt;li>Bitnamiのコンテナ、仮想マシン、クラウドイメージは、同じコンポーネントと構成アプローチを使用しているため、プロジェクトのニーズに応じて形式を簡単に切り替えることができます。&lt;/li>
&lt;li>Bitnamiのイメージはすべて、minideb（最小限のDebianベースのコンテナイメージ）またはscratch（明示的に空のイメージ）をベースにしています。&lt;/li>
&lt;li>Docker Hubで利用可能なすべてのBitnamiイメージは、Docker Content Trust（DCT）で署名されています。DOCKER_CONTENT_TRUST=1 を使用して、イメージの完全性を確認できます。&lt;/li>
&lt;li>Bitnamiコンテナイメージは定期的にリリースされ、最新のディストリビューションパッケージが利用可能です。&lt;/li>
&lt;/ul>
&lt;p>MongoDB®を本番環境で使用したいですか？Bitnami Application Catalogのエンタープライズ版であるVMware Tanzu Application Catalogをお試しください。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://hub.docker.com/_/mongo">mongo&lt;/a> の公式イメージは ubuntu をベースイメージにしている。ubuntu よりは minideb の方が軽いのかな？そしてちゃんと upstream にも追随しているみたい。このベースイメージの違いによるものかは定かではないが、結合テストのイメージも移行してみたところ、10-20秒ほど結合テストの実行時間が速くなった。割合にすると10%程度かな。&lt;/p>
&lt;blockquote>
&lt;p>KubernetesにMongoDB®をデプロイするには？&lt;/p>
&lt;p>Bitnami アプリケーションを Helm Chart としてデプロイすることは、Kubernetes 上で当社のアプリケーションを使い始める最も簡単な方法です。インストールの詳細については、Bitnami MongoDB® Chart GitHub リポジトリを参照してください。&lt;/p>
&lt;p>Bitnami コンテナは、クラスタへの Helm Charts のデプロイと管理に Kubeapps と一緒に使用できます。&lt;/p>
&lt;/blockquote>
&lt;p>helm chart も提供しているようで、いずれクラウド版を作るときに MongoDB も k8s 上にデプロイする上でこのことは都合がよいように思える。&lt;/p>
&lt;p>レプリケーションを前提とした初期設定があり、entrypoint スクリプトもいくつか読んでみた感じだと、きれいに管理されていて保守もちゃんとやってくれそうにみえる。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/bitnami/containers/tree/main/bitnami/mongodb">https://github.com/bitnami/containers/tree/main/bitnami/mongodb&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>昨日、導入したばかりの公式イメージ + 自作スクリプトによるレプリケーション設定を廃止して、Bitnami のコンテナイメージを使うことに決めた。&lt;/p></content></item><item><title>owner/permission の違うファイルとリポジトリ管理</title><link>/diary/posts/2023/1211/</link><pubDate>Mon, 11 Dec 2023 09:37:23 +0900</pubDate><guid>/diary/posts/2023/1211/</guid><description>23時に寝て2時に起きて6時に起きて7時過ぎに起きた。なんか微妙な寝方をした。
先日の mongodb のレプリカセットの調査 の整理をしてマージリクエストを作成した。共通鍵の keyFile をどう扱えばいいのか、わからなくて、一旦コンテナ内の tmp 領域にコピーして、それを entrypoint スクリプトでコピーしてから owner/permission を変更するというやり方で、リポジトリ管理で共有しやすいようにしてみた。entrypoint スクリプトは root 権限で実行されることも理解した。
volumes: - ./mongo/keyfile:/var/tmp/keyfile.orig command: - mongod - --keyFile - /data/keyfile - --replSet - &amp;#34;myrs&amp;#34; entrypoint: - bash - -c - | if [[ ! -f /data/keyfile ]]; then cp /var/tmp/keyfile.orig /data/keyfile chmod 400 /data/keyfile chown mongodb:mongodb /data/keyfile fi exec docker-entrypoint.sh $$@ テックブログを読む会 昨日、西原さんに教えてもらった テックブログを読むイベント を探したら毎週月曜日に行われているようだった。早速 テックブログ一気読み選手権20231211杯 に参加した。HackMD で読んだメモを管理している。記事を選択して、読んで、所感をまとめて、他の人たちと共有する。ただそれだけのイベント。ちょうど30分で終わって、自分の勉強にもなったし、他の人の話しも聞いて参考になった。たった30分でも、なにもやらないよりずっとよい。1ヶ月ほど参加してやり方を学んだらチームにも展開してみようかと考えている。</description><content>&lt;p>23時に寝て2時に起きて6時に起きて7時過ぎに起きた。なんか微妙な寝方をした。&lt;/p>
&lt;p>先日の &lt;a href="/diary/diary/posts/2023/1207/">mongodb のレプリカセットの調査&lt;/a> の整理をしてマージリクエストを作成した。共通鍵の keyFile をどう扱えばいいのか、わからなくて、一旦コンテナ内の tmp 領域にコピーして、それを entrypoint スクリプトでコピーしてから owner/permission を変更するというやり方で、リポジトリ管理で共有しやすいようにしてみた。entrypoint スクリプトは root 権限で実行されることも理解した。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">volumes&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">./mongo/keyfile:/var/tmp/keyfile.orig&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">command&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">mongod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --&lt;span style="color:#ae81ff">keyFile&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">/data/keyfile&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --&lt;span style="color:#ae81ff">replSet&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;myrs&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">entrypoint&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">bash&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - -&lt;span style="color:#ae81ff">c&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - |&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> if [[ ! -f /data/keyfile ]]; then
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> cp /var/tmp/keyfile.orig /data/keyfile
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> chmod 400 /data/keyfile
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> chown mongodb:mongodb /data/keyfile
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> fi
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> exec docker-entrypoint.sh $$@&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="テックブログを読む会">テックブログを読む会&lt;/h2>
&lt;p>昨日、西原さんに教えてもらった &lt;a href="/diary/diary/posts/2023/1210/">テックブログを読むイベント&lt;/a> を探したら毎週月曜日に行われているようだった。早速 &lt;a href="https://blogreading.connpass.com/event/304979/">テックブログ一気読み選手権20231211杯&lt;/a> に参加した。HackMD で読んだメモを管理している。記事を選択して、読んで、所感をまとめて、他の人たちと共有する。ただそれだけのイベント。ちょうど30分で終わって、自分の勉強にもなったし、他の人の話しも聞いて参考になった。たった30分でも、なにもやらないよりずっとよい。1ヶ月ほど参加してやり方を学んだらチームにも展開してみようかと考えている。&lt;/p></content></item><item><title>mongodb のレプリカセットのデプロイ調査</title><link>/diary/posts/2023/1207/</link><pubDate>Thu, 07 Dec 2023 09:26:50 +0900</pubDate><guid>/diary/posts/2023/1207/</guid><description>4時前に寝て6時半に起きた。1時過ぎまで作業して、帰って少しゲームして、うまく眠れなくてだらだらしていた。
mongodb のレプリカセットの調査 以前 mongodb でトランザクションを使うときにレプリカセットが必要 なことがわかった。他機能の開発途中だったので一旦後回しにしていたものを回収している。状況によってはメンバーに委譲してもよかったんだけど、私が遊撃で出張ってみることにした。実際に調べてみてコンテナの運用も考慮するとけっこう難しいことがわかってきた。
mongosh からは Replication Methods を使ってレプリカセットの操作ができる。これはユーティリティのようなもので mongodb としての低レベルのコマンド操作は Replication Commands になる。mongo-go-driver はレプリカセット向けのユーティリティを提供していないため、Replication Commands を RunCommand() の低レベル API を使って自分で実装しないといけない。
例えば、レプリカセットの初期化をするときは次のように replSetInitiate というコマンドを適切なパラメーターで呼び出す。あまりドキュメントで丁寧に説明されていないので試行錯誤でエラーメッセージをみながら実装することになる。とくにはまるのが mongod のサーバーは --replSet myrs のようにレプリカセットを指定して起動させるものの、初期化コマンドを実行するときはまだレプリカセットを設定していないため、レプリカセットを指定せず、且つ direct パラメーターをセットしないと mongod サーバーに接続できない。この微妙な設定を把握するのにはまった。これが正しい手順かどうかもわからないが、ググったりしているとフォーラムでそういったコメントが散見されたりする。おそらく mongosh の Replication Methods を使うと、クライアントからサーバー接続は裏方でよしなにやってくれるのでそっちの方が簡単ではある。
func (r *ReplicaSet) Initiate(ctx context.Context, config bson.M) error { client, err := r.connectDirect(ctx) if err != nil { return fmt.Errorf(&amp;#34;failed to connect with direct: %w&amp;#34;, err) } defer client.Disconnect(ctx) var result bson.</description><content>&lt;p>4時前に寝て6時半に起きた。1時過ぎまで作業して、帰って少しゲームして、うまく眠れなくてだらだらしていた。&lt;/p>
&lt;h2 id="mongodb-のレプリカセットの調査">mongodb のレプリカセットの調査&lt;/h2>
&lt;p>以前 &lt;a href="/diary/diary/posts/2023/1101/#mongo-とトランザクションとレプリカセット">mongodb でトランザクションを使うときにレプリカセットが必要&lt;/a> なことがわかった。他機能の開発途中だったので一旦後回しにしていたものを回収している。状況によってはメンバーに委譲してもよかったんだけど、私が遊撃で出張ってみることにした。実際に調べてみてコンテナの運用も考慮するとけっこう難しいことがわかってきた。&lt;/p>
&lt;p>&lt;a href="https://www.mongodb.com/docs/mongodb-shell/">mongosh&lt;/a> からは &lt;a href="https://www.mongodb.com/docs/v7.0/reference/method/js-replication/">Replication Methods&lt;/a> を使ってレプリカセットの操作ができる。これはユーティリティのようなもので mongodb としての低レベルのコマンド操作は &lt;a href="https://www.mongodb.com/docs/manual/reference/command/nav-replication/">Replication Commands&lt;/a> になる。&lt;a href="https://github.com/mongodb/mongo-go-driver">mongo-go-driver&lt;/a> はレプリカセット向けのユーティリティを提供していないため、Replication Commands を RunCommand() の低レベル API を使って自分で実装しないといけない。&lt;/p>
&lt;p>例えば、レプリカセットの初期化をするときは次のように &lt;code>replSetInitiate&lt;/code> というコマンドを適切なパラメーターで呼び出す。あまりドキュメントで丁寧に説明されていないので試行錯誤でエラーメッセージをみながら実装することになる。とくにはまるのが mongod のサーバーは &lt;code>--replSet myrs&lt;/code> のようにレプリカセットを指定して起動させるものの、初期化コマンドを実行するときはまだレプリカセットを設定していないため、レプリカセットを指定せず、且つ &lt;code>direct&lt;/code> パラメーターをセットしないと mongod サーバーに接続できない。この微妙な設定を把握するのにはまった。これが正しい手順かどうかもわからないが、ググったりしているとフォーラムでそういったコメントが散見されたりする。おそらく mongosh の Replication Methods を使うと、クライアントからサーバー接続は裏方でよしなにやってくれるのでそっちの方が簡単ではある。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> (&lt;span style="color:#a6e22e">r&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">ReplicaSet&lt;/span>) &lt;span style="color:#a6e22e">Initiate&lt;/span>(&lt;span style="color:#a6e22e">ctx&lt;/span> &lt;span style="color:#a6e22e">context&lt;/span>.&lt;span style="color:#a6e22e">Context&lt;/span>, &lt;span style="color:#a6e22e">config&lt;/span> &lt;span style="color:#a6e22e">bson&lt;/span>.&lt;span style="color:#a6e22e">M&lt;/span>) &lt;span style="color:#66d9ef">error&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">client&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">r&lt;/span>.&lt;span style="color:#a6e22e">connectDirect&lt;/span>(&lt;span style="color:#a6e22e">ctx&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">fmt&lt;/span>.&lt;span style="color:#a6e22e">Errorf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;failed to connect with direct: %w&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">defer&lt;/span> &lt;span style="color:#a6e22e">client&lt;/span>.&lt;span style="color:#a6e22e">Disconnect&lt;/span>(&lt;span style="color:#a6e22e">ctx&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">result&lt;/span> &lt;span style="color:#a6e22e">bson&lt;/span>.&lt;span style="color:#a6e22e">M&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">cmd&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">bson&lt;/span>.&lt;span style="color:#a6e22e">D&lt;/span>{{&lt;span style="color:#a6e22e">Key&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;replSetInitiate&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">Value&lt;/span>: &lt;span style="color:#a6e22e">config&lt;/span>}}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">client&lt;/span>.&lt;span style="color:#a6e22e">Database&lt;/span>(&lt;span style="color:#a6e22e">r&lt;/span>.&lt;span style="color:#a6e22e">db&lt;/span>).&lt;span style="color:#a6e22e">RunCommand&lt;/span>(&lt;span style="color:#a6e22e">ctx&lt;/span>, &lt;span style="color:#a6e22e">cmd&lt;/span>).&lt;span style="color:#a6e22e">Decode&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>&lt;span style="color:#a6e22e">result&lt;/span>); &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">fmt&lt;/span>.&lt;span style="color:#a6e22e">Errorf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;failed to run replSetInitiate(): %w&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">log&lt;/span>.&lt;span style="color:#a6e22e">PrettyPrint&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;completed to initiate&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">result&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> (&lt;span style="color:#a6e22e">r&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">ReplicaSet&lt;/span>) &lt;span style="color:#a6e22e">connectDirect&lt;/span>(&lt;span style="color:#a6e22e">ctx&lt;/span> &lt;span style="color:#a6e22e">context&lt;/span>.&lt;span style="color:#a6e22e">Context&lt;/span>) (&lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">mongo&lt;/span>.&lt;span style="color:#a6e22e">Client&lt;/span>, &lt;span style="color:#66d9ef">error&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">opts&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">options&lt;/span>.&lt;span style="color:#a6e22e">Client&lt;/span>().
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetAuth&lt;/span>(&lt;span style="color:#a6e22e">options&lt;/span>.&lt;span style="color:#a6e22e">Credential&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Username&lt;/span>: &lt;span style="color:#a6e22e">r&lt;/span>.&lt;span style="color:#a6e22e">config&lt;/span>.&lt;span style="color:#a6e22e">User&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Password&lt;/span>: &lt;span style="color:#a6e22e">r&lt;/span>.&lt;span style="color:#a6e22e">config&lt;/span>.&lt;span style="color:#a6e22e">Passwd&lt;/span>.&lt;span style="color:#a6e22e">String&lt;/span>(),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }).
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetHosts&lt;/span>(&lt;span style="color:#a6e22e">r&lt;/span>.&lt;span style="color:#a6e22e">config&lt;/span>.&lt;span style="color:#a6e22e">Hosts&lt;/span>).
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetDirect&lt;/span>(&lt;span style="color:#66d9ef">true&lt;/span>) &lt;span style="color:#75715e">// must be true
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">mongo&lt;/span>.&lt;span style="color:#a6e22e">Connect&lt;/span>(&lt;span style="color:#a6e22e">ctx&lt;/span>, &lt;span style="color:#a6e22e">opts&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">InitSingleReplicaSet&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ctx&lt;/span> &lt;span style="color:#a6e22e">context&lt;/span>.&lt;span style="color:#a6e22e">Context&lt;/span>, &lt;span style="color:#a6e22e">cfg&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">config&lt;/span>.&lt;span style="color:#a6e22e">MongoDB&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>) &lt;span style="color:#66d9ef">error&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">rs&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">NewReplicaSet&lt;/span>(&lt;span style="color:#a6e22e">cfg&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">initConfig&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">bson&lt;/span>.&lt;span style="color:#a6e22e">M&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;_id&amp;#34;&lt;/span>: &lt;span style="color:#a6e22e">cfg&lt;/span>.&lt;span style="color:#a6e22e">ReplicaSet&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;members&amp;#34;&lt;/span>: []&lt;span style="color:#a6e22e">bson&lt;/span>.&lt;span style="color:#a6e22e">M&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;_id&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;host&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;localhost:27017&amp;#34;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">rs&lt;/span>.&lt;span style="color:#a6e22e">Initiate&lt;/span>(&lt;span style="color:#a6e22e">ctx&lt;/span>, &lt;span style="color:#a6e22e">initConfig&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>さらに mongod サーバーを起動するときに &lt;code>--replSet&lt;/code> と &lt;code>--keyFile&lt;/code> (認証が必要な場合のみ？) という2つのパラメーターを指定する必要がある。&lt;code>--replSet&lt;/code> はレプリカセットの識別子を指定する。そして &lt;code>--keyFile&lt;/code> は共通鍵を指定する。この共通鍵を生成するには次のようにする。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ openssl rand -base64 &lt;span style="color:#ae81ff">756&lt;/span> &amp;gt; my-mongo-keyfile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ chown mongodb:mongodb my-mongo-keyfile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ chmod &lt;span style="color:#ae81ff">400&lt;/span> my-mongo-keyfile
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>普通のサーバーインスタンスならすぐできることだが、コンテナの運用において面倒なのが owner とパーミッションを設定しないといけないところ。mongo のコンテナは mongodb ユーザーで起動するため、root でマウントされたファイルシステムには書き込みできなかったりして keyFile の配置をどう扱えばよいのかが難しい。docker hub の mongo の issues でもどうやって設定したらいいの？って議論が発散している。mongo 本体が公式のスクリプトや仕組みを提供していれば済む話しだけど、どうもそうではないみたい。だから泥臭い方法で自分でなんとかしないといけないようにみえる。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/docker-library/mongo/issues/246">Creating a mongo image set with &amp;ndash;replSet #246&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/docker-library/mongo/issues/339">Cannot configure replica sets with entrypoint-initdb #339&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>dockertest でもレプリカセットの設定について次の issue として登録されている。mongo のコンテナを使ったテストの場合、dockertest のレイヤーが挟まるのでさらにわかりにくくなっている。テストを動かすためにどういった設定が必要かは把握できたのでなにかよい方法を考えてコントリビュートしたい。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/ory/dockertest/issues/480">Create an example for starting mongodb as a replica set #480&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>selinux はなるべく有効にして使うもの</title><link>/diary/posts/2023/1003/</link><pubDate>Tue, 03 Oct 2023 16:28:42 +0900</pubDate><guid>/diary/posts/2023/1003/</guid><description>22時ぐらいから寝始めて何度か起きて6時に起きた。早く寝たから早く起きた。
selinux の微妙な振る舞い 今日は火曜日なのでチームの定例会議をやって、ドキュメントを書いて、その後はインフラの細かい作業をわちゃわちゃやって、ドキュメントを書いてとわちゃわちゃやってた。
先週、最新の almalinux 8 をインストールして、その後、lvm の論理ボリュームの結合 とか、rootless コンテナ の設定とか、テスト環境を構築していた。gitlab ci/cd から ssh で公開鍵認証を使ってデプロイしている。作り直したこのテスト環境に対してその公開鍵認証がうまく動かない現象に遭遇した。よくある設定や権限のトラブルではなく、デバッグ用の sshd を起動すると公開鍵認証できた。なにかしら systemd 経由で起動する sshd の設定ミスなんじゃないかと、2-3時間デバッグしてもわからなくて社内の有識者に尋ねてみた。
$ sudo /usr/sbin/sshd -d -p 2222 selinux を無効にしてみたら？というアドバイスをいただいて、試しに enforced から disabled にしたら動いたので selinux のなにかしらの設定を変えてしまったのかな？とそのときは思っていた。しかし、別の開発者からデフォルト設定で enforced でも動くはずという情報をもらって、もう一度 disabled から enforced に戻して再起動したら普通に動いて、その前の公開鍵認証の失敗を再現できなくなった。私にはこの先のデバッグはまったくわからない。お手伝い先のシニアエンジニアの方にみてもらって次のようなことを教えてもらった。
SElinuxが怪しいなと思ったら、/var/log/audit/audit.log とかausearch -m avcコマンドを確認。
authorized_keysのアクセスが拒否されているので確かにSELinuxの問題があったことがわかります。
type=AVC msg=audit(1696315292.258:1446): avc: denied { read } for pid=446534 comm=&amp;ldquo;sshd&amp;rdquo; name=&amp;ldquo;authorized_keys&amp;rdquo; dev=&amp;ldquo;dm-0&amp;rdquo; ino=201836096 scontext=system_u:system_r:sshd_t:s0-s0:c0.c1023 tcontext=unconfined_u:object_r:default_t:s0 tclass=file permissive=0
現在、authorized_keysのコンテキストは期待通りunconfined_u:object_r:ssh_home_t:s0となっているけど、問題が起きていたときは、unconfined_u:object_r:default_t:s0 だったことがわかります。
詳しい経緯はわからないけど、.ssh/authorized_keysを作成した時点でopenssh用のselinuxポリシーが適用されていなかったと考えられます。
その後なにかのイベント(再起動?)でrestorecon 相当が行われて、コンテキストがssh_home_tに変更され問題は解消した。
なんだかよくわかないけど、OSのマイナーバージョンアップで微妙にセキュリティコンテキストが変更されてrestoreconすると解決する、ってのは時々起きてますね。</description><content>&lt;p>22時ぐらいから寝始めて何度か起きて6時に起きた。早く寝たから早く起きた。&lt;/p>
&lt;h2 id="selinux-の微妙な振る舞い">selinux の微妙な振る舞い&lt;/h2>
&lt;p>今日は火曜日なのでチームの定例会議をやって、ドキュメントを書いて、その後はインフラの細かい作業をわちゃわちゃやって、ドキュメントを書いてとわちゃわちゃやってた。&lt;/p>
&lt;p>先週、最新の almalinux 8 をインストールして、その後、&lt;a href="/diary/diary/posts/2023/0929/#lvm-の論理ボリュームの結合">lvm の論理ボリュームの結合&lt;/a> とか、&lt;a href="/diary/diary/posts/2023/1002/">rootless コンテナ&lt;/a> の設定とか、テスト環境を構築していた。gitlab ci/cd から ssh で公開鍵認証を使ってデプロイしている。作り直したこのテスト環境に対してその公開鍵認証がうまく動かない現象に遭遇した。よくある設定や権限のトラブルではなく、デバッグ用の sshd を起動すると公開鍵認証できた。なにかしら systemd 経由で起動する sshd の設定ミスなんじゃないかと、2-3時間デバッグしてもわからなくて社内の有識者に尋ねてみた。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ sudo /usr/sbin/sshd -d -p &lt;span style="color:#ae81ff">2222&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>selinux を無効にしてみたら？というアドバイスをいただいて、試しに enforced から disabled にしたら動いたので selinux のなにかしらの設定を変えてしまったのかな？とそのときは思っていた。しかし、別の開発者からデフォルト設定で enforced でも動くはずという情報をもらって、もう一度 disabled から enforced に戻して再起動したら普通に動いて、その前の公開鍵認証の失敗を再現できなくなった。私にはこの先のデバッグはまったくわからない。お手伝い先のシニアエンジニアの方にみてもらって次のようなことを教えてもらった。&lt;/p>
&lt;blockquote>
&lt;p>SElinuxが怪しいなと思ったら、/var/log/audit/audit.log とか&lt;code>ausearch -m avc&lt;/code>コマンドを確認。&lt;br />
authorized_keysのアクセスが拒否されているので確かにSELinuxの問題があったことがわかります。&lt;br />
type=AVC msg=audit(1696315292.258:1446): avc: denied { read } for pid=446534 comm=&amp;ldquo;sshd&amp;rdquo; name=&amp;ldquo;authorized_keys&amp;rdquo; dev=&amp;ldquo;dm-0&amp;rdquo; ino=201836096 scontext=system_u:system_r:sshd_t:s0-s0:c0.c1023 tcontext=unconfined_u:object_r:default_t:s0 tclass=file permissive=0&lt;br />
現在、authorized_keysのコンテキストは期待通りunconfined_u:object_r:ssh_home_t:s0となっているけど、問題が起きていたときは、unconfined_u:object_r:default_t:s0 だったことがわかります。&lt;br />
詳しい経緯はわからないけど、.ssh/authorized_keysを作成した時点でopenssh用のselinuxポリシーが適用されていなかったと考えられます。&lt;br />
その後なにかのイベント(再起動?)でrestorecon 相当が行われて、コンテキストがssh_home_tに変更され問題は解消した。&lt;br />
なんだかよくわかないけど、OSのマイナーバージョンアップで微妙にセキュリティコンテキストが変更されてrestoreconすると解決する、ってのは時々起きてますね。&lt;br />
たぶんopensshインストール前にrsyncしたのでコンテキストがdefault_tになってたんじゃないかと。なかなかの罠ですね。&lt;br />&lt;/p>
&lt;/blockquote>
&lt;p>おそらく lvm の論理ボリュームのバックアップ／リストアに &lt;code>rsync -a&lt;/code> を使った (本当は &lt;code>cp -a&lt;/code>の方がよい) ことによる問題ではないかということ。私が報告した状況と selinux のログからすぐ助言できるのが素晴らしいと思う。まだまだ私のインフラエンジニアとしての未熟さを実感した瞬間でもあった。一昔前は selinux は disabled にするものという常識だったが、最近は初期設定で動くようになっているのでなるべく selinux は有効にして運用するものという意識に変わってきているらしい。&lt;/p></content></item><item><title>インフラの式年遷宮</title><link>/diary/posts/2023/1002/</link><pubDate>Mon, 02 Oct 2023 09:08:26 +0900</pubDate><guid>/diary/posts/2023/1002/</guid><description>1時に寝て何度か起きて5時に起きた。それからだらだらして寝てまた7時に起きた。
テスト環境の再整備と rootless コンテナ インフラの式年遷宮のようなことをしていて、テスト環境をリファクタリングして再整備していた。これまで root でコンテナを実行していたが、最近は rootless コンテナがセキュリティ強化の観点から望ましいということで次のドキュメントをみながら設定した。
Linux post-installation steps for Docker Engine 設定はとくに難しくないが、dockerd や containerd の起動を systemd のユーザーインスタンスに依存することになる。systemd のユーザーインスタンスは基本的にユーザーがログインしたときに生成されるものなので OS が再起動したときなどに困る。OS 再起動時にも systemd のユーザーインスタンスを生成するには linger という仕組みを有効にすればよいらしい。systemd &amp;ndash;user の扱いと linger のことまで理解していれば、たぶん大丈夫なのかな？これで運用がうまくいくことを祈りたい。
$ sudo loginctl enable-linger ucidm</description><content>&lt;p>1時に寝て何度か起きて5時に起きた。それからだらだらして寝てまた7時に起きた。&lt;/p>
&lt;h2 id="テスト環境の再整備と-rootless-コンテナ">テスト環境の再整備と rootless コンテナ&lt;/h2>
&lt;p>インフラの式年遷宮のようなことをしていて、テスト環境をリファクタリングして再整備していた。これまで root でコンテナを実行していたが、最近は rootless コンテナがセキュリティ強化の観点から望ましいということで次のドキュメントをみながら設定した。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.docker.com/engine/install/linux-postinstall/">Linux post-installation steps for Docker Engine&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>設定はとくに難しくないが、dockerd や containerd の起動を systemd のユーザーインスタンスに依存することになる。systemd のユーザーインスタンスは基本的にユーザーがログインしたときに生成されるものなので OS が再起動したときなどに困る。OS 再起動時にも systemd のユーザーインスタンスを生成するには linger という仕組みを有効にすればよいらしい。systemd &amp;ndash;user の扱いと linger のことまで理解していれば、たぶん大丈夫なのかな？これで運用がうまくいくことを祈りたい。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ sudo loginctl enable-linger ucidm
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>初めて lvm を操作してみた</title><link>/diary/posts/2023/0929/</link><pubDate>Fri, 29 Sep 2023 08:20:06 +0900</pubDate><guid>/diary/posts/2023/0929/</guid><description>0時に寝て何度か起きて7時に起きた。最近は寝る前にはてブのアプリで適当に記事を読みながら寝ることが多い。
隔週の雑談 顧問のはらさんと隔週の打ち合わせ。今日の議題はこれら。
今後のお手伝いの展望 fun/done/learn をカスタマイズする話し hugo のテンプレートを作る話し 課題管理のコンテンツを作っていく話し ここ1-2週間ぼーっとしていて、忙しくもなく、なにかやっているわけでもないけど、のんびり過ごしている。軽いバーンアウトだと思う。先週末は休みも取った。「hugo のテンプレート作り」のような新規開発を、どこかのもくもく会やイベントに行って、その場で集中してやったらいいんじゃないか？というアドバイスをいただいて、確かにそういうやり方もよいように思えた。今週末は課題管理のコンテンツを考えて、できればブログに書いてみようと思う。
lvm の論理ボリュームの結合 新規に almalinux 8 で仮想マシンを作った。デフォルト設定でインストールしたら / と /home でパーティション分割されていて、これは使い勝手が悪いなと思ってパーティションを結合することにした。
$ df -h ファイルシス サイズ 使用 残り 使用% マウント位置 devtmpfs 1.9G 0 1.9G 0% /dev tmpfs 2.0G 0 2.0G 0% /dev/shm tmpfs 2.0G 8.6M 2.0G 1% /run tmpfs 2.0G 0 2.0G 0% /sys/fs/cgroup /dev/mapper/almalinux-root 70G 6.5G 64G 10% / /dev/mapper/almalinux-home 437G 5.0G 432G 2% /home /dev/vda1 1014M 221M 794M 22% /boot tmpfs 393M 12K 393M 1% /run/user/1000 基本的には次の記事をみてやったらうまくいった。</description><content>&lt;p>0時に寝て何度か起きて7時に起きた。最近は寝る前にはてブのアプリで適当に記事を読みながら寝ることが多い。&lt;/p>
&lt;h2 id="隔週の雑談">隔週の雑談&lt;/h2>
&lt;p>顧問のはらさんと隔週の打ち合わせ。今日の議題はこれら。&lt;/p>
&lt;ul>
&lt;li>今後のお手伝いの展望&lt;/li>
&lt;li>&lt;a href="/diary/diary/posts/2023/0926/">fun/done/learn をカスタマイズする話し&lt;/a>&lt;/li>
&lt;li>hugo のテンプレートを作る話し&lt;/li>
&lt;li>課題管理のコンテンツを作っていく話し&lt;/li>
&lt;/ul>
&lt;p>ここ1-2週間ぼーっとしていて、忙しくもなく、なにかやっているわけでもないけど、のんびり過ごしている。軽いバーンアウトだと思う。先週末は休みも取った。「hugo のテンプレート作り」のような新規開発を、どこかのもくもく会やイベントに行って、その場で集中してやったらいいんじゃないか？というアドバイスをいただいて、確かにそういうやり方もよいように思えた。今週末は課題管理のコンテンツを考えて、できればブログに書いてみようと思う。&lt;/p>
&lt;h2 id="lvm-の論理ボリュームの結合">lvm の論理ボリュームの結合&lt;/h2>
&lt;p>新規に almalinux 8 で仮想マシンを作った。デフォルト設定でインストールしたら &lt;code>/&lt;/code> と &lt;code>/home&lt;/code> でパーティション分割されていて、これは使い勝手が悪いなと思ってパーティションを結合することにした。&lt;/p>
&lt;pre tabindex="0">&lt;code>$ df -h
ファイルシス サイズ 使用 残り 使用% マウント位置
devtmpfs 1.9G 0 1.9G 0% /dev
tmpfs 2.0G 0 2.0G 0% /dev/shm
tmpfs 2.0G 8.6M 2.0G 1% /run
tmpfs 2.0G 0 2.0G 0% /sys/fs/cgroup
/dev/mapper/almalinux-root 70G 6.5G 64G 10% /
/dev/mapper/almalinux-home 437G 5.0G 432G 2% /home
/dev/vda1 1014M 221M 794M 22% /boot
tmpfs 393M 12K 393M 1% /run/user/1000
&lt;/code>&lt;/pre>&lt;p>基本的には次の記事をみてやったらうまくいった。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://blog.andersonbanihirwe.dev/posts/2021/how-to-merge-disk-partitions-on-centos/">How to merge two or more disk partitions on Centos 7&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>/home をバックアップする&lt;/p>
&lt;pre tabindex="0">&lt;code># mkdir /home-bkup
# cp -a /home/ /home-bkup/
&lt;/code>&lt;/pre>&lt;p>emergency モードに入る？&lt;/p>
&lt;pre tabindex="0">&lt;code># systemctl emergency
&lt;/code>&lt;/pre>&lt;p>結合したい領域を unmount して、home の論理ボリュームを削除する。&lt;/p>
&lt;pre tabindex="0">&lt;code># umount /dev/mapper/almalinux-home
# lvremove /dev/mapper/almalinux-home
Do you really want to remove active logical volume almalinux/home? [y/n]: y
Logical volume &amp;#34;home&amp;#34; successfully removed.
&lt;/code>&lt;/pre>&lt;p>バックアップからデータを戻す。&lt;/p>
&lt;pre tabindex="0">&lt;code># cp -a /home-bkup/ /home/
&lt;/code>&lt;/pre>&lt;p>/etc/fstab から不要なパーティション設定を削除する。&lt;/p>
&lt;pre tabindex="0">&lt;code># vi /etc/fstab
...
/dev/mapper/almalinux-home /home xfs defaults 0 0 # &amp;lt;- この行を削除
...
&lt;/code>&lt;/pre>&lt;p>root の論理ボリュームに余っている領域を拡張する。&lt;/p>
&lt;pre tabindex="0">&lt;code># lvextend -l +100%FREE -r /dev/mapper/almalinux-root
Size of logical volume almalinux/root changed from 70.00 GiB (17920 extents) to &amp;lt;507.04 GiB (129802 extents).
Logical volume almalinux/root successfully resized.
meta-data=/dev/mapper/almalinux-root isize=512 agcount=4, agsize=4587520 blks
= sectsz=512 attr=2, projid32bit=1
= crc=1 finobt=1, sparse=1, rmapbt=0
= reflink=1 bigtime=0 inobtcount=0
data = bsize=4096 blocks=18350080, imaxpct=25
= sunit=0 swidth=0 blks
naming =version 2 bsize=4096 ascii-ci=0, ftype=1
log =internal log bsize=4096 blocks=8960, version=2
= sectsz=512 sunit=0 blks, lazy-count=1
realtime =none extsz=4096 blocks=0, rtextents=0
data blocks changed from 18350080 to 132917248
&lt;/code>&lt;/pre>&lt;p>この時点で1つの領域に結合されたことがわかる。&lt;/p>
&lt;pre tabindex="0">&lt;code># df -h
ファイルシス サイズ 使用 残り 使用% マウント位置
devtmpfs 1.9G 0 1.9G 0% /dev
tmpfs 2.0G 0 2.0G 0% /dev/shm
tmpfs 2.0G 78M 1.9G 4% /run
tmpfs 2.0G 0 2.0G 0% /sys/fs/cgroup
/dev/mapper/almalinux-root 508G 14G 494G 3% /
/dev/vda1 1014M 221M 794M 22% /boot
&lt;/code>&lt;/pre>&lt;p>バックアップを削除する。&lt;/p>
&lt;pre tabindex="0">&lt;code># rm -rf /home-bkup/
&lt;/code>&lt;/pre>&lt;p>マシンを再起動する。&lt;/p>
&lt;pre tabindex="0">&lt;code># reboot
&lt;/code>&lt;/pre>&lt;p>これで問題なければ完了。&lt;/p></content></item><item><title>また podman に苦戦する</title><link>/diary/posts/2023/0922/</link><pubDate>Fri, 22 Sep 2023 08:07:44 +0900</pubDate><guid>/diary/posts/2023/0922/</guid><description>23時に寝て何度か起きて7時に起きた。出張帰りでなんかバテててなにもせず休んでいた。少し喉に引っかかりがある。出張で飲み歩いたし、そろそろコロナ感染？の疑いをもって生活してみる。
podman と dbus-daemon とsystemd の調査 2次開発の成果物をドッグフーディングの目的で社内へ導入する。メンバーが作業していて nginx が正常に動作しないという。ログをみろとすぐにコンテナネットワーク内の dns サービスが正常に動いていないということはわかった。podman は aardvark-dns というサービスを使って dns を管理する。但し、このサービスがまだまだ安定していなくて不具合があるのを以前にも確認した。このサービスの振る舞いがよく分からなくて、意図しない状況や状態に対して正常に動作してくれない。
他にも調査をしていて rootless で podman コマンドを実行すると次の issue で書かれているようなワーニングが出力される。dbus-user-session というパッケージを導入すれば解決するとある。
WARN[0000] The cgroupv2 manager is set to systemd but there is no systemd user session available #12983 dbus-daemon のサービスは systemd で動いていて、systemd のユーザーモードと dbus が正常に動いていないというところまではすぐに分かった。その状態だと rootless な podman が正常に動作しないということもすぐに分かった。ここまではすぐに調査できたが、問題はどうやれば sytemd のユーザーモードを dbus を正常に動くように復旧できるのかがまったく分からない。systemd がそもそも難しいのに、そのユーザーモードは権限管理が関係するのでさらにもっと難しい。1日調べてお手上げで他の社員さんにも相談してみた。
今日は自分の作業は進捗しなかったけど、メンバーの作業の進捗をみていて、メンバーがはまっていたところを助言して、その問題は解決してうまくいって、それだけで満足していた。</description><content>&lt;p>23時に寝て何度か起きて7時に起きた。出張帰りでなんかバテててなにもせず休んでいた。少し喉に引っかかりがある。出張で飲み歩いたし、そろそろコロナ感染？の疑いをもって生活してみる。&lt;/p>
&lt;h2 id="podman-と-dbus-daemon-とsystemd-の調査">podman と dbus-daemon とsystemd の調査&lt;/h2>
&lt;p>2次開発の成果物をドッグフーディングの目的で社内へ導入する。メンバーが作業していて nginx が正常に動作しないという。ログをみろとすぐにコンテナネットワーク内の dns サービスが正常に動いていないということはわかった。podman は &lt;a href="https://github.com/containers/aardvark-dns">aardvark-dns&lt;/a> というサービスを使って dns を管理する。但し、このサービスがまだまだ安定していなくて不具合があるのを以前にも確認した。このサービスの振る舞いがよく分からなくて、意図しない状況や状態に対して正常に動作してくれない。&lt;/p>
&lt;p>他にも調査をしていて rootless で podman コマンドを実行すると次の issue で書かれているようなワーニングが出力される。dbus-user-session というパッケージを導入すれば解決するとある。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containers/podman/issues/12983">WARN[0000] The cgroupv2 manager is set to systemd but there is no systemd user session available #12983&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>dbus-daemon のサービスは systemd で動いていて、systemd のユーザーモードと dbus が正常に動いていないというところまではすぐに分かった。その状態だと rootless な podman が正常に動作しないということもすぐに分かった。ここまではすぐに調査できたが、問題はどうやれば sytemd のユーザーモードを dbus を正常に動くように復旧できるのかがまったく分からない。systemd がそもそも難しいのに、そのユーザーモードは権限管理が関係するのでさらにもっと難しい。1日調べてお手上げで他の社員さんにも相談してみた。&lt;/p>
&lt;p>今日は自分の作業は進捗しなかったけど、メンバーの作業の進捗をみていて、メンバーがはまっていたところを助言して、その問題は解決してうまくいって、それだけで満足していた。&lt;/p></content></item><item><title>msgraph-sdk-go のビルド問題</title><link>/diary/posts/2023/0825/</link><pubDate>Fri, 25 Aug 2023 18:28:18 +0900</pubDate><guid>/diary/posts/2023/0825/</guid><description>1時に寝て何度か起きて7時に起きた。昨日は少し早めにお仕事を終えて家で休んでいたので少し回復した。
msgraph-sdk-go を使った開発 昨日の続き 。前日に作ったマージリクエストをチームのメンバーにレビューしてもらっていくつか修正して、マージを終えた。一段落。
さらにこの sdk を使うことで8月の前半に開発していた差分比較のところも変更しないといけないことに気付いた。public な構造体のメンバーにアクセスして差分比較する処理を実装していたが、この sdk は getter で構造体のメンバーにアクセスしないといけないことに気付いた。reflectoin の処理に追加で実装を入れるだけなのでそんなに難しくはない。そういった修正をしていたら1日終わってしまった。開発していると時間が過ぎるのは早い。
たまたま ci/cd ジョブの実行時間の上限を10分にしていて超えるときがあってジョブが失敗した。 調べてみると、msgraph-sdk-go の api が巨大過ぎてメモリを浪費したりコンパイルに時間がかかったりするという issue をみつけた。
Memory Leak when creating msgraph client #436 私のローカル環境で測ってみると、約36秒で完了していたテストが2分23秒かかるようになっていた。テストの実行が4-5倍ぐらい遅くなった。さらにコンテナイメージのサイズは 36 MiB から 109 MiB と3倍ほど増えた。無駄に開発を遅らせる環境要因になっているのでこれは別途調査して対応しないといけないことに気付いた。</description><content>&lt;p>1時に寝て何度か起きて7時に起きた。昨日は少し早めにお仕事を終えて家で休んでいたので少し回復した。&lt;/p>
&lt;h2 id="msgraph-sdk-go-を使った開発">msgraph-sdk-go を使った開発&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2023/0824/#msgraph-sdk-go-を使った開発">昨日の続き&lt;/a> 。前日に作ったマージリクエストをチームのメンバーにレビューしてもらっていくつか修正して、マージを終えた。一段落。&lt;/p>
&lt;p>さらにこの sdk を使うことで8月の前半に開発していた差分比較のところも変更しないといけないことに気付いた。public な構造体のメンバーにアクセスして差分比較する処理を実装していたが、この sdk は getter で構造体のメンバーにアクセスしないといけないことに気付いた。reflectoin の処理に追加で実装を入れるだけなのでそんなに難しくはない。そういった修正をしていたら1日終わってしまった。開発していると時間が過ぎるのは早い。&lt;/p>
&lt;p>たまたま ci/cd ジョブの実行時間の上限を10分にしていて超えるときがあってジョブが失敗した。
調べてみると、msgraph-sdk-go の api が巨大過ぎてメモリを浪費したりコンパイルに時間がかかったりするという issue をみつけた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/microsoftgraph/msgraph-sdk-go/issues/436">Memory Leak when creating msgraph client #436&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>私のローカル環境で測ってみると、約36秒で完了していたテストが2分23秒かかるようになっていた。テストの実行が4-5倍ぐらい遅くなった。さらにコンテナイメージのサイズは 36 MiB から 109 MiB と3倍ほど増えた。無駄に開発を遅らせる環境要因になっているのでこれは別途調査して対応しないといけないことに気付いた。&lt;/p></content></item><item><title>厄介なインフラ問題をやっつけた</title><link>/diary/posts/2023/0621/</link><pubDate>Wed, 21 Jun 2023 08:24:11 +0900</pubDate><guid>/diary/posts/2023/0621/</guid><description>2時に寝て6時に起きて7時に起きた。夜に作業していたら遅くなった。
厄介なインフラの問題 解決編 運用のトラブルシューティング の続き。アプリケーションアカウントを作って compose 環境を構築したら nginx のコンテナが起動して即時終了する状態になったという。これまで起きていた現象とまた違う問題が発生してさらに混迷をもたらすかに思えたが、私の中では nginx のコンテナでなにかがおかしいと問題の発生箇所を局所化できたのでそこからの調査はそんなに時間を必要としなかった。
結論からいうと podman の aardvark-dns の不具合だった。なんらかのトリガーでコンテナネットワーク内の名前解決が不整合な状態に陥る。
vagrant@bookworm:$ podman-compose exec proxy /bin/bash ... root@3742c45c7c60:/# dig app ; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.16.37-Debian &amp;lt;&amp;lt;&amp;gt;&amp;gt; app ;; global options: +cmd ;; Got answer: ;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NOERROR, id: 56696 ;; flags: qr rd ra ad; QUERY: 1, ANSWER: 8, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: 37ff0fd63315d70e (echoed) ;; QUESTION SECTION: ;app.</description><content>&lt;p>2時に寝て6時に起きて7時に起きた。夜に作業していたら遅くなった。&lt;/p>
&lt;h2 id="厄介なインフラの問題-解決編">厄介なインフラの問題 解決編&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2023/0619/#運用のトラブルシューティング">運用のトラブルシューティング&lt;/a> の続き。アプリケーションアカウントを作って compose 環境を構築したら nginx のコンテナが起動して即時終了する状態になったという。これまで起きていた現象とまた違う問題が発生してさらに混迷をもたらすかに思えたが、私の中では nginx のコンテナでなにかがおかしいと問題の発生箇所を局所化できたのでそこからの調査はそんなに時間を必要としなかった。&lt;/p>
&lt;p>結論からいうと podman の &lt;a href="https://github.com/containers/aardvark-dns">aardvark-dns&lt;/a> の不具合だった。なんらかのトリガーでコンテナネットワーク内の名前解決が不整合な状態に陥る。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>vagrant@bookworm:$ podman-compose exec proxy /bin/bash
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@3742c45c7c60:/# dig app
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.16.37-Debian &amp;lt;&amp;lt;&amp;gt;&amp;gt; app
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; global options: +cmd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; Got answer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; -&amp;gt;&amp;gt;HEADER&lt;span style="color:#e6db74">&amp;lt;&amp;lt;- opco&lt;/span>de: QUERY, status: NOERROR, id: &lt;span style="color:#ae81ff">56696&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; flags: qr rd ra ad; QUERY: 1, ANSWER: 8, AUTHORITY: 0, ADDITIONAL: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; OPT PSEUDOSECTION:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>; EDNS: version: 0, flags:; udp: &lt;span style="color:#ae81ff">4096&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>; COOKIE: 37ff0fd63315d70e &lt;span style="color:#f92672">(&lt;/span>echoed&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; QUESTION SECTION:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;app. IN A
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; ANSWER SECTION:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.36
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.36
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.136
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.136
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.146
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.146
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.156
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.156
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; Query time: &lt;span style="color:#ae81ff">4&lt;/span> msec
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; SERVER: 10.89.0.1#53&lt;span style="color:#f92672">(&lt;/span>10.89.0.1&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; WHEN: Thu Jun &lt;span style="color:#ae81ff">22&lt;/span> 02:45:26 UTC &lt;span style="color:#ae81ff">2023&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; MSG SIZE rcvd: &lt;span style="color:#ae81ff">172&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>podman 4.0 から aardvark-dns がコンテナネットワーク内での dns を提供する。nginx が app を名前解決したときに起動しているコンテナの ip アドレスではなく、削除された過去のコンテナの ip アドレスが返される状況が発生する。app という名前に対して複数の ip アドレスが返る。&lt;/p>
&lt;p>このとき nginx は複数の ip アドレスのうちの1つに接続しようとするが、正しい ip アドレスでない場合、リクエストがタイムアウトする。タイムアウトした後に fallback で他の ip アドレスに接続しにいく。このときに正しい ip アドレスがみつかればクライアントにレスポンスが返る。この fallback のリトライの回数分だけリクエストのレイテンシの時間がかかっていた。&lt;/p>
&lt;pre tabindex="0">&lt;code>vagrant@bookworm:$ podman logs -f proxy
...
2023/06/22 02:46:26 [error] 15#15: *41 connect() failed (113: No route to host) while connecting to upstream, client: 10.89.0.38, server: ucidmsv1-app, request: &amp;#34;GET / HTTP/1.1&amp;#34;, upstream: &amp;#34;http://10.89.0.136:3000/&amp;#34;, host: &amp;#34;localhost:4430&amp;#34;
2023/06/22 02:46:29 [error] 15#15: *41 connect() failed (113: No route to host) while connecting to upstream, client: 10.89.0.38, server: ucidmsv1-app, request: &amp;#34;GET / HTTP/1.1&amp;#34;, upstream: &amp;#34;http://10.89.0.156:3000/&amp;#34;, host: &amp;#34;localhost:4430&amp;#34;
2023/06/22 02:46:32 [error] 15#15: *41 connect() failed (113: No route to host) while connecting to upstream, client: 10.89.0.38, server: ucidmsv1-app, request: &amp;#34;GET / HTTP/1.1&amp;#34;, upstream: &amp;#34;http://10.89.0.136:3000/&amp;#34;, host: &amp;#34;localhost:4430&amp;#34;
10.89.0.38 - - [22/Jun/2023:02:46:32 +0000] &amp;#34;GET / HTTP/1.1&amp;#34; 200 2864 &amp;#34;-&amp;#34; &amp;#34;curl/7.88.1&amp;#34;
&lt;/code>&lt;/pre>&lt;p>ワークアラウンドとして、次のファイルに複数の app の ip アドレスが登録されていれば不整合な状態なのでネットワークを削除して、このファイルも手動で削除してしまえばよい。&lt;/p>
&lt;pre tabindex="0">&lt;code>$ cat /run/user/$(id -u)/containers/networks/aardvark-dns/mynetwork
&lt;/code>&lt;/pre>&lt;p>ファイルを監視していると、どうやら mynetwork ファイルから名前と ip アドレスの情報が削除されるのは該当のコンテナが削除されるタイミングになる。なんらかのエラーにより、コンテナ削除時にマッピングの削除が実行されないと、古いコンテナのマッピング設定が残ったままとなり、compose サービスを起動したときに複数の ip アドレスの名前解決できる状態になってしまう。ちょっと調べても aardvark-dns に関する issue はたくさん登録されている。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containers/podman/issues/18783">https://github.com/containers/podman/issues/18783&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/containers/podman/issues/18530">https://github.com/containers/podman/issues/18530&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/containers/podman/issues/17370">https://github.com/containers/podman/issues/17370&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="コワーキングのオンラインイベント">コワーキングのオンラインイベント&lt;/h2>
&lt;p>月例のカフーツさんのオンラインイベントに参加した。&lt;a href="/diary/diary/posts/2023/0517/#コワーキングのオンラインイベント">先月の所感はここ&lt;/a> 。今日はもともと予定していた話しをする参加者が急遽参加できなくなってしまったので他の参加者での雑談会になった。&lt;/p>
&lt;p>いとうさん曰く、これまで外国人のデジタルノマドは自分で業務時間を選べるフリーランスの、さらにお金に余裕をもった人たちが多いと考えられていた。しかし、実際にコワーキングスペースに来られている外国人にキャリアを伺うと、大企業の普通の社員であることがわかってきた。グローバルな会社だと、働く場所に制限のない会社もあって、ただ日本へ行ってみたかった的な理由で日本へ来られて数ヶ月滞在して普通に会社のお仕事をするといったデジタルノマドもいるという。過去に私が働いていた職場の同僚も、コロナのときに会社がフルリモートワークの体制を設けて、airbnb で全国を旅しながら1年ほど働いていた。日本でもそういう社員はいるのだから外国人はなおさらという感じ。&lt;/p>
&lt;p>そういった外国人のデジタルノマドが要求することが3つある。&lt;/p>
&lt;ul>
&lt;li>24時間利用できること (勤め先の会社と時差があるから)&lt;/li>
&lt;li>セカンドモニターがあること&lt;/li>
&lt;li>・・・ (あともう1つあったが、忘れてしまった)&lt;/li>
&lt;/ul>
&lt;p>コワーキングスペースに外国人のデジタルノマドを呼び込むにはどうすればよいか。実際にコワーキングスペースへ来られた外国人に理由を伺うと英語のホームページをみて来ましたということらしい。至極、当たり前の話し。英語のホームページをちゃんと作ろうねみたいな話題で話していた。&lt;/p></content></item><item><title>運用トラブルの調査</title><link>/diary/posts/2023/0619/</link><pubDate>Mon, 19 Jun 2023 08:31:20 +0900</pubDate><guid>/diary/posts/2023/0619/</guid><description>0時に寝て5時に起きて7時に起きた。もう暑くて家でもエアコンを解禁した。エアコンがあると寝心地が違う、快適。
運用のトラブルシューティング 厄介なインフラの問題 のクリティカルな方から着手し始めた。podman-compose を使って rootless な環境構築をやってみたところ、nginx を tls 終端としてリバースプロキシとするアプリケーションサーバーとの通信が数回に1回ぐらいの頻度で遅くなる。通常は 100msec 程度でレスポンスが返るのが数秒から数十秒かかる。
もともと podman-compose はサポート対象外なのでそんながんばる必要はない。しかし、これも調査の過程でコンテナの技術を学ぶ1つだと考え再現環境を構築しようとした。vagrant の debian 12 と podman-compose をインストールして同様に環境構築してみたが、仮想環境では再現しない。どうやら環境要因のようだ。そこで問題が発生しているマシンで私のアカウントで環境構築してみたが、やはり再現しない。なんと個人アカウントの違いによって起きる現象のようだ。また質が悪いのは私のアカウントでは再現しないが、メンバー2人のアカウントでは再現している。一般ユーザーから他人のユーザーのプロセスやコンテナの情報にアクセスできるわけがないので調査ができない。個人アカウントで compose 環境を構築するのは諦めてアプリケーションアカウントを作ってやりましょうという話しにした。アプリケーションアカウントで再現すれば調査するし、再現しなければこんな環境要因のトラブルシューティングの優先度を下げてもいいかなぁとも考えている。どうなるかなぁ。
サイトデザインのサンプルページ サイトデザイン打ち合わせ の続き。実際のサンプルが出てきたのでデザインの雰囲気やコードも含めて確認していく。ざっとサンプルページを確認した。デザインはとても気に入っている。あとは私が hugo のテーマとしてテンプレートの組み込めるかどうか次第。今週末には実家帰らないといけないし、毎日やることがいっぱいいっぱい。</description><content>&lt;p>0時に寝て5時に起きて7時に起きた。もう暑くて家でもエアコンを解禁した。エアコンがあると寝心地が違う、快適。&lt;/p>
&lt;h2 id="運用のトラブルシューティング">運用のトラブルシューティング&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2023/0613/#厄介なインフラの問題-x-2">厄介なインフラの問題&lt;/a> のクリティカルな方から着手し始めた。&lt;a href="https://github.com/containers/podman-compose">podman-compose&lt;/a> を使って rootless な環境構築をやってみたところ、nginx を tls 終端としてリバースプロキシとするアプリケーションサーバーとの通信が数回に1回ぐらいの頻度で遅くなる。通常は 100msec 程度でレスポンスが返るのが数秒から数十秒かかる。&lt;/p>
&lt;p>もともと podman-compose はサポート対象外なのでそんながんばる必要はない。しかし、これも調査の過程でコンテナの技術を学ぶ1つだと考え再現環境を構築しようとした。vagrant の debian 12 と podman-compose をインストールして同様に環境構築してみたが、仮想環境では再現しない。どうやら環境要因のようだ。そこで問題が発生しているマシンで私のアカウントで環境構築してみたが、やはり再現しない。なんと個人アカウントの違いによって起きる現象のようだ。また質が悪いのは私のアカウントでは再現しないが、メンバー2人のアカウントでは再現している。一般ユーザーから他人のユーザーのプロセスやコンテナの情報にアクセスできるわけがないので調査ができない。個人アカウントで compose 環境を構築するのは諦めてアプリケーションアカウントを作ってやりましょうという話しにした。アプリケーションアカウントで再現すれば調査するし、再現しなければこんな環境要因のトラブルシューティングの優先度を下げてもいいかなぁとも考えている。どうなるかなぁ。&lt;/p>
&lt;h2 id="サイトデザインのサンプルページ">サイトデザインのサンプルページ&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2023/0522/#サイトデザイン打ち合わせ">サイトデザイン打ち合わせ&lt;/a> の続き。実際のサンプルが出てきたのでデザインの雰囲気やコードも含めて確認していく。ざっとサンプルページを確認した。デザインはとても気に入っている。あとは私が hugo のテーマとしてテンプレートの組み込めるかどうか次第。今週末には実家帰らないといけないし、毎日やることがいっぱいいっぱい。&lt;/p></content></item><item><title>アイディアのコラボレーション</title><link>/diary/posts/2023/0616/</link><pubDate>Fri, 16 Jun 2023 09:57:15 +0900</pubDate><guid>/diary/posts/2023/0616/</guid><description>2時に寝て何度か起きて7時に起きた。今日は勉強会の登板なのになにも準備できてなくてどうしようとか思いながら寝た。
パッケージングとリリースのドキュメント 5月の落ち穂拾いの時期にプロジェクトの開発ドキュメントを刷新していろいろな要項を書き残している。最後に残ったまだ書いていないドキュメントに次の2つがある。水曜日から課題管理やコードレビューの隙間時間に書き出していって3日ほどかけて一通り書き終えた。私は文章を書くのが遅いのと、1回ですべての内容を網羅できなくて、書いているうちに思い出して追記したり、寝かした文章をあとで見返して推敲したりすることが多い。そのため、ドキュメントを1週間ぐらいかけて書いたりする。
パッケージング リリース たまたまメンバーにあるモジュールをリファクタリングのために再実装してもらっている。その開発を完了したら古いモジュールと入れ替える必要があるので、開発だけではなく、パッケージングやリリース周りの作業を把握してもらう、よい機会と言える。これまでパッケージングやリリース周りのインフラはほとんど私が作って運用をまわしてきたので徐々にそれらを引き継ぐタイミングとも言える。開発者はインフラのことを気にせず、アプリケーションを開発してコミットすれば後はすべて ci/cd が自動的にいろいろやってくれて便利ぐらいの感覚で扱えるようにするのが、外資系などではプラットフォームに投資しろと言われたりするものだと推測する。うちらが開発しているプロダクトはコンテナ、RPM、Windows インストーラーとパッケージングする種類が多い。その要項に加えて QA 責任の考え方などについても書いておいた。主には私がいなくなった後に役に立つドキュメントとなるだろう。
チーム勉強会 本当は go の generics の勉強会をやりますと先週宣言していた。しかし、私が遊んでいて全然準備できなかったので代わりに決済とスマートプラグの話しをした。先週末と月曜日 に作った決済とスマートプラグを組み合わせたもの。サービスや仕組みを簡単に説明して、実際にデモを動かして電球を灯す。
参加者からスマートプラグをラズベリーパイに置き換えてパワーリレーというモジュールを使えばもっとスマートにできるんじゃないかという意見が出た。たしかにラズベリーパイなら stripe の決済イベントを受け取るサーバーをデバイス内に同梱させることもできるかもしれない。そうすると、スマートプラグのような電源の on/off だけではなく、ラズベリーパイが扱えるセンサーとインテグレーションすることもできそう。また時間のあるときにやってみたい。コラボレーションって正にこういうことを言うとわかってきた。私が1人でこの仕組みを実装していたときにはラズベリーパイというキーワードはまったく頭の中になかった。他者の視点が加わるから新しいアイディアが広がる。</description><content>&lt;p>2時に寝て何度か起きて7時に起きた。今日は勉強会の登板なのになにも準備できてなくてどうしようとか思いながら寝た。&lt;/p>
&lt;h2 id="パッケージングとリリースのドキュメント">パッケージングとリリースのドキュメント&lt;/h2>
&lt;p>5月の落ち穂拾いの時期にプロジェクトの開発ドキュメントを刷新していろいろな要項を書き残している。最後に残ったまだ書いていないドキュメントに次の2つがある。水曜日から課題管理やコードレビューの隙間時間に書き出していって3日ほどかけて一通り書き終えた。私は文章を書くのが遅いのと、1回ですべての内容を網羅できなくて、書いているうちに思い出して追記したり、寝かした文章をあとで見返して推敲したりすることが多い。そのため、ドキュメントを1週間ぐらいかけて書いたりする。&lt;/p>
&lt;ul>
&lt;li>パッケージング&lt;/li>
&lt;li>リリース&lt;/li>
&lt;/ul>
&lt;p>たまたまメンバーにあるモジュールをリファクタリングのために再実装してもらっている。その開発を完了したら古いモジュールと入れ替える必要があるので、開発だけではなく、パッケージングやリリース周りの作業を把握してもらう、よい機会と言える。これまでパッケージングやリリース周りのインフラはほとんど私が作って運用をまわしてきたので徐々にそれらを引き継ぐタイミングとも言える。開発者はインフラのことを気にせず、アプリケーションを開発してコミットすれば後はすべて ci/cd が自動的にいろいろやってくれて便利ぐらいの感覚で扱えるようにするのが、外資系などではプラットフォームに投資しろと言われたりするものだと推測する。うちらが開発しているプロダクトはコンテナ、RPM、Windows インストーラーとパッケージングする種類が多い。その要項に加えて QA 責任の考え方などについても書いておいた。主には私がいなくなった後に役に立つドキュメントとなるだろう。&lt;/p>
&lt;h2 id="チーム勉強会">チーム勉強会&lt;/h2>
&lt;p>本当は go の generics の勉強会をやりますと先週宣言していた。しかし、私が遊んでいて全然準備できなかったので代わりに決済とスマートプラグの話しをした。&lt;a href="/diary/diary/posts/2023/0612/">先週末と月曜日&lt;/a> に作った決済とスマートプラグを組み合わせたもの。サービスや仕組みを簡単に説明して、実際にデモを動かして電球を灯す。&lt;/p>
&lt;figure>&lt;img src="/diary/diary/img/2023/0616_payment-and-smartplug.png"/>
&lt;/figure>
&lt;p>参加者からスマートプラグをラズベリーパイに置き換えてパワーリレーというモジュールを使えばもっとスマートにできるんじゃないかという意見が出た。たしかにラズベリーパイなら stripe の決済イベントを受け取るサーバーをデバイス内に同梱させることもできるかもしれない。そうすると、スマートプラグのような電源の on/off だけではなく、ラズベリーパイが扱えるセンサーとインテグレーションすることもできそう。また時間のあるときにやってみたい。コラボレーションって正にこういうことを言うとわかってきた。私が1人でこの仕組みを実装していたときにはラズベリーパイというキーワードはまったく頭の中になかった。他者の視点が加わるから新しいアイディアが広がる。&lt;/p></content></item><item><title>久しぶりのテックブログの執筆</title><link>/diary/posts/2023/0509/</link><pubDate>Tue, 09 May 2023 20:59:03 +0900</pubDate><guid>/diary/posts/2023/0509/</guid><description>3時に寝て7時に起きた。昨日は連休明け初日なのに2時まで作業していた。開発が落ち着いたので夜に自社のお仕事をする余力が戻ってきたとも言える。
Docker についてのテックブログ執筆 先日から Docker エコシステムの調査 をして、そこでわかったことをテックブログとしてまとめていた。一通り書き終えてマージリクエストを作成して社内レビューを依頼した。当初は 「ライブラリとしての Docker とは何か？」 というタイトルで書いていたものの、レビューを経て「ライブラリとしての」という修飾は必要ないことに気付いた。そのまま Docker のコンポーネントのソフトウェアスタックや過去の経緯や現状の雰囲気がわかるような記事にした。Docker を中心としたコンテナプラットフォームと標準化の概要について追っていく記事に仕上がった。インフラに関心をもつ人たちは少ないかもしれないけど、個人的にはコンテナの学びになってよい記事に仕上がったと思う。調査と執筆とレビューを含めると5人日ぐらいかけている。かけた工数を考えれば当然と言えるかもしれない。
デザイナーさんと契約締結 昨日の続き。デザイナーさんからいただいたワイヤーフレームをレビューして、契約書の叩き台も送付して、先方の所感や意見も伺いながら契約を締結した。基本的にはこちらが提示した契約書の通りに進んだのでとくに揉めることなくうまくいったと言える。顧問のはらさんからデザイナーと契約をするときの要項を事前にヒアリングしていて、その詳細を盛り込めんたこともよかったと思える。サイトデザインをお願いするものの、hugo のテンプレートは私が実装しないといけない。ある意味デザイナーと協調してサイトデザインを構築すると言えるかもしれない。私もそこから学ぶ機会があるだろうし、その過程でできた成果物は hugo のテーマとして oss で公開したい。</description><content>&lt;p>3時に寝て7時に起きた。昨日は連休明け初日なのに2時まで作業していた。開発が落ち着いたので夜に自社のお仕事をする余力が戻ってきたとも言える。&lt;/p>
&lt;h2 id="docker-についてのテックブログ執筆">Docker についてのテックブログ執筆&lt;/h2>
&lt;p>先日から &lt;a href="/diary/diary/posts/2023/0502/#docker-エコシステムの調査">Docker エコシステムの調査&lt;/a> をして、そこでわかったことをテックブログとしてまとめていた。一通り書き終えてマージリクエストを作成して社内レビューを依頼した。当初は &lt;em>「ライブラリとしての Docker とは何か？」&lt;/em> というタイトルで書いていたものの、レビューを経て「ライブラリとしての」という修飾は必要ないことに気付いた。そのまま Docker のコンポーネントのソフトウェアスタックや過去の経緯や現状の雰囲気がわかるような記事にした。Docker を中心としたコンテナプラットフォームと標準化の概要について追っていく記事に仕上がった。インフラに関心をもつ人たちは少ないかもしれないけど、個人的にはコンテナの学びになってよい記事に仕上がったと思う。調査と執筆とレビューを含めると5人日ぐらいかけている。かけた工数を考えれば当然と言えるかもしれない。&lt;/p>
&lt;h2 id="デザイナーさんと契約締結">デザイナーさんと契約締結&lt;/h2>
&lt;p>昨日の続き。デザイナーさんからいただいたワイヤーフレームをレビューして、契約書の叩き台も送付して、先方の所感や意見も伺いながら契約を締結した。基本的にはこちらが提示した契約書の通りに進んだのでとくに揉めることなくうまくいったと言える。顧問のはらさんからデザイナーと契約をするときの要項を事前にヒアリングしていて、その詳細を盛り込めんたこともよかったと思える。サイトデザインをお願いするものの、hugo のテンプレートは私が実装しないといけない。ある意味デザイナーと協調してサイトデザインを構築すると言えるかもしれない。私もそこから学ぶ機会があるだろうし、その過程でできた成果物は hugo のテーマとして oss で公開したい。&lt;/p></content></item><item><title>飛び石のお仕事</title><link>/diary/posts/2023/0502/</link><pubDate>Tue, 02 May 2023 19:02:31 +0900</pubDate><guid>/diary/posts/2023/0502/</guid><description>0時に寝て何度か起きて7時に起きた。休んでもよかったんだけど、休む理由がないので飛び石でお仕事することにした。
docker エコシステムの調査 少し前に docker をライブラリとして使って運用ツールを作った 。その内容をテックブログに書こうと思って docker のソフトウェアスタックやアーキテクチャの背景を調べ直していた。もっとたくさんいろんな記事を読んだのだけど、次の記事とそこから辿れるものを読むとよいだろうと思う。
Docker社がエンタープライズ事業を譲渡した今、Dockerの父が思うこと The differences between Docker, containerd, CRI-O and runc Red Hat, Google Engineers Work on a Way for Kubernetes to Run Containers Without Docker コンテナに関して cri と oci という2つの標準化があることを学び、その実装として docker 社が使っているツールに関係があるのが次の3つになる。おもしろいことにすべて docker 社のリポジトリにはなく、oss として然るべき所管の organization にリポジトリがある。
https://github.com/moby/moby https://github.com/opencontainers/runc https://github.com/containerd/containerd すべて docker 社がオリジナルを作って、いまも moby は docker 社が主体となって開発を継続しているだろうけれど、コンテナの実行環境のプラットフォームは k8s に取って変わられ、cri のコンテナランタイムとしての containerd があれば moby は docker daemon や docker engine のためのツールでしかなくなっている。当初 docker と moby を分割したのは、docker を開発ツール、moby を infrastructure にするという判断の下、moby を k8s のようなプラットフォームにしたかったはずである。しかし、結果的にその標準化競争に破れ containerd があれば dockerd daemon は不要になったとも解釈できる。docker という名前はコンテナのエコシステムにおいて docker 社が提供するコマンドラインやプロダクトの総称としての名前でしかなくなってしまっていて、一世を風靡した docker というパッケージングシステムの開発元に同情してしまう感もある。いまの docker engine は docker daemon と containerd の2つの daemon を起動していて docker 社としては微妙なアーキテクチャになっているのではないかと推測する。とくに swarm なんか最早削除したいだろう。</description><content>&lt;p>0時に寝て何度か起きて7時に起きた。休んでもよかったんだけど、休む理由がないので飛び石でお仕事することにした。&lt;/p>
&lt;h2 id="docker-エコシステムの調査">docker エコシステムの調査&lt;/h2>
&lt;p>少し前に &lt;a href="/diary/diary/posts/2023/0327/">docker をライブラリとして使って運用ツールを作った&lt;/a> 。その内容をテックブログに書こうと思って docker のソフトウェアスタックやアーキテクチャの背景を調べ直していた。もっとたくさんいろんな記事を読んだのだけど、次の記事とそこから辿れるものを読むとよいだろうと思う。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://blog.inductor.me/entry/2019/11/22/072353">Docker社がエンタープライズ事業を譲渡した今、Dockerの父が思うこと&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/">The differences between Docker, containerd, CRI-O and runc&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://thenewstack.io/oci-building-way-kubernetes-run-containers-without-docker/">Red Hat, Google Engineers Work on a Way for Kubernetes to Run Containers Without Docker&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>コンテナに関して cri と oci という2つの標準化があることを学び、その実装として docker 社が使っているツールに関係があるのが次の3つになる。おもしろいことにすべて docker 社のリポジトリにはなく、oss として然るべき所管の organization にリポジトリがある。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/moby/moby">https://github.com/moby/moby&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/opencontainers/runc">https://github.com/opencontainers/runc&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/containerd/containerd">https://github.com/containerd/containerd&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>すべて docker 社がオリジナルを作って、いまも moby は docker 社が主体となって開発を継続しているだろうけれど、コンテナの実行環境のプラットフォームは k8s に取って変わられ、cri のコンテナランタイムとしての containerd があれば moby は docker daemon や docker engine のためのツールでしかなくなっている。当初 docker と moby を分割したのは、docker を開発ツール、moby を infrastructure にするという判断の下、moby を k8s のようなプラットフォームにしたかったはずである。しかし、結果的にその標準化競争に破れ containerd があれば dockerd daemon は不要になったとも解釈できる。docker という名前はコンテナのエコシステムにおいて docker 社が提供するコマンドラインやプロダクトの総称としての名前でしかなくなってしまっていて、一世を風靡した docker というパッケージングシステムの開発元に同情してしまう感もある。いまの docker engine は docker daemon と containerd の2つの daemon を起動していて docker 社としては微妙なアーキテクチャになっているのではないかと推測する。とくに swarm なんか最早削除したいだろう。&lt;/p>
&lt;p>こういった docker を取り巻くエコシステムやツールの背景を説明するだけでも1つの記事になりそうなことが1日調べていてわかった。&lt;/p>
&lt;h2 id="しくじり先生">しくじり先生&lt;/h2>
&lt;p>たまたま &lt;a href="https://www.tv-asahi.co.jp/shikujiri/backnumber2/0099/">竹原慎二先生「５０歳過ぎてもケンカを売られ続けてバリしんどい先生」&lt;/a> をみたらおもしろかった。ガチンコをリアルタイムでみていた世代なので竹原氏には好感をもっている。少し前に &lt;a href="https://www.nhk.or.jp/kenko/atc_731.html">【あの人の健康法】元プロボクサー・竹原慎二の膀胱（ぼうこう）がんとの闘い&lt;/a> のような、闘病生活の記事もみかけていた。病気は克服してがんばっているといるようにみえる。よかった。もう51歳なのか。&lt;/p>
&lt;p>この番組の中で若い頃に上京してボクシングジムへ通ったときに根性の定義が変わったという話しが出てくる。&lt;/p>
&lt;blockquote>
&lt;p>殴られても耐えることを根性だと思っていた。そのときに初めて殴られようが何しようが毎日辛い練習を重ねるのが根性だと気付かされた。&lt;/p>
&lt;/blockquote>
&lt;p>地元で最強だったのが、ボクシングジムのヤンキーでもない先輩にまったく太刀打ちできなかったという。そのとき1番違うのはスタミナだったらしく、竹原氏は1分で息がきれるのに相手は軽く流すといった様相だった。この先輩にボコボコにされた経験を経てそれから心を改めて真面目になったと言う。その後ボクシングと真剣に向き合い、1995年に日本人初のミドル級世界チャンピオンになる。おそらく番組の主旨的に若ものへのメッセージとして「強さ」について話されていた。&lt;/p>
&lt;blockquote>
&lt;p>「ケンカに強い」だけが「強さ」じゃない、「強さ」の意味を履き違えずに生きよう。大人になると、いろんな強さを知る。大切な人のために仕事をどれだけ頑張れるか、辛い状況でもじっと耐えることができるか。そういう強さもある。&lt;/p>
&lt;/blockquote>
&lt;p>私は誰かのために仕事をがんばったことないし、辛い状況を耐えるみたいなこともほぼやってなくて、嫌になったら仕事を辞めてて、こういう言葉にあうと自分を恥じてしまう。その後、喧嘩自慢で youtube 動画を検索していたらまさにそういうのがあった。竹原氏が「勝てるはずねぇだろ、お前らこんな茶番なことすんなよ」とぼやいていた。先の番組の中でも触れていたが、本当に真剣勝負したいと思って来ているのではなく、記念に戦ってみたいという不純な動機でやってくる人が多いらしい。たしかにそれは相手するのがしんどそう。&lt;/p>
&lt;div class="video-container">
&lt;iframe src="https://www.youtube.com/embed/O2rjpxf9W7U" allowfullscreen title="懲りない喧嘩自慢に終止符を打ってみた">&lt;/iframe>
&lt;/div></content></item><item><title>docker compose に期待しない</title><link>/diary/posts/2022/1212/</link><pubDate>Mon, 12 Dec 2022 12:52:27 +0900</pubDate><guid>/diary/posts/2022/1212/</guid><description>0時に寝て5時に起きて7時に起きた。起きたら冷やしたのかお腹痛かったが、まぁまぁ眠れたと思う。
テスト環境の構築 GitLab CI/CD にだいぶ慣れてきてジョブを追加したり改善したりしながらようやくアプリケーションの docker image もコンテナレジストリに push されるようになった。それを pull してきて、テスト環境を docker compose で構築する。Use Compose in production とドキュメントでは威勢がよいが、これが全然イケてない。複数の compose.yml で項目によっては変更したいところを置き換えるといった振る舞いになっていない。例えば、ポート番号などを dev と prod で置き換えたいといった運用の要件を考える。
dev.yml services: myapp: ports: - 18080:8080 prod.yml services: myapp: ports: - 8080:8080 これを次のように指定すると、
$ docker compose -f dev.yml -f prod.yml up -d 実際のサービスは次のように振る舞う。全然あかん。
services: myapp: ports: - 18080:8080 - 8080:8080 他にもそれぞれの yml ファイルで読み込む environment file のマージなどもよくわからない振る舞いをしていて複数の compose.yml で制御するのは断念した。dry の原則に反して設定は重複するけど、それぞれの環境を個別に compose.yml として管理した方が保守コストは小さくなると私は判断した。複数の compose.yml の使い分けのデバッグを1-2日やった後に諦めてテスト環境の構築は完了した。
年金事務所の住所変更手続き 先週 法務局で法人登記の変更申請 をしていて、そのときに問題がなければ今日から登記事項証明書を取得できると案内をもらっていた。決定書が漏れていて再提出というトラブルはあったものの、最小限の損失で留めたせいか、問題なく登記事項証明書を発行できた。住所の変更だけわかればよいので履歴事項証明書ではなく現在事項証明書を発行してみた。この書類もおもしろくて1つ前の住所といまの住所の2つを確認できる。法務局へ行った帰りに年金事務所へ立ち寄って社会保険の住所変更の手続きを行った。次の3つの書類をもって窓口へ。</description><content>&lt;p>0時に寝て5時に起きて7時に起きた。起きたら冷やしたのかお腹痛かったが、まぁまぁ眠れたと思う。&lt;/p>
&lt;h2 id="テスト環境の構築">テスト環境の構築&lt;/h2>
&lt;p>&lt;a href="https://docs.gitlab.com/ee/ci/">GitLab CI/CD&lt;/a> にだいぶ慣れてきてジョブを追加したり改善したりしながらようやくアプリケーションの docker image もコンテナレジストリに push されるようになった。それを pull してきて、テスト環境を &lt;a href="https://docs.docker.com/compose/">docker compose&lt;/a> で構築する。&lt;a href="https://docs.docker.com/compose/production/">Use Compose in production&lt;/a> とドキュメントでは威勢がよいが、これが全然イケてない。複数の compose.yml で項目によっては変更したいところを置き換えるといった振る舞いになっていない。例えば、ポート番号などを dev と prod で置き換えたいといった運用の要件を考える。&lt;/p>
&lt;ul>
&lt;li>dev.yml&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yml" data-lang="yml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">services&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">myapp&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">18080&lt;/span>:&lt;span style="color:#ae81ff">8080&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>prod.yml&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yml" data-lang="yml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">services&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">myapp&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">8080&lt;/span>:&lt;span style="color:#ae81ff">8080&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>これを次のように指定すると、&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ docker compose -f dev.yml -f prod.yml up -d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>実際のサービスは次のように振る舞う。全然あかん。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yml" data-lang="yml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">services&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">myapp&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">18080&lt;/span>:&lt;span style="color:#ae81ff">8080&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">8080&lt;/span>:&lt;span style="color:#ae81ff">8080&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>他にもそれぞれの yml ファイルで読み込む environment file のマージなどもよくわからない振る舞いをしていて複数の compose.yml で制御するのは断念した。dry の原則に反して設定は重複するけど、それぞれの環境を個別に compose.yml として管理した方が保守コストは小さくなると私は判断した。複数の compose.yml の使い分けのデバッグを1-2日やった後に諦めてテスト環境の構築は完了した。&lt;/p>
&lt;h2 id="年金事務所の住所変更手続き">年金事務所の住所変更手続き&lt;/h2>
&lt;p>先週 &lt;a href="/diary/diary/posts/2022/1205/#法務局で法人登記の変更申請">法務局で法人登記の変更申請&lt;/a> をしていて、そのときに問題がなければ今日から登記事項証明書を取得できると案内をもらっていた。決定書が漏れていて再提出というトラブルはあったものの、最小限の損失で留めたせいか、問題なく登記事項証明書を発行できた。住所の変更だけわかればよいので履歴事項証明書ではなく現在事項証明書を発行してみた。この書類もおもしろくて1つ前の住所といまの住所の2つを確認できる。法務局へ行った帰りに年金事務所へ立ち寄って社会保険の住所変更の手続きを行った。次の3つの書類をもって窓口へ。&lt;/p>
&lt;ul>
&lt;li>登記事項証明書: 番地まで記載されている&lt;/li>
&lt;li>オフィスの一時使用契約書: ビル名はあるがこのビル名は来月に改名&lt;/li>
&lt;li>ビル名変更の証明書類: ビル名の変更のみが記載されている&lt;/li>
&lt;/ul>
&lt;p>この3つの書類で完全に指定された住所 (Fully Qualified Address: 造語) を丁寧に説明したところ担当者に納得してもらえて事なきを得た。&lt;/p></content></item><item><title>ログおじさん</title><link>/diary/posts/2022/1111/</link><pubDate>Fri, 11 Nov 2022 05:14:26 +0900</pubDate><guid>/diary/posts/2022/1111/</guid><description>23時に寝て3時半に起きて眠れそうになかったからそのまま5時からオフィスで作業してた。
システム構成の検討 コンサルタントから顧客要件のヒアリングを行い、プロダクトを提供するインフラのシステム概要を mermaid で書いた。オンプレとクラウド環境のそれぞれを同じコンテナアプリケーションで動かすための構成を検討した。クラウド環境の一例として aws の構成を考えていて、https と http のプロトコル変換のようなことをするには api gateway を経由しないといけないと考えていたら、alb に証明書を設定して api gateway なくてもいけるとはらさんに教えてもらった。昔からできたそうで、なぜか私が長い間ずっと勘違いしていた。また時間があるときに自分でもやってみようと思う。
AWS Certificate Managerを使用してインターネットからELBへの通信をHTTPS化してみた Logger の再実装 プロダクトのコアな部分の実装は私がみた方がよいだろうと考えていて、そのうちの1つ Logger の設計がよくなかったので私が作り直した。といっても cybozu-go/log を使った薄いラッパーを設けただけ。チームメンバーからどこでエラーが起きているか追跡しにくいという声があったのでログ出力したところのソースコードの情報を出力しようと考えた。ググればたくさん出てくる。スタックフレームにアクセスする標準パッケージとして runtime を使うとできる。runtime.Caller と runtime.Callers は似て非なる関数のようでファイル名と行番号だけでよければ Caller を使った方がシンプルになると思う。関数名もほしかったら Callers を使ったスタックフレーム自体から取得する必要がある。
func Trace(skip int) (file string, funcName string) { pc := make([]uintptr, 15) n := runtime.Callers(skip, pc) frames := runtime.CallersFrames(pc[:n]) frame, _ := frames.Next() _file := frame.File[strings.Index(frame.File, sourceRepositoryPath)+8:] file = fmt.Sprintf(&amp;#34;%s:%d&amp;#34;, _file, frame.Line) return file, frame.</description><content>&lt;p>23時に寝て3時半に起きて眠れそうになかったからそのまま5時からオフィスで作業してた。&lt;/p>
&lt;h2 id="システム構成の検討">システム構成の検討&lt;/h2>
&lt;p>コンサルタントから顧客要件のヒアリングを行い、プロダクトを提供するインフラのシステム概要を mermaid で書いた。オンプレとクラウド環境のそれぞれを同じコンテナアプリケーションで動かすための構成を検討した。クラウド環境の一例として aws の構成を考えていて、https と http のプロトコル変換のようなことをするには api gateway を経由しないといけないと考えていたら、alb に証明書を設定して api gateway なくてもいけるとはらさんに教えてもらった。昔からできたそうで、なぜか私が長い間ずっと勘違いしていた。また時間があるときに自分でもやってみようと思う。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://dev.classmethod.jp/articles/for-begginer-ssl-communication-by-aws-certificate-manager/">AWS Certificate Managerを使用してインターネットからELBへの通信をHTTPS化してみた&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="logger-の再実装">Logger の再実装&lt;/h2>
&lt;p>プロダクトのコアな部分の実装は私がみた方がよいだろうと考えていて、そのうちの1つ Logger の設計がよくなかったので私が作り直した。といっても &lt;a href="https://github.com/cybozu-go/log">cybozu-go/log&lt;/a> を使った薄いラッパーを設けただけ。チームメンバーからどこでエラーが起きているか追跡しにくいという声があったのでログ出力したところのソースコードの情報を出力しようと考えた。ググればたくさん出てくる。スタックフレームにアクセスする標準パッケージとして runtime を使うとできる。&lt;a href="https://pkg.go.dev/runtime#Caller">runtime.Caller&lt;/a> と runtime.Callers は似て非なる関数のようでファイル名と行番号だけでよければ Caller を使った方がシンプルになると思う。関数名もほしかったら Callers を使ったスタックフレーム自体から取得する必要がある。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">Trace&lt;/span>(&lt;span style="color:#a6e22e">skip&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span>) (&lt;span style="color:#a6e22e">file&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>, &lt;span style="color:#a6e22e">funcName&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">pc&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> make([]&lt;span style="color:#66d9ef">uintptr&lt;/span>, &lt;span style="color:#ae81ff">15&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">n&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">runtime&lt;/span>.&lt;span style="color:#a6e22e">Callers&lt;/span>(&lt;span style="color:#a6e22e">skip&lt;/span>, &lt;span style="color:#a6e22e">pc&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">frames&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">runtime&lt;/span>.&lt;span style="color:#a6e22e">CallersFrames&lt;/span>(&lt;span style="color:#a6e22e">pc&lt;/span>[:&lt;span style="color:#a6e22e">n&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">frame&lt;/span>, &lt;span style="color:#a6e22e">_&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">frames&lt;/span>.&lt;span style="color:#a6e22e">Next&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">_file&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">frame&lt;/span>.&lt;span style="color:#a6e22e">File&lt;/span>[&lt;span style="color:#a6e22e">strings&lt;/span>.&lt;span style="color:#a6e22e">Index&lt;/span>(&lt;span style="color:#a6e22e">frame&lt;/span>.&lt;span style="color:#a6e22e">File&lt;/span>, &lt;span style="color:#a6e22e">sourceRepositoryPath&lt;/span>)&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#ae81ff">8&lt;/span>:]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">file&lt;/span> = &lt;span style="color:#a6e22e">fmt&lt;/span>.&lt;span style="color:#a6e22e">Sprintf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;%s:%d&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">_file&lt;/span>, &lt;span style="color:#a6e22e">frame&lt;/span>.&lt;span style="color:#a6e22e">Line&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">file&lt;/span>, &lt;span style="color:#a6e22e">frame&lt;/span>.&lt;span style="color:#a6e22e">Function&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>この情報を cybozu-go/log の map に追加するようなログ関数を提供するようにした。cybozu-go/log は標準の log パッケージに足りないところだけを追加していて、そのシンプルさと拡張性の高さを私は気に入ってよく使っている。私が気に入っているのでもっと有名になってほしい。&lt;/p>
&lt;p>前のお手伝いでもログ基盤を含めて Logger を作っていて、またいまも Logger を作り直していて、気付いたら私は Logger やログ出力に一家言あるような、ログおじさんになりつつある。&lt;/p></content></item><item><title>副反応終わり</title><link>/diary/posts/2022/1017/</link><pubDate>Mon, 17 Oct 2022 10:00:52 +0900</pubDate><guid>/diary/posts/2022/1017/</guid><description>0時に寝て7時半に起きた。久しぶりによく寝た。ワクチン接種の副反応はおさまり体調は回復した。昼間は UI 改善やバグ修正をしていた。
インフラ勉強会 引き継ぎを兼ねて cdk の使い方、簡単な本番環境向けのデプロイ作業を共有した。wiki に一通りの説明は書いてあって、実際に cdk アプリケーションのソースや設定をみたり、cloud formation のドリフトを検出して解消するのを実践したりしてた。そんな中、別の不具合もみつけた。rds のデータベースのバージョン情報が次のように差分として出力される。
Resources [~] AWS::RDS::DBInstance GodPostgreSQL/Instance1 GodPostgreSQLInstance1857D2683 └─ [-] EngineVersion └─ 13.6 少し前にたまたま cdk のバージョンを 2.44.0 にアップグレードしてあったのだけど、この差分は 2.44.0 以降で発生する。次の issue の説明によると、メジャーバージョンをまたぐ rds のアップグレードを cdk で実行すると、Stack が回復不能な状態になるのでそのワークアラウンドとしてバージョン情報をみえなくしてしまって、誤って cdk で rds のアップグレードできないようにしているとのこと。この差分はとくに気にする必要なく、一度デプロイしてしまえば次回からは出力されなくなる。インフラにも影響は与えないとのこと。
(rds): AWS::RDS::DBInstance should not have EngineVersion property set for Aurora clusters #21758 過去にマイナーバージョンアップは試したことがあって、やはりエラーにはなるのだけど、実際のインフラはアップグレードされて、それで運用上は問題なかったようにみえる。但し、cdk 正しく rds のアップグレードを扱えないというのは確かであるようにみえる。
試しに今日 aurora postgresql のバージョンを 13.4 から 13.6 にアップグレードしたらデプロイは失敗してるのに実際のインフラはアップグレードして、cf のテンプレートが実際のインフラと不整合になった。手動でテンプレートを更新して cdk のコードと同期する必要がある。
&amp;mdash; Tetsuya Morimoto (@t2y) April 23, 2022</description><content>&lt;p>0時に寝て7時半に起きた。久しぶりによく寝た。ワクチン接種の副反応はおさまり体調は回復した。昼間は UI 改善やバグ修正をしていた。&lt;/p>
&lt;h2 id="インフラ勉強会">インフラ勉強会&lt;/h2>
&lt;p>引き継ぎを兼ねて cdk の使い方、簡単な本番環境向けのデプロイ作業を共有した。wiki に一通りの説明は書いてあって、実際に cdk アプリケーションのソースや設定をみたり、cloud formation のドリフトを検出して解消するのを実践したりしてた。そんな中、別の不具合もみつけた。rds のデータベースのバージョン情報が次のように差分として出力される。&lt;/p>
&lt;pre tabindex="0">&lt;code>Resources
[~] AWS::RDS::DBInstance GodPostgreSQL/Instance1 GodPostgreSQLInstance1857D2683
└─ [-] EngineVersion
└─ 13.6
&lt;/code>&lt;/pre>&lt;p>少し前にたまたま cdk のバージョンを 2.44.0 にアップグレードしてあったのだけど、この差分は 2.44.0 以降で発生する。次の issue の説明によると、メジャーバージョンをまたぐ rds のアップグレードを cdk で実行すると、Stack が回復不能な状態になるのでそのワークアラウンドとしてバージョン情報をみえなくしてしまって、誤って cdk で rds のアップグレードできないようにしているとのこと。この差分はとくに気にする必要なく、一度デプロイしてしまえば次回からは出力されなくなる。インフラにも影響は与えないとのこと。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/21758">(rds): AWS::RDS::DBInstance should not have EngineVersion property set for Aurora clusters #21758&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>過去にマイナーバージョンアップは試したことがあって、やはりエラーにはなるのだけど、実際のインフラはアップグレードされて、それで運用上は問題なかったようにみえる。但し、cdk 正しく rds のアップグレードを扱えないというのは確かであるようにみえる。&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="ja" dir="ltr">試しに今日 aurora postgresql のバージョンを 13.4 から 13.6 にアップグレードしたらデプロイは失敗してるのに実際のインフラはアップグレードして、cf のテンプレートが実際のインフラと不整合になった。手動でテンプレートを更新して cdk のコードと同期する必要がある。&lt;/p>&amp;mdash; Tetsuya Morimoto (@t2y) &lt;a href="https://twitter.com/t2y/status/1517831237997838336?ref_src=twsrc%5Etfw">April 23, 2022&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></content></item><item><title>サイトコントローラーの障害は大変そう</title><link>/diary/posts/2022/1013/</link><pubDate>Thu, 13 Oct 2022 08:20:27 +0900</pubDate><guid>/diary/posts/2022/1013/</guid><description>0時に寝て3時に起きて6時半に起きた。だいぶよく眠れるようになってきた。
サイトコントローラーの障害 お手伝いしている宿泊業のシステムでトラブルが発生している。厳密には外部サービスになるのだが、複数のオンライン予約サイトの違いを吸収して単一のインターフェースを提供する宿泊業のハブシステムのような存在をサイトコントローラーと呼ぶらしい。週明けから全国旅行支援という新しいGoToトラベルが開始されて、その予約が想定以上のトラフィックになっていてサイトコントローラーがダウンしてしまった。web 開発者向けに例えれば aws の s3 が落ちて大半のサービスに影響が出てなんもできないみたいな状況かな。
宿泊施設向け予約・販売管理システム「TL-リンカーン」、復旧目処立たず　約5,100施設に影響 全国レベルのニュースになるぐらいの障害規模が大きいらしい。それだけユーザーが多いシステム／事業者なんだろうとは思う。システムと向き合う上で障害が発生するのは仕方ないが、フォールトトレランスは常に意識して設計・運用しないといけないことを、今回の障害を傍からみていて感じた。
java のちょっとした小技 java で1つのリストを複数のリストに分割したいときに List.subList というメソッドがある。複数の値を並行処理するときなど入力を分割するのに便利そう。使い方は次の通り。toIndex を超えると IndexOutOfBoundsException が投げられるのでそこだけ注意かな。
var total = myList.size(); var step = 20; for (int i = 0; i &amp;lt; total; i += step) { var toIndex = i + step; if (toIndex &amp;gt; total) { toIndex = total; } var subList = myList.subList(i, toIndex); // do something } シャンプー 散髪屋さんのマスターからシャンプーを変えた方がいいんじゃないかとアドバイスされた。髪は油分と水分のバランスが大事らしく、シャンプーによって変わることもあるらしい。いつからか記憶にないけど、少なくとも学生時代から私はずっと メリット リンスのいらないシャンプー を使っている。20年ぐらい？こだわりがあるわけでもないし、このシャンプーを使っていて懸念があったことは一度もない。マスターから h&amp;amp;s scalp がよいとお勧めされた。せっかくの機会だから試してみようかなと思う。</description><content>&lt;p>0時に寝て3時に起きて6時半に起きた。だいぶよく眠れるようになってきた。&lt;/p>
&lt;h2 id="サイトコントローラーの障害">サイトコントローラーの障害&lt;/h2>
&lt;p>お手伝いしている宿泊業のシステムでトラブルが発生している。厳密には外部サービスになるのだが、複数のオンライン予約サイトの違いを吸収して単一のインターフェースを提供する宿泊業のハブシステムのような存在をサイトコントローラーと呼ぶらしい。週明けから全国旅行支援という新しいGoToトラベルが開始されて、その予約が想定以上のトラフィックになっていてサイトコントローラーがダウンしてしまった。web 開発者向けに例えれば aws の s3 が落ちて大半のサービスに影響が出てなんもできないみたいな状況かな。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.traicy.com/posts/20221012252499/">宿泊施設向け予約・販売管理システム「TL-リンカーン」、復旧目処立たず　約5,100施設に影響&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>全国レベルのニュースになるぐらいの障害規模が大きいらしい。それだけユーザーが多いシステム／事業者なんだろうとは思う。システムと向き合う上で障害が発生するのは仕方ないが、フォールトトレランスは常に意識して設計・運用しないといけないことを、今回の障害を傍からみていて感じた。&lt;/p>
&lt;h2 id="java-のちょっとした小技">java のちょっとした小技&lt;/h2>
&lt;p>java で1つのリストを複数のリストに分割したいときに &lt;a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/List.html#subList%5C(int,int%5C)">List.subList&lt;/a> というメソッドがある。複数の値を並行処理するときなど入力を分割するのに便利そう。使い方は次の通り。toIndex を超えると IndexOutOfBoundsException が投げられるのでそこだけ注意かな。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-java" data-lang="java">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> total &lt;span style="color:#f92672">=&lt;/span> myList.&lt;span style="color:#a6e22e">size&lt;/span>();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> step &lt;span style="color:#f92672">=&lt;/span> 20;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> (&lt;span style="color:#66d9ef">int&lt;/span> i &lt;span style="color:#f92672">=&lt;/span> 0; i &lt;span style="color:#f92672">&amp;lt;&lt;/span> total; i &lt;span style="color:#f92672">+=&lt;/span> step) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">var&lt;/span> toIndex &lt;span style="color:#f92672">=&lt;/span> i &lt;span style="color:#f92672">+&lt;/span> step;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> (toIndex &lt;span style="color:#f92672">&amp;gt;&lt;/span> total) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> toIndex &lt;span style="color:#f92672">=&lt;/span> total;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">var&lt;/span> subList &lt;span style="color:#f92672">=&lt;/span> myList.&lt;span style="color:#a6e22e">subList&lt;/span>(i, toIndex);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// do something&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="シャンプー">シャンプー&lt;/h2>
&lt;p>散髪屋さんのマスターからシャンプーを変えた方がいいんじゃないかとアドバイスされた。髪は油分と水分のバランスが大事らしく、シャンプーによって変わることもあるらしい。いつからか記憶にないけど、少なくとも学生時代から私はずっと &lt;a href="https://www.kao.co.jp/merit/products/norinse/">メリット リンスのいらないシャンプー&lt;/a> を使っている。20年ぐらい？こだわりがあるわけでもないし、このシャンプーを使っていて懸念があったことは一度もない。マスターから &lt;a href="https://hscare.jp/shop-products/men/all-collections">h&amp;amp;s scalp&lt;/a> がよいとお勧めされた。せっかくの機会だから試してみようかなと思う。&lt;/p></content></item><item><title>勉強会が2つ</title><link>/diary/posts/2022/0922/</link><pubDate>Thu, 22 Sep 2022 08:54:56 +0900</pubDate><guid>/diary/posts/2022/0922/</guid><description>0時に寝て6時に起きた。涼しくて体調は抜群。
インフラ引き継ぎ勉強会 先週引き継ぎのためのインフラドキュメントを書いていたものをチームの開発者に共有した。今日は開発者向けの話しなので 5日以上かけた非開発者向けのインフラドキュメント は使わないが、社員さんによると、(私が参加していない社員さんだけの) 別の開発者チャンネルで読まれているとのこと。時間をかけたので多くの人に読んでもらえるに越したことはない。但し、そのフィードバックは私には一切ないので役に立っているのかどうかの判断しようがない。業務委託にそういう外様感を与えるかどうかは組織によって大きく異なる。過去に働いたある会社では自由に勉強会に参加したり他チームのメンバーとも技術の議論をできたりした。私は技術の話題に対しては真摯なので当たり前のように感じていたが、あの会社のあの文化は特別だったのだとそれから別の数社で働いてみて気付いた。
書いたインフラドキュメントに沿って一通り説明して、ほとんど実務をやっていないメンバーではあまりイメージができないと思うので質問もそれほどなかった。次回は実際にどうやってインフラのデプロイをするかの作業をみんなで確認しながら cdk の使い方などの話しをしようと思う。
インフレ勉強会 fin-py の月例のインフレ勉強会に参加した。背景はわかってないが、connpass イベントではない。市場調査や金融の勉強のために最近は毎月出席している。
明日9/22(木)19:00-20:00にインフレ研究会をやりますhttps://t.co/b6gabjIRYY
研究会といっても物価に関連する雑談です、どなたでも参加できます
Twitterのコミュニティはこちらですhttps://t.co/jBBn07Ahtg
&amp;mdash; driller/どりらん (@patraqushe) September 21, 2022 直前に政府の円買いの為替介入あったのが話題になってた。政府が本気？出せばこんな勢いで5円も動くらしい。為替介入はサプライズが大事らしくて、サプライズという側面ではみんなびっくりしたので効果はあったのかもしれない。
きょうの外国為替市場では、政府・日銀が市場介入を行ったことで、それまで1ドル＝145円台後半の円安ドル高水準で推移していた円相場が、一時、一気に140円台前半まで円高方向に。
夕方の円相場の動きです。https://t.co/2PZ8BqzH00#nhk_video pic.twitter.com/FPg9Ydvihw
&amp;mdash; NHKニュース (@nhk_news) September 22, 2022 fin-py の中の人もこんな勢いで動くのは珍しいと話していた。日本が為替介入するときは米国にお伺いを立てないといけないらしく、米国がうんと言わないと実弾の為替介入はできないことが多いらしい。中の人によれば、いまも米国は日本の為替介入をよくは思ってないだろうと推測されるので、こんなことをずっと続けられるかどうかは懐疑的だという。さらに世の中のトレンドはドル高なので為替介入しても一時的なもので意味がないという考え方もあるとのこと。今回の介入の意味があったかどうかは今後の為替が安定するかどうかで判断される。日本は世界でトップクラスに外貨準備のドルをもっている国なのも事実なので為替介入の実弾もたくさんあるから今後も介入する可能性はある。</description><content>&lt;p>0時に寝て6時に起きた。涼しくて体調は抜群。&lt;/p>
&lt;h2 id="インフラ引き継ぎ勉強会">インフラ引き継ぎ勉強会&lt;/h2>
&lt;p>先週引き継ぎのためのインフラドキュメントを書いていたものをチームの開発者に共有した。今日は開発者向けの話しなので &lt;a href="/diary/diary/posts/2022/0919/#ドキュメントを書き終えた">5日以上かけた非開発者向けのインフラドキュメント&lt;/a> は使わないが、社員さんによると、(私が参加していない社員さんだけの) 別の開発者チャンネルで読まれているとのこと。時間をかけたので多くの人に読んでもらえるに越したことはない。但し、そのフィードバックは私には一切ないので役に立っているのかどうかの判断しようがない。業務委託にそういう外様感を与えるかどうかは組織によって大きく異なる。過去に働いたある会社では自由に勉強会に参加したり他チームのメンバーとも技術の議論をできたりした。私は技術の話題に対しては真摯なので当たり前のように感じていたが、あの会社のあの文化は特別だったのだとそれから別の数社で働いてみて気付いた。&lt;/p>
&lt;p>書いたインフラドキュメントに沿って一通り説明して、ほとんど実務をやっていないメンバーではあまりイメージができないと思うので質問もそれほどなかった。次回は実際にどうやってインフラのデプロイをするかの作業をみんなで確認しながら cdk の使い方などの話しをしようと思う。&lt;/p>
&lt;h2 id="インフレ勉強会">インフレ勉強会&lt;/h2>
&lt;p>&lt;a href="https://fin-py.connpass.com/">fin-py&lt;/a> の月例のインフレ勉強会に参加した。背景はわかってないが、connpass イベントではない。市場調査や金融の勉強のために最近は毎月出席している。&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="ja" dir="ltr">明日9/22(木)19:00-20:00にインフレ研究会をやります&lt;a href="https://t.co/b6gabjIRYY">https://t.co/b6gabjIRYY&lt;/a>&lt;br>研究会といっても物価に関連する雑談です、どなたでも参加できます&lt;br>Twitterのコミュニティはこちらです&lt;a href="https://t.co/jBBn07Ahtg">https://t.co/jBBn07Ahtg&lt;/a>&lt;/p>&amp;mdash; driller/どりらん (@patraqushe) &lt;a href="https://twitter.com/patraqushe/status/1572558645837320193?ref_src=twsrc%5Etfw">September 21, 2022&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>直前に政府の円買いの為替介入あったのが話題になってた。政府が本気？出せばこんな勢いで5円も動くらしい。為替介入はサプライズが大事らしくて、サプライズという側面ではみんなびっくりしたので効果はあったのかもしれない。&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="ja" dir="ltr">きょうの外国為替市場では、政府・日銀が市場介入を行ったことで、それまで1ドル＝145円台後半の円安ドル高水準で推移していた円相場が、一時、一気に140円台前半まで円高方向に。&lt;br>夕方の円相場の動きです。&lt;a href="https://t.co/2PZ8BqzH00">https://t.co/2PZ8BqzH00&lt;/a>&lt;a href="https://twitter.com/hashtag/nhk_video?src=hash&amp;amp;ref_src=twsrc%5Etfw">#nhk_video&lt;/a> &lt;a href="https://t.co/FPg9Ydvihw">pic.twitter.com/FPg9Ydvihw&lt;/a>&lt;/p>&amp;mdash; NHKニュース (@nhk_news) &lt;a href="https://twitter.com/nhk_news/status/1572880709274181632?ref_src=twsrc%5Etfw">September 22, 2022&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>fin-py の中の人もこんな勢いで動くのは珍しいと話していた。日本が為替介入するときは米国にお伺いを立てないといけないらしく、米国がうんと言わないと実弾の為替介入はできないことが多いらしい。中の人によれば、いまも米国は日本の為替介入をよくは思ってないだろうと推測されるので、こんなことをずっと続けられるかどうかは懐疑的だという。さらに世の中のトレンドはドル高なので為替介入しても一時的なもので意味がないという考え方もあるとのこと。今回の介入の意味があったかどうかは今後の為替が安定するかどうかで判断される。日本は世界でトップクラスに外貨準備のドルをもっている国なのも事実なので為替介入の実弾もたくさんあるから今後も介入する可能性はある。&lt;/p></content></item><item><title>インフラと非開発者</title><link>/diary/posts/2022/0915/</link><pubDate>Thu, 15 Sep 2022 08:37:44 +0900</pubDate><guid>/diary/posts/2022/0915/</guid><description>0時に寝て5時に起きて7時に起きた。
インフラのドキュメント作成の続き 昨日はインフラ構成図を主に書いていたが、今日は引き継ぎのために wiki に概要を書いて補足事項なども肉付けしながら、私がやったことや今後運用していく上で知っておくべきことなどをまとめていた。それと同時に backlog の wiki の階層構造なども見直していた。backlog の wiki はタイトルに / を含めることで階層構造を表す。実際の url はエイリアスリンクが使われていて、タイトルとは無関係なので wiki のタイトルを変更することで階層構造が変わる。これはこれでお手軽とも言えるけれど、この階層以下のドキュメントをすべて移動したいといったときは1つずつタイトルを変えないといけないので面倒になる。デイリースクラムのときに来週のインフラ勉強会の時間もスケジュールを抑えてもらった。おそらく1回では終わらないのではないかと推測するが、説明してみて開発者の反応もみながら2回目の有無を決める。
さらに「プロダクトオーナーのためのインフラ入門」というタイトルで別のドキュメントも書き始めた。他の重要な業務がなくて暇だと言ってしまえばそうなのだけど、非開発者向けにクラウドのインフラや自分たちの web システムをどう理解してもらうのがよいか、私自身、試行錯誤で答えをもっているわけではないが、難しいからプロダクトオーナーはインフラを理解しなくてよいというつもりもない。以前からプロダクトオーナーが「自分たちもインフラがどうなっているのかを理解したい」というコメントを何度か聞いていたので筆を取った次第だ。技術的な詳細は理解できないだろうが、いまどきのインフラにおいてどういった概念や考え方が重要なのか、なぜこのような複雑なインフラになってしまっているのか、そうした背景を理解してもらおうと考えている。初めての試みなので、後日やってみて反応をみてふりかえりしようと思う。</description><content>&lt;p>0時に寝て5時に起きて7時に起きた。&lt;/p>
&lt;h2 id="インフラのドキュメント作成の続き">インフラのドキュメント作成の続き&lt;/h2>
&lt;p>昨日はインフラ構成図を主に書いていたが、今日は引き継ぎのために wiki に概要を書いて補足事項なども肉付けしながら、私がやったことや今後運用していく上で知っておくべきことなどをまとめていた。それと同時に backlog の wiki の階層構造なども見直していた。backlog の wiki はタイトルに &lt;code>/&lt;/code> を含めることで階層構造を表す。実際の url はエイリアスリンクが使われていて、タイトルとは無関係なので wiki のタイトルを変更することで階層構造が変わる。これはこれでお手軽とも言えるけれど、この階層以下のドキュメントをすべて移動したいといったときは1つずつタイトルを変えないといけないので面倒になる。デイリースクラムのときに来週のインフラ勉強会の時間もスケジュールを抑えてもらった。おそらく1回では終わらないのではないかと推測するが、説明してみて開発者の反応もみながら2回目の有無を決める。&lt;/p>
&lt;p>さらに「プロダクトオーナーのためのインフラ入門」というタイトルで別のドキュメントも書き始めた。他の重要な業務がなくて暇だと言ってしまえばそうなのだけど、非開発者向けにクラウドのインフラや自分たちの web システムをどう理解してもらうのがよいか、私自身、試行錯誤で答えをもっているわけではないが、難しいからプロダクトオーナーはインフラを理解しなくてよいというつもりもない。以前からプロダクトオーナーが「自分たちもインフラがどうなっているのかを理解したい」というコメントを何度か聞いていたので筆を取った次第だ。技術的な詳細は理解できないだろうが、いまどきのインフラにおいてどういった概念や考え方が重要なのか、なぜこのような複雑なインフラになってしまっているのか、そうした背景を理解してもらおうと考えている。初めての試みなので、後日やってみて反応をみてふりかえりしようと思う。&lt;/p></content></item><item><title>インフラと引き継ぎ</title><link>/diary/posts/2022/0914/</link><pubDate>Wed, 14 Sep 2022 09:35:51 +0900</pubDate><guid>/diary/posts/2022/0914/</guid><description>0時に寝て6時に起きた。
インフラのドキュメント作成 契約終了まであと1ヶ月半。私が唯一引き継ぎしないといけないものにインフラがある。
引き継ぐ前のインフラは本当にひどい状態だった。手抜き工事のまま放置されたような状態だった。cdk のコードから実際のインフラを構築することはできなくて、コード化されていないところを aws のマネジメントコンソールから手作業で設定していた上、どこを手作業で設定していたかの情報は一切残されていなかった。新しいインフラが追加されるときに cdk のコードと乖離があるからデプロイできなくて、それを前任者は直せなくて私が引き継いだという経緯がある。私が1ヶ月ほどかけて30以上の pr を作ってコードと実際のインフラをほぼ完全に同期させた。その過程で3回ほど (テスト環境ではあるが) 障害も発生させている。デプロイしたら手動設定が消えて壊れてしまう。何が手動設定で何がコード管理なのかの情報が何もないのだからデプロイしてみないと動くかどうかわからないという状況だった。そのため、現在のインフラは私が作り直したと言っても過言ではない。cdk のバージョンも v1 から v2 にアップグレードした。半分以上の cdk のコードは私が書いたと思う。その引き継ぎならびにドキュメント化の作業にようやく着手した。これまで書いてなかったのはインフラを管理しているのは私だけだったのでドキュメントなくても運用上の問題はなく、ドキュメントタスクの優先順位が低かったから。今日は draw.io で aws のシステム構成図を書き上げて、wiki のインフラドキュメントの再構築に着手したところ。
今週中に引き継ぎのドキュメントを書いて、来週ぐらいからインフラ勉強会やって引き継ぎを完了させる予定。</description><content>&lt;p>0時に寝て6時に起きた。&lt;/p>
&lt;h2 id="インフラのドキュメント作成">インフラのドキュメント作成&lt;/h2>
&lt;p>契約終了まであと1ヶ月半。私が唯一引き継ぎしないといけないものにインフラがある。&lt;/p>
&lt;p>引き継ぐ前のインフラは本当にひどい状態だった。手抜き工事のまま放置されたような状態だった。cdk のコードから実際のインフラを構築することはできなくて、コード化されていないところを aws のマネジメントコンソールから手作業で設定していた上、どこを手作業で設定していたかの情報は一切残されていなかった。新しいインフラが追加されるときに cdk のコードと乖離があるからデプロイできなくて、それを前任者は直せなくて私が引き継いだという経緯がある。私が1ヶ月ほどかけて30以上の pr を作ってコードと実際のインフラをほぼ完全に同期させた。その過程で3回ほど (テスト環境ではあるが) 障害も発生させている。デプロイしたら手動設定が消えて壊れてしまう。何が手動設定で何がコード管理なのかの情報が何もないのだからデプロイしてみないと動くかどうかわからないという状況だった。そのため、現在のインフラは私が作り直したと言っても過言ではない。cdk のバージョンも v1 から v2 にアップグレードした。半分以上の cdk のコードは私が書いたと思う。その引き継ぎならびにドキュメント化の作業にようやく着手した。これまで書いてなかったのはインフラを管理しているのは私だけだったのでドキュメントなくても運用上の問題はなく、ドキュメントタスクの優先順位が低かったから。今日は draw.io で aws のシステム構成図を書き上げて、wiki のインフラドキュメントの再構築に着手したところ。&lt;/p>
&lt;p>今週中に引き継ぎのドキュメントを書いて、来週ぐらいからインフラ勉強会やって引き継ぎを完了させる予定。&lt;/p></content></item><item><title>たまには画面作り</title><link>/diary/posts/2022/0817/</link><pubDate>Wed, 17 Aug 2022 11:12:26 +0900</pubDate><guid>/diary/posts/2022/0817/</guid><description>1時に寝て6時に起きた。
リファクタリングとインフラ移行 ここ2週間ほどリファクタリングやらインフラ変更やらをしてきて、来週からまた新しい施設がサービスインするので区切りとしてリファクタリングは一旦終わりにする。今日がその集大成となるインフラ移行も含めた本番リリースだった。インフラ移行するときはなにかしら障害が起きる前提で待機しているものの、今日もすんなりと意図した通りに移行できて、してやったりではあるものの、モノ足りなさで拍子抜けしてしまった。また昨日から社内 wiki にも minikube の使い方、k8s cronjob の設計、バッチ処理の設計と実装についてドキュメントなどを書いていた。いままですべて私が1人で担当していたものを他メンバーでも作業できるようにドキュメント化した。近いうちにいなくなるので引き継ぎのドキュメントにもなる。
nuxt で画面作り ここ最近2種類の web api の機能を作ったのでその管理画面も2つ作る必要がある。私はフロントエンド開発の素人なので他のメンバーが作ってくれないかと声をかけてはいたけど、みんな忙しいようなので私が作ることにした。今週は nuxtjs の新規画面の開発をがんばってみようと思う。既存のソースを読む限りはそんなに複雑ではなさそう。素人が雰囲気で実装しても動くんじゃないかと思っている。ソースコードを読んでいて url 設計はめちゃくちゃだし、一覧画面にはページング機能も実装されていない。素人がソースを読んで基本的な骨子や機能が正しく実装されてないことがわかってしまうのは品質レベルとしてなにかがおかしい。圧倒的低品質と呼ぶのか、こんなことが起こってしまうのはよい開発文化がないせいなのだろうと考えている。</description><content>&lt;p>1時に寝て6時に起きた。&lt;/p>
&lt;h2 id="リファクタリングとインフラ移行">リファクタリングとインフラ移行&lt;/h2>
&lt;p>ここ2週間ほどリファクタリングやらインフラ変更やらをしてきて、来週からまた新しい施設がサービスインするので区切りとしてリファクタリングは一旦終わりにする。今日がその集大成となるインフラ移行も含めた本番リリースだった。インフラ移行するときはなにかしら障害が起きる前提で待機しているものの、今日もすんなりと意図した通りに移行できて、してやったりではあるものの、モノ足りなさで拍子抜けしてしまった。また昨日から社内 wiki にも minikube の使い方、k8s cronjob の設計、バッチ処理の設計と実装についてドキュメントなどを書いていた。いままですべて私が1人で担当していたものを他メンバーでも作業できるようにドキュメント化した。近いうちにいなくなるので引き継ぎのドキュメントにもなる。&lt;/p>
&lt;h2 id="nuxt-で画面作り">nuxt で画面作り&lt;/h2>
&lt;p>ここ最近2種類の web api の機能を作ったのでその管理画面も2つ作る必要がある。私はフロントエンド開発の素人なので他のメンバーが作ってくれないかと声をかけてはいたけど、みんな忙しいようなので私が作ることにした。今週は &lt;a href="https://nuxtjs.org/ja/">nuxtjs&lt;/a> の新規画面の開発をがんばってみようと思う。既存のソースを読む限りはそんなに複雑ではなさそう。素人が雰囲気で実装しても動くんじゃないかと思っている。ソースコードを読んでいて url 設計はめちゃくちゃだし、一覧画面にはページング機能も実装されていない。素人がソースを読んで基本的な骨子や機能が正しく実装されてないことがわかってしまうのは品質レベルとしてなにかがおかしい。圧倒的低品質と呼ぶのか、こんなことが起こってしまうのはよい開発文化がないせいなのだろうと考えている。&lt;/p></content></item><item><title>無障害インフラ移行</title><link>/diary/posts/2022/0727/</link><pubDate>Wed, 27 Jul 2022 08:16:42 +0900</pubDate><guid>/diary/posts/2022/0727/</guid><description>0時に寝て6時に起きた。
本番環境リリースのサポート サービスイン のバタバタによって毎週の定例リリースを先週はやらなかった。そのため、今回は2週間分の変更をまとめてリリースすることになった。インフラの変更が複数あったので私も万全の準備をして作業に備えた。
waf を使ったメンテナンス切り替えの導入 k8s のノードグループと nodeSelector の導入 他にもいくつか本番のマスターデータの更新など、箇条書きにした作業項目は15個になった。今回はインフラとアプリケーションが相互に依存する変更を加えているので意図した順番で作業しないといけない。ノードグループ導入による k8s の作業内容は wiki にすべて作業方法を書いていたのでその通りに作業してもらった。メンテナンスモードに切り替える初動もうまくいったので現場の運用担当者が間違って使うこともない。意図した作業内容と順番でうまくリリース作業もできた。メンテナンス時間を2時間もらっていて、宿泊業だとその時間帯が12-14時になる。宿泊客のチェックアウト後、チェックイン前の空き時間だ。メンテナンス後に運用担当者にすぐ確認もしてもらえるし、リリース作業の時間帯としては深夜早朝にしなくてよいのは助かる。私が想定した通りに作業は進み、1時間でリリース作業を終えた。障害も一切なかった。インフラ担当者は想定外の障害に備えて、切り戻しやリスク管理をする。まったく問題なく想定内だったので晴れ晴れしい気持ちになった。こういう気持ちはインフラ担当者にしかわからないと思う。</description><content>&lt;p>0時に寝て6時に起きた。&lt;/p>
&lt;h2 id="本番環境リリースのサポート">本番環境リリースのサポート&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2022/0719/#2つ目のサービスイン">サービスイン&lt;/a> のバタバタによって毎週の定例リリースを先週はやらなかった。そのため、今回は2週間分の変更をまとめてリリースすることになった。インフラの変更が複数あったので私も万全の準備をして作業に備えた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="/diary/diary/posts/2022/0725/#waf-のカスタムレスポンス">waf を使ったメンテナンス切り替えの導入&lt;/a>&lt;/li>
&lt;li>&lt;a href="/diary/diary/posts/2022/0720/">k8s のノードグループと nodeSelector の導入&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>他にもいくつか本番のマスターデータの更新など、箇条書きにした作業項目は15個になった。今回はインフラとアプリケーションが相互に依存する変更を加えているので意図した順番で作業しないといけない。ノードグループ導入による k8s の作業内容は wiki にすべて作業方法を書いていたのでその通りに作業してもらった。メンテナンスモードに切り替える初動もうまくいったので現場の運用担当者が間違って使うこともない。意図した作業内容と順番でうまくリリース作業もできた。メンテナンス時間を2時間もらっていて、宿泊業だとその時間帯が12-14時になる。宿泊客のチェックアウト後、チェックイン前の空き時間だ。メンテナンス後に運用担当者にすぐ確認もしてもらえるし、リリース作業の時間帯としては深夜早朝にしなくてよいのは助かる。私が想定した通りに作業は進み、1時間でリリース作業を終えた。障害も一切なかった。インフラ担当者は想定外の障害に備えて、切り戻しやリスク管理をする。まったく問題なく想定内だったので晴れ晴れしい気持ちになった。こういう気持ちはインフラ担当者にしかわからないと思う。&lt;/p></content></item><item><title>メンテンナンス切り替えの設計</title><link>/diary/posts/2022/0725/</link><pubDate>Mon, 25 Jul 2022 08:14:06 +0900</pubDate><guid>/diary/posts/2022/0725/</guid><description>0時に寝て6時に起きた。
waf のカスタムレスポンス メンテンナンス時にエンドユーザーがアプリケーションにアクセスできないようにしたい。いろんなやり方があるけど、どういった手段で対応するのが最もシンプルで運用も容易かを少し前から頭を悩ませていた。
当初は cloudfront のディストリビューションをアプリケーションの assets とメンテナンスページの html の2つを作って、cdk のデプロイで切り替えできないかと考えた。cloudfront はドメイン名と密接に紐付いていて、同じドメイン名を2つのディストリビューションに設定することはできないようにみえる。あらかじめそれぞれのディストリビューションを2つ作っておいて route53 で必要に応じて向き先を切り替えるといった構成はできなかった。その次に waf について調べていたら waf がルールでカスタムレスポンスを返すことができることに気付いた。
AWS WAFによるリクエストとレスポンスのカスタマイズ ちょうどいまエンドユーザーからのアクセスは ipSet で許可し、それ以外のネットワークからのアクセスをブロックしていた。そのルールを更新して、メンテナンス時はエンドユーザーからのリクエストのみをブロックに変更してカスタムレスポンスを返せることに気付いた。waf を管理するスタックなら cdk デプロイで1分ぐらいで更新できた。このやり方のよいところの1つに開発者の ip アドレスを許可することで開発者だけはアクセスできるようにもできる。あと具体的に cdk でカスタムレスポンスをどう実装するかのドキュメントがわかりにくい。サンプルとしてはこんな感じ。
const acl = new wafv2.CfnWebACL(this, &amp;#39;MyAcl&amp;#39;, { defaultAction: { block: {} }, name: &amp;#39;my-acl&amp;#39;, scope: &amp;#39;CLOUDFRONT&amp;#39;, customResponseBodies: { maintenanceInProgress: { content: &amp;#39;&amp;lt;html lang=&amp;#34;en&amp;#34;&amp;gt;Maintenance in progress&amp;lt;/html&amp;gt;&amp;#39;, contentType: &amp;#39;TEXT_HTML&amp;#39;, }, }, rules: [{ action: { block: { customResponse: { responseCode: 503, customResponseBodyKey: &amp;#39;maintenanceInProgress&amp;#39;, }, }, }, statement: { ipSetReferenceStatement: { arn: ipSet.</description><content>&lt;p>0時に寝て6時に起きた。&lt;/p>
&lt;h2 id="waf-のカスタムレスポンス">waf のカスタムレスポンス&lt;/h2>
&lt;p>メンテンナンス時にエンドユーザーがアプリケーションにアクセスできないようにしたい。いろんなやり方があるけど、どういった手段で対応するのが最もシンプルで運用も容易かを少し前から頭を悩ませていた。&lt;/p>
&lt;p>当初は cloudfront のディストリビューションをアプリケーションの assets とメンテナンスページの html の2つを作って、cdk のデプロイで切り替えできないかと考えた。cloudfront はドメイン名と密接に紐付いていて、同じドメイン名を2つのディストリビューションに設定することはできないようにみえる。あらかじめそれぞれのディストリビューションを2つ作っておいて route53 で必要に応じて向き先を切り替えるといった構成はできなかった。その次に waf について調べていたら waf がルールでカスタムレスポンスを返すことができることに気付いた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aws.amazon.com/jp/blogs/news/customize-requests-and-responses-with-aws-waf/">AWS WAFによるリクエストとレスポンスのカスタマイズ&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>ちょうどいまエンドユーザーからのアクセスは ipSet で許可し、それ以外のネットワークからのアクセスをブロックしていた。そのルールを更新して、メンテナンス時はエンドユーザーからのリクエストのみをブロックに変更してカスタムレスポンスを返せることに気付いた。waf を管理するスタックなら cdk デプロイで1分ぐらいで更新できた。このやり方のよいところの1つに開発者の ip アドレスを許可することで開発者だけはアクセスできるようにもできる。あと具体的に cdk でカスタムレスポンスをどう実装するかのドキュメントがわかりにくい。サンプルとしてはこんな感じ。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-typescript" data-lang="typescript">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">acl&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">new&lt;/span> &lt;span style="color:#a6e22e">wafv2&lt;/span>.&lt;span style="color:#a6e22e">CfnWebACL&lt;/span>(&lt;span style="color:#66d9ef">this&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;MyAcl&amp;#39;&lt;/span>, {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">defaultAction&lt;/span>&lt;span style="color:#f92672">:&lt;/span> { &lt;span style="color:#a6e22e">block&lt;/span>&lt;span style="color:#f92672">:&lt;/span> {} },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">name&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;my-acl&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">scope&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;CLOUDFRONT&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">customResponseBodies&lt;/span>&lt;span style="color:#f92672">:&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">maintenanceInProgress&lt;/span>&lt;span style="color:#f92672">:&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">content&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;&amp;lt;html lang=&amp;#34;en&amp;#34;&amp;gt;Maintenance in progress&amp;lt;/html&amp;gt;&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">contentType&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;TEXT_HTML&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">rules&lt;/span>&lt;span style="color:#f92672">:&lt;/span> [{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">action&lt;/span>&lt;span style="color:#f92672">:&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">block&lt;/span>&lt;span style="color:#f92672">:&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">customResponse&lt;/span>&lt;span style="color:#f92672">:&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">responseCode&lt;/span>: &lt;span style="color:#66d9ef">503&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">customResponseBodyKey&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;maintenanceInProgress&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">statement&lt;/span>&lt;span style="color:#f92672">:&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ipSetReferenceStatement&lt;/span>&lt;span style="color:#f92672">:&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">arn&lt;/span>: &lt;span style="color:#66d9ef">ipSet.attrArn&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>});
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>サービスインは突然に</title><link>/diary/posts/2022/0705/</link><pubDate>Tue, 05 Jul 2022 08:26:22 +0900</pubDate><guid>/diary/posts/2022/0705/</guid><description>1時に寝て6時に起きた。暑くてあまり眠れない。今朝も雨降りで徒歩通勤。
cdk の ECS サービスに紐づくセキュリティグループの設定 明日がサービスイン初日だと思っていたら、私の勘違いで今日だった。私が直近でやっている作業はアプリケーションの直接的な機能ではなく、インフラやバッチ処理などの間接的な機能を作っているわけだけど、それでも1日調整を間違えていて、あれーって感じでサービスインが始まった。とはいえ、私は本番環境にアクセスできなければ、ログすらもみれないので同僚ががんばっているのを傍から応援しつつ、平常通りタスクをこなしていくだけのはずであった。
のほほんと通常通りのタスクをやっていたら、本番環境の ecs サービスと通信できないという連絡がくる。私が前任者から引き継いで構築したインフラなので何だろう？と調査していて、本番環境でセキュリティグループの設定が漏れていることがわかった。これはわかりにくい問題で cdk の FargateService で ecs サービスを構築している。このプロパティは securityGroups のパラメーターをもっている。このパラメーターを指定しない場合、新規にセキュリティグループそのものは作成してくれるけれど、その ecs サービスへ通信するポートへのインバウンドルールは作ってくれない。
securityGroups?
Type: ISecurityGroup[] (optional, default: A new security group is created.)
The security groups to associate with the service. If you do not specify a security group, a new security group is created.
https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_ecs.FargateService.html#securitygroups
テスト環境はセキュリティグループに対して、インバウンドルールを管理画面から手動で追加していたために疎通できていた。セキュリティグループは ecs サービスに紐付いているものだから、ecs サービスを再作成しない限りはインバウンドルールが消えることもなくてこの作業漏れに気付けなかったという落ちだった。さらに、そのセキュリティグループのインバウンドルールを設定したのは私ではない。その説明欄に次のコメントが書かれていた。
atode-kesu
今すぐ消してやろうかという気持ちを抑えつつ、cdk でインバウンドルールを設定したセキュリティグループを紐付けるようにして解決した。疎通ができないと、ロードバランサーのヘルスチェックが通らず、ecs のタスクが延々と再起動を繰り返すというわかりにくい障害となっていた。1時間ぐらい唸っていた。
未検証の本番環境 前節で障害の原因自体はわかりにくいものだが、なぜサービスインの初日にこんなことが起こるのだろうか？という当然の疑問。そう。これまでこのインフラの本番環境は一切検証されていなかった。4月から5月にかけて構築されたインフラだった。この後にデータを格納するための s3 bucket がない、一部の設定はテスト環境の設定しかない、アプリケーションのコード中にテスト環境の設定がハードコードされているとか。追加であちこち直してデプロイしていた。私は本番環境に一切アクセスできないので過去にこれらの検証をすることはできなかったわけではあるけど、いろいろ思うところはあるなぁと感慨に浸っていた。</description><content>&lt;p>1時に寝て6時に起きた。暑くてあまり眠れない。今朝も雨降りで徒歩通勤。&lt;/p>
&lt;h2 id="cdk-の-ecs-サービスに紐づくセキュリティグループの設定">cdk の ECS サービスに紐づくセキュリティグループの設定&lt;/h2>
&lt;p>明日がサービスイン初日だと思っていたら、私の勘違いで今日だった。私が直近でやっている作業はアプリケーションの直接的な機能ではなく、インフラやバッチ処理などの間接的な機能を作っているわけだけど、それでも1日調整を間違えていて、あれーって感じでサービスインが始まった。とはいえ、私は本番環境にアクセスできなければ、ログすらもみれないので同僚ががんばっているのを傍から応援しつつ、平常通りタスクをこなしていくだけのはずであった。&lt;/p>
&lt;p>のほほんと通常通りのタスクをやっていたら、本番環境の ecs サービスと通信できないという連絡がくる。私が前任者から引き継いで構築したインフラなので何だろう？と調査していて、本番環境でセキュリティグループの設定が漏れていることがわかった。これはわかりにくい問題で cdk の FargateService で ecs サービスを構築している。このプロパティは securityGroups のパラメーターをもっている。このパラメーターを指定しない場合、新規にセキュリティグループそのものは作成してくれるけれど、その ecs サービスへ通信するポートへのインバウンドルールは作ってくれない。&lt;/p>
&lt;blockquote>
&lt;p>securityGroups?&lt;/p>
&lt;p>Type: ISecurityGroup[] (optional, default: A new security group is created.)&lt;/p>
&lt;p>The security groups to associate with the service.
If you do not specify a security group, a new security group is created.&lt;/p>
&lt;p>&lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_ecs.FargateService.html#securitygroups">https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_ecs.FargateService.html#securitygroups&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>テスト環境はセキュリティグループに対して、インバウンドルールを管理画面から手動で追加していたために疎通できていた。セキュリティグループは ecs サービスに紐付いているものだから、ecs サービスを再作成しない限りはインバウンドルールが消えることもなくてこの作業漏れに気付けなかったという落ちだった。さらに、そのセキュリティグループのインバウンドルールを設定したのは私ではない。その説明欄に次のコメントが書かれていた。&lt;/p>
&lt;blockquote>
&lt;p>atode-kesu&lt;/p>
&lt;/blockquote>
&lt;p>今すぐ消してやろうかという気持ちを抑えつつ、cdk でインバウンドルールを設定したセキュリティグループを紐付けるようにして解決した。疎通ができないと、ロードバランサーのヘルスチェックが通らず、ecs のタスクが延々と再起動を繰り返すというわかりにくい障害となっていた。1時間ぐらい唸っていた。&lt;/p>
&lt;h2 id="未検証の本番環境">未検証の本番環境&lt;/h2>
&lt;p>前節で障害の原因自体はわかりにくいものだが、なぜサービスインの初日にこんなことが起こるのだろうか？という当然の疑問。そう。これまでこのインフラの本番環境は一切検証されていなかった。4月から5月にかけて構築されたインフラだった。この後にデータを格納するための s3 bucket がない、一部の設定はテスト環境の設定しかない、アプリケーションのコード中にテスト環境の設定がハードコードされているとか。追加であちこち直してデプロイしていた。私は本番環境に一切アクセスできないので過去にこれらの検証をすることはできなかったわけではあるけど、いろいろ思うところはあるなぁと感慨に浸っていた。&lt;/p></content></item><item><title>m1 chip macbook と cdk の追加調査</title><link>/diary/posts/2022/0704/</link><pubDate>Mon, 04 Jul 2022 08:11:50 +0900</pubDate><guid>/diary/posts/2022/0704/</guid><description>0時に寝て7時に起きた。昨日はずっと寝てて、今朝は雨降りで徒歩通勤。週始めからしんどい。
デバッグ調査を断念 以前 m1 chip macbook で aws-lambda-python-alpha のデプロイができない ことについて書いた。同僚がそのためにデプロイできないと運用上の不都合があるので調査してみることにした。ワークアラウンドの1つとして、ビルドに使う Docker イメージを任意のものに置き換える仕組みを使えば、arm64 アーキテクチャでビルド処理のプロセス自体は通ることを確認していた。しかし、その後の python distribution が生成されていなかった。同僚にデバッグを手伝ってもらってログをみていると、python distribution をバンドルする処理のログが出ていない。
おそらく docker イメージのビルド処理ではなく、aws-cdk の core 側の bundling のところに原因がありそうに思える。core のライブラリをみると、たしかにいくつか条件次第で bundling をスキップする実装はあったし、ログが出力されていないことからも意図したステップが実行されていないことだけはわかった。その周辺から当たりをつけて aws-cdk の issues なども検索してみたけど、それっぽい issue をみつけることはできなかった。何よりも私がもっていないマシン環境の、様々な環境設定を調べることもできず、aws-cdk の core をデバッグするのも大変かなと午前中いっぱい調べて断念することに決めた。もしかしたら同僚のホスト環境に特化した問題が発生している可能性もある。
アプリケーションレベルで再現できた問題を解決できないというのは情けないけど、自分でデバッグできないものは仕方ないかと諦めることにした。本当に悔しいけれど。</description><content>&lt;p>0時に寝て7時に起きた。昨日はずっと寝てて、今朝は雨降りで徒歩通勤。週始めからしんどい。&lt;/p>
&lt;h2 id="デバッグ調査を断念">デバッグ調査を断念&lt;/h2>
&lt;p>以前 &lt;a href="/diary/diary/posts/2022/0630/#m1-chip-macbook-で-aws-lambda-python-alpha-のデプロイができない">m1 chip macbook で aws-lambda-python-alpha のデプロイができない&lt;/a> ことについて書いた。同僚がそのためにデプロイできないと運用上の不都合があるので調査してみることにした。ワークアラウンドの1つとして、ビルドに使う Docker イメージを任意のものに置き換える仕組みを使えば、arm64 アーキテクチャでビルド処理のプロセス自体は通ることを確認していた。しかし、その後の python distribution が生成されていなかった。同僚にデバッグを手伝ってもらってログをみていると、python distribution をバンドルする処理のログが出ていない。&lt;/p>
&lt;p>おそらく docker イメージのビルド処理ではなく、aws-cdk の core 側の bundling のところに原因がありそうに思える。core のライブラリをみると、たしかにいくつか条件次第で bundling をスキップする実装はあったし、ログが出力されていないことからも意図したステップが実行されていないことだけはわかった。その周辺から当たりをつけて aws-cdk の issues なども検索してみたけど、それっぽい issue をみつけることはできなかった。何よりも私がもっていないマシン環境の、様々な環境設定を調べることもできず、aws-cdk の core をデバッグするのも大変かなと午前中いっぱい調べて断念することに決めた。もしかしたら同僚のホスト環境に特化した問題が発生している可能性もある。&lt;/p>
&lt;p>アプリケーションレベルで再現できた問題を解決できないというのは情けないけど、自分でデバッグできないものは仕方ないかと諦めることにした。本当に悔しいけれど。&lt;/p></content></item><item><title>m1 chip macbook と cdk/aws-lambda は相性が悪い</title><link>/diary/posts/2022/0630/</link><pubDate>Thu, 30 Jun 2022 08:20:27 +0900</pubDate><guid>/diary/posts/2022/0630/</guid><description>0時に寝て6時に起きた。
m1 chip macbook で aws-lambda-python-alpha のデプロイができない 少し前に aws lambda の管理を serverless framework から cdk 移行した 。lambda 関数は python スクリプトで実装されているので @aws-cdk/aws-lambda-python-alpha を使っている。このライブラリでは python distribution を作るときの python インタープリターをローカルのものではなく docker イメージを使って管理しているようにみえる。私の環境 (linux, x86_64) では何の問題もなかったのだけど、同僚が m1 chip macbook を使っていて、そのマシンからだと docker イメージを使ったビルド処理でエラーが発生する。それは既知の問題で次の issue で報告されている。
(lambda-python): arm64 architecture is not respected #18696 (aws-lambda): Ability to specify CPU architecturefor building image #20907 このワークアラウンドの1つとして Custom Bundling の仕組みがある。任意の Dockerfile を指定することで任意の Docker イメージやプラットフォーム向けにビルド用の python インタープリターを設定できる。そうしたらビルド処理そのものは通るようになったけど、python distribution (python の依存関係を含めたスクリプト群) が asset として生成されない。この現象自体も cdk でよくある issue として報告されていて cdk.</description><content>&lt;p>0時に寝て6時に起きた。&lt;/p>
&lt;h2 id="m1-chip-macbook-で-aws-lambda-python-alpha-のデプロイができない">m1 chip macbook で aws-lambda-python-alpha のデプロイができない&lt;/h2>
&lt;p>少し前に aws lambda の管理を &lt;a href="/diary/diary/posts/2022/0609/#serverless-framework-から-cdk-移行の背景">serverless framework から cdk 移行した&lt;/a> 。lambda 関数は python スクリプトで実装されているので &lt;a href="https://www.npmjs.com/package/@aws-cdk/aws-lambda-python-alpha">@aws-cdk/aws-lambda-python-alpha&lt;/a> を使っている。このライブラリでは python distribution を作るときの python インタープリターをローカルのものではなく docker イメージを使って管理しているようにみえる。私の環境 (linux, x86_64) では何の問題もなかったのだけど、同僚が m1 chip macbook を使っていて、そのマシンからだと docker イメージを使ったビルド処理でエラーが発生する。それは既知の問題で次の issue で報告されている。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/18696">(lambda-python): arm64 architecture is not respected #18696&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/20907">(aws-lambda): Ability to specify CPU architecturefor building image #20907&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>このワークアラウンドの1つとして &lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-lambda-python-alpha-readme.html#custom-bundling">Custom Bundling&lt;/a> の仕組みがある。任意の Dockerfile を指定することで任意の Docker イメージやプラットフォーム向けにビルド用の python インタープリターを設定できる。そうしたらビルド処理そのものは通るようになったけど、python distribution (python の依存関係を含めたスクリプト群) が asset として生成されない。この現象自体も cdk でよくある issue として報告されていて &lt;code>cdk.out&lt;/code> を削除して再実行したら直ったという報告もいくつかあるものの、同僚のマシンではそれでは解決しなかった。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/18459">(aws-lambda-nodejs): Uploaded file must be a non-empty zip #18459&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>私が m1 chip macbook をもっていないので cdk のコードを修正して push して同僚に git pull して実行してもらうみたいな作業になっている。このデバッグはなかなか大変。&lt;/p></content></item><item><title>cdk で既存の eks クラスターを管理すべきか</title><link>/diary/posts/2022/0629/</link><pubDate>Wed, 29 Jun 2022 08:19:59 +0900</pubDate><guid>/diary/posts/2022/0629/</guid><description>0時に寝て6時に起きた。
cdk から既存の eks クラスターを制御する 1ヶ月ほど前に検証していた cdk による eks クラスターの helm 管理 を再検証した。kubectlRoleArn にどういった権限をもつ iam role を設定したらよいかがよくわからなくて苦労していた。最終的にそれが理解できて helm 管理もできるようになったのでまとめておく。
kubectlRoleArn - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually.</description><content>&lt;p>0時に寝て6時に起きた。&lt;/p>
&lt;h2 id="cdk-から既存の-eks-クラスターを制御する">cdk から既存の eks クラスターを制御する&lt;/h2>
&lt;p>1ヶ月ほど前に検証していた &lt;a href="/diary/diary/posts/2022/0518/#cdk-のパッチ検証">cdk による eks クラスターの helm 管理&lt;/a> を再検証した。kubectlRoleArn にどういった権限をもつ iam role を設定したらよいかがよくわからなくて苦労していた。最終的にそれが理解できて helm 管理もできるようになったのでまとめておく。&lt;/p>
&lt;blockquote>
&lt;p>kubectlRoleArn - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually. The trust policy of this role should include the the arn:aws::iam::${accountId}:root principal in order to allow the execution role of the kubectl resource to assume it.&lt;/p>
&lt;p>&lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters">https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>aws-auth の configmap に設定されている system:masters に所属している iam role を調べる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl describe configmap -n kube-system aws-auth
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>この iam role には &lt;code>sts:AssumeRole&lt;/code> 権限を与え、trust relationships に &lt;code>arn:aws:iam::${accountId}:root&lt;/code> といった root ユーザーを含める必要がある。この root ユーザーの設定がないと次のような権限エラーが発生する。この権限エラーの修正方法がわからなくて苦労していた。結果的には関係なかった kubectlLambdaRole の設定も必要なんじゃないかと検証していたのが前回の作業の中心だった。&lt;/p>
&lt;pre tabindex="0">&lt;code>An error occurred (AccessDenied) when calling the AssumeRole operation:
User: arn:aws:sts::${accountId}:assumed-role/xxx is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::${accountId}:role/myrole
Error: Kubernetes cluster unreachable: Get &amp;#34;https://xxx.gr7.ap-northeast-1.eks.amazonaws.com/version
&lt;/code>&lt;/pre>&lt;p>ようやく cdk で既存の eks クラスターをインポートして helm パッケージを管理できるようになった。とはいえ、cdk/cf の実行時間を測ってみると次のようになった。&lt;/p>
&lt;ul>
&lt;li>helm パッケージの新規インストール: 約5分&lt;/li>
&lt;li>helm パッケージのアンインストール: 約25分&lt;/li>
&lt;/ul>
&lt;p>これは cdk が helm パッケージを管理するための lambda 環境を構築/削除するときの時間になる。cdk はアプリケーションの stack から nested stack を作成して、そこに lambda や iam role などをまとめて作成する。一度作成してしまえば、バージョンのアップグレードは30秒ほどで完了した。&lt;/p>
&lt;p>この振る舞いを検証した上で、cdk で eks クラスターをインポートする管理はやめようとチームに提案した。正しい設定を作ってしまえば運用は楽になると言える一面もあるが、新規に helm パッケージを追加するときのちょっとした typo や設定ミスなどがあると、1回の試行に30分かかる。私がこの検証に1週間以上のデバッグ時間を割いている理由がそれに相当する。お手伝い先の運用ではテスト/本番環境ともにローカルから接続できる状態なので helm コマンドを直接実行した方が遥かに管理コストや保守コストを下げると言える。cdk を使って嬉しいことは helm コマンドでわかるバージョン情報と設定内容が cdk のコードとして管理されているぐらいでしかない。ドキュメントと helm コマンドで管理する方が現状ではよいだろうと私は結論付けた。同じような理由で eks クラスターも cdk ではなく eksctl コマンドで管理されている。&lt;/p>
&lt;p>1週間以上の労力と時間を費やしてやらない方がよいとわかったという、一般的には失敗と呼ばれる作業に終わったわけだけど、eks/cdk の勉強にはなった。&lt;/p></content></item><item><title>接待もどき</title><link>/diary/posts/2022/0617/</link><pubDate>Fri, 17 Jun 2022 19:15:58 +0900</pubDate><guid>/diary/posts/2022/0617/</guid><description>親戚と親の来訪 姪が体調悪くて病院で検査を受けるという話しで姉夫婦が車で神戸にやってきて、それに便乗して親もやってきた。朝から道案内したりとかしてた。お昼ご飯は ヒシミツ醤油 というお店に行った。週末とか前を通ると10人は並んでいる。なんでこんなに人気があるんだろう？とずっと不思議に思ってた。どうやら醤油も売っているんだけど、醤油によくあう和食の定食も食べられる飲食店らしい。ご飯が8種類あってお替り自由なのでバリエーションの広さを楽しめる。普通のランチよりちょっと贅沢な雰囲気がするので接待などにも向きそう。但し、ランチのピーク時間外さないと並ばないといけない。
その後、親戚のお土産に 亀井堂総本店のバターサンド を買ってきた。西元町にあるお店で、4年ほど前にたまたまお店の前の通りかかったきっかけで買ってみたらおいしかったのでそれからずっと印象に残っていて、4年ぶりに買ってみた。老舗のお菓子なので接待のお土産にはちょうどよさそうな気がする。たまたま親戚がきたから接待モードになってお店とお土産を開拓してた。
バッチ処理のプラットフォーム検討 昨日の続き。aws batch の機能説明や faq を読んでいたらよさそうなので触ってみた。事前に次の3つのリソースを設定しないといけない。
コンピューティング環境 ジョブキュー ジョブ定義 これらの設定をした後、ジョブというリソースを発行することでジョブ定義の処理が実行される。ecs, fargate, ec2, ec2 spot instace から環境を選べる。ec2 spot instance を使えば安くていいかと思っていたんだけど、セキュリティを考慮すると外部のよく分からんインスタンスでバッチ処理を実行するのは懸念があるなぁと思い始めてやめることにした。aws lambda の代わりに aws batch を使うのはセキュリティの懸念さえなければ悪くはないんだけど、インフラの面倒さはどっちも同じぐらいで immutable infrastructure でバッチ処理のようなものを作るのはなかなか面倒くさい。
チームメンバーと3つの選択肢について議論した。
aws batch を使う eks (k8s) を使う github actions を使う github actions の self-hosted runner に ec2 spot instance を使う記事もみかけた。これもいいかなと思ったんだけど、aws batch 同様、セキュリティの懸念は払拭できないのでダメだと断念した。
Extra CI flexibility with Github Runner on AWS Spot Instances 消去法で eks (k8s) でやることにした。CronJob を使って実装していくことになりそう。</description><content>&lt;h2 id="親戚と親の来訪">親戚と親の来訪&lt;/h2>
&lt;p>姪が体調悪くて病院で検査を受けるという話しで姉夫婦が車で神戸にやってきて、それに便乗して親もやってきた。朝から道案内したりとかしてた。お昼ご飯は &lt;a href="https://hisimitu.thebase.in/">ヒシミツ醤油&lt;/a> というお店に行った。週末とか前を通ると10人は並んでいる。なんでこんなに人気があるんだろう？とずっと不思議に思ってた。どうやら醤油も売っているんだけど、醤油によくあう和食の定食も食べられる飲食店らしい。ご飯が8種類あってお替り自由なのでバリエーションの広さを楽しめる。普通のランチよりちょっと贅沢な雰囲気がするので接待などにも向きそう。但し、ランチのピーク時間外さないと並ばないといけない。&lt;/p>
&lt;figure>&lt;img src="/diary/diary/img/2022/0617_lunch.jpg"/>
&lt;/figure>
&lt;p>その後、親戚のお土産に &lt;a href="https://www.kameido.co.jp/tonowa-index.html">亀井堂総本店のバターサンド&lt;/a> を買ってきた。西元町にあるお店で、4年ほど前にたまたまお店の前の通りかかったきっかけで買ってみたらおいしかったのでそれからずっと印象に残っていて、4年ぶりに買ってみた。老舗のお菓子なので接待のお土産にはちょうどよさそうな気がする。たまたま親戚がきたから接待モードになってお店とお土産を開拓してた。&lt;/p>
&lt;h2 id="バッチ処理のプラットフォーム検討">バッチ処理のプラットフォーム検討&lt;/h2>
&lt;p>昨日の続き。aws batch の機能説明や faq を読んでいたらよさそうなので触ってみた。事前に次の3つのリソースを設定しないといけない。&lt;/p>
&lt;ul>
&lt;li>コンピューティング環境&lt;/li>
&lt;li>ジョブキュー&lt;/li>
&lt;li>ジョブ定義&lt;/li>
&lt;/ul>
&lt;p>これらの設定をした後、ジョブというリソースを発行することでジョブ定義の処理が実行される。ecs, fargate, ec2, ec2 spot instace から環境を選べる。ec2 spot instance を使えば安くていいかと思っていたんだけど、セキュリティを考慮すると外部のよく分からんインスタンスでバッチ処理を実行するのは懸念があるなぁと思い始めてやめることにした。aws lambda の代わりに aws batch を使うのはセキュリティの懸念さえなければ悪くはないんだけど、インフラの面倒さはどっちも同じぐらいで immutable infrastructure でバッチ処理のようなものを作るのはなかなか面倒くさい。&lt;/p>
&lt;p>チームメンバーと3つの選択肢について議論した。&lt;/p>
&lt;ul>
&lt;li>aws batch を使う&lt;/li>
&lt;li>eks (k8s) を使う&lt;/li>
&lt;li>github actions を使う&lt;/li>
&lt;/ul>
&lt;p>github actions の self-hosted runner に ec2 spot instance を使う記事もみかけた。これもいいかなと思ったんだけど、aws batch 同様、セキュリティの懸念は払拭できないのでダメだと断念した。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.incredibuild.com/blog/extra-ci-flexibility-with-github-runner-on-aws-spot-instances">Extra CI flexibility with Github Runner on AWS Spot Instances&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>消去法で eks (k8s) でやることにした。&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob&lt;/a> を使って実装していくことになりそう。&lt;/p></content></item><item><title>バッチ処理のインフラ構築に着手</title><link>/diary/posts/2022/0616/</link><pubDate>Thu, 16 Jun 2022 12:23:17 +0900</pubDate><guid>/diary/posts/2022/0616/</guid><description>0時に寝て5時に起きた。なんか体調がいまいちな雰囲気。
ブログ記事のレビュー 昨日の続き。会社ブログの担当者にもレビューも通って体裁を整えて会社ブログにも転記された。あとは情シスグループのリーダーレビューが最終チェックになるみたい。
aws batch バッチ処理のインフラを構築しようと考えていて、これまで aws lambda で作られているのが非効率だなと思うようになってきた。aws batch を使えないかを検討している。いまバッチ処理のための web api のエンドポイントを実装するスタイルになっていて、この流れでやっていくと将来的にバッチ処理が api サーバーに増えていく。たまたま負荷の高いバッチ処理と通常のリクエストがスパイクするとサーバーを過負荷にして通常業務に影響を及ぼす懸念がある。ec2 を作って cron でバッチ処理動かせばいいやんという気もするけど、せっかく cdk を使っているのでフルマネージドな仕組みで構築できればカッコいいなとも思ったりする。
インフレ研究会 たまたまみかけたので fin-py のインフレ研究会に参加した。前は twitter spaces でやっていて、私はスマホに twitter アプリが入っていないから参加しにくくて微妙だった。最近は brave talk でやっているのでパソコンからも参加しやすくなった。物価、金利、中央銀行の金融政策などの話しを私はまったくわからないのでほとんど聞いているだけ。聞いているうちに世の中のお金の流れが少しずつわかってくればいいかな。国債の値段が下がれば金利が上がるみたいなそういう話しを聞いてそうなんだという感じ。</description><content>&lt;p>0時に寝て5時に起きた。なんか体調がいまいちな雰囲気。&lt;/p>
&lt;h2 id="ブログ記事のレビュー">ブログ記事のレビュー&lt;/h2>
&lt;p>昨日の続き。会社ブログの担当者にもレビューも通って体裁を整えて会社ブログにも転記された。あとは情シスグループのリーダーレビューが最終チェックになるみたい。&lt;/p>
&lt;h2 id="aws-batch">aws batch&lt;/h2>
&lt;p>バッチ処理のインフラを構築しようと考えていて、これまで aws lambda で作られているのが非効率だなと思うようになってきた。&lt;a href="https://aws.amazon.com/jp/batch/">aws batch&lt;/a> を使えないかを検討している。いまバッチ処理のための web api のエンドポイントを実装するスタイルになっていて、この流れでやっていくと将来的にバッチ処理が api サーバーに増えていく。たまたま負荷の高いバッチ処理と通常のリクエストがスパイクするとサーバーを過負荷にして通常業務に影響を及ぼす懸念がある。ec2 を作って cron でバッチ処理動かせばいいやんという気もするけど、せっかく cdk を使っているのでフルマネージドな仕組みで構築できればカッコいいなとも思ったりする。&lt;/p>
&lt;h2 id="インフレ研究会">インフレ研究会&lt;/h2>
&lt;p>たまたまみかけたので &lt;a href="https://fin-py.connpass.com/">fin-py&lt;/a> のインフレ研究会に参加した。前は twitter spaces でやっていて、私はスマホに twitter アプリが入っていないから参加しにくくて微妙だった。最近は &lt;a href="https://brave.com/ja/talk/">brave talk&lt;/a> でやっているのでパソコンからも参加しやすくなった。物価、金利、中央銀行の金融政策などの話しを私はまったくわからないのでほとんど聞いているだけ。聞いているうちに世の中のお金の流れが少しずつわかってくればいいかな。国債の値段が下がれば金利が上がるみたいなそういう話しを聞いてそうなんだという感じ。&lt;/p></content></item><item><title>コンテンツは狙ってバズらない</title><link>/diary/posts/2022/0609/</link><pubDate>Thu, 09 Jun 2022 13:02:23 +0900</pubDate><guid>/diary/posts/2022/0609/</guid><description>1時に寝て8時に起きた。昨晩はたくさん話してテンション上がって眠れなくてバテ気味。
serverless framework から cdk 移行の背景 木曜日はスプリントレビューがある。ステークホルダーが出席する唯一のスプリントイベントなので、大半はステークホルダーとの情報共有や質疑応答、プロジェクトにとっての大きい括りでの現状の共有になる。大半はお手伝い先の社員さん同士のやり取りで、協力会社の開発者は詳細が必要になったときだけ説明するといったイベント。前スプリントで 既存の lambda 関数を serverless framework から cdk へ移行 した。プロジェクトメンバーではないのだけど、業務のリーダークラスの方が cdk に関心をもって質問してくれた。聞くところによると、他プロジェクトでも cdk を使うようになってきているらしく、なぜいま移行しているのか？という質問だった。普段インフラ作業を孤独にやっているのもあって、業務の人が関心を示してくれたのが嬉しくて、変なスイッチが入っていろいろ説明した。serverless framework は2015年10月リリース、cdk は2018年7月リリースで、歴史的に serverless framework が普及して、その後に cdk が台頭してきたので実績や機能性から serverless framework が広く利用されている。但し、いま aws のインフラ管理をするのであれば、cdk は serverless framework 以上の管理ができることから cdk に一元管理した方がツールの学習コストを減らし、保守コストを下げることにつながるといった話しを丁寧に説明した。相手がそこまでの回答を求めていたかはわからないけど、関心を示してくれたことそのもので私が救われた気がした。
terapyon channel のコンテンツ公開 昨日の今日で公開された。ほとんど無編集だったのかな。web サイトのコンテンツ紹介の内容も手伝って夕方には公開された。
Podcast terapyon channel 「#58 もりもとさんの年1ゲスト会 マイクロ法人と開発ワークフローのカイゼン話ほか」を共有 https://t.co/aQfbuA6XrO #terapyon_channel
&amp;mdash; terapyon (Manabu TERADA) (@terapyon) June 9, 2022 私の中ではいろんな話題を楽しく話せて充実感があった。一部にだけ関心をもつ人にも聞きたいところだけ聞けていいんじゃないかと思えた。基本的に自分の podcast を聞き直すことはないんだけど、今回は自分が充実感をもって話せたせいか、2回ほど聞き直しておもしろい話だなぁと自画自賛してた (笑) 。自分がよいものは周りもそう思うはずだと、ついつい先入観で考えがちだけど、全然そうじゃなかった。全くいいねがつかなかったので周りからみたら私の自己満足のコンテンツでしかなかった (笑) 。コンテンツあるあるだけど、狙ってコンテンツをバズらせるのは難しい。ブログでもがんばって書いた記事が全く読まれないことはあるし、手間暇かけずに軽く書いた記事がめっちゃバズったこともある。コンテンツがバズるかどうかは、最初にみた人たちが拡散するかにも依ってくる。いずれにしても、他者が関心をもつようなコンテンツかどうかは本人ではわからないというのは確かかな。
今期から会社のマーケティングも少しずつやっていく予定になる。自分がよいと思ったコンテンツに全く関心を示されないことは今後も多々あるだろう。作ったコンテンツを多くの人に見聞きしてもらおうと思ったらやることは1つだけで、当たるまでひたすら作り続けて、いつか当たるのを気長に待つという戦略しか、いまのところ思いつかない。</description><content>&lt;p>1時に寝て8時に起きた。昨晩はたくさん話してテンション上がって眠れなくてバテ気味。&lt;/p>
&lt;h2 id="serverless-framework-から-cdk-移行の背景">serverless framework から cdk 移行の背景&lt;/h2>
&lt;p>木曜日はスプリントレビューがある。ステークホルダーが出席する唯一のスプリントイベントなので、大半はステークホルダーとの情報共有や質疑応答、プロジェクトにとっての大きい括りでの現状の共有になる。大半はお手伝い先の社員さん同士のやり取りで、協力会社の開発者は詳細が必要になったときだけ説明するといったイベント。前スプリントで &lt;a href="/diary/diary/posts/2022/0601/#mvpminimum-viable-productで対応した">既存の lambda 関数を serverless framework から cdk へ移行&lt;/a> した。プロジェクトメンバーではないのだけど、業務のリーダークラスの方が cdk に関心をもって質問してくれた。聞くところによると、他プロジェクトでも cdk を使うようになってきているらしく、なぜいま移行しているのか？という質問だった。普段インフラ作業を孤独にやっているのもあって、業務の人が関心を示してくれたのが嬉しくて、変なスイッチが入っていろいろ説明した。serverless framework は2015年10月リリース、cdk は2018年7月リリースで、歴史的に serverless framework が普及して、その後に cdk が台頭してきたので実績や機能性から serverless framework が広く利用されている。但し、いま aws のインフラ管理をするのであれば、cdk は serverless framework 以上の管理ができることから cdk に一元管理した方がツールの学習コストを減らし、保守コストを下げることにつながるといった話しを丁寧に説明した。相手がそこまでの回答を求めていたかはわからないけど、関心を示してくれたことそのもので私が救われた気がした。&lt;/p>
&lt;h2 id="terapyon-channel-のコンテンツ公開">terapyon channel のコンテンツ公開&lt;/h2>
&lt;p>昨日の今日で公開された。ほとんど無編集だったのかな。web サイトのコンテンツ紹介の内容も手伝って夕方には公開された。&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="ja" dir="ltr">Podcast terapyon channel 「#58 もりもとさんの年1ゲスト会 マイクロ法人と開発ワークフローのカイゼン話ほか」を共有 &lt;a href="https://t.co/aQfbuA6XrO">https://t.co/aQfbuA6XrO&lt;/a> &lt;a href="https://twitter.com/hashtag/terapyon_channel?src=hash&amp;amp;ref_src=twsrc%5Etfw">#terapyon_channel&lt;/a>&lt;/p>&amp;mdash; terapyon (Manabu TERADA) (@terapyon) &lt;a href="https://twitter.com/terapyon/status/1534817292080451584?ref_src=twsrc%5Etfw">June 9, 2022&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>私の中ではいろんな話題を楽しく話せて充実感があった。一部にだけ関心をもつ人にも聞きたいところだけ聞けていいんじゃないかと思えた。基本的に自分の podcast を聞き直すことはないんだけど、今回は自分が充実感をもって話せたせいか、2回ほど聞き直しておもしろい話だなぁと自画自賛してた (笑) 。自分がよいものは周りもそう思うはずだと、ついつい先入観で考えがちだけど、全然そうじゃなかった。全くいいねがつかなかったので周りからみたら私の自己満足のコンテンツでしかなかった (笑) 。コンテンツあるあるだけど、狙ってコンテンツをバズらせるのは難しい。ブログでもがんばって書いた記事が全く読まれないことはあるし、手間暇かけずに軽く書いた記事がめっちゃバズったこともある。コンテンツがバズるかどうかは、最初にみた人たちが拡散するかにも依ってくる。いずれにしても、他者が関心をもつようなコンテンツかどうかは本人ではわからないというのは確かかな。&lt;/p>
&lt;p>今期から会社のマーケティングも少しずつやっていく予定になる。自分がよいと思ったコンテンツに全く関心を示されないことは今後も多々あるだろう。作ったコンテンツを多くの人に見聞きしてもらおうと思ったらやることは1つだけで、当たるまでひたすら作り続けて、いつか当たるのを気長に待つという戦略しか、いまのところ思いつかない。&lt;/p></content></item><item><title>aws sns を介した pubsub</title><link>/diary/posts/2022/0606/</link><pubDate>Mon, 06 Jun 2022 04:43:44 +0900</pubDate><guid>/diary/posts/2022/0606/</guid><description>0時に寝て4時半に起きた。21時頃からオフィスで作業してたらそのまま寝落ちした。朝の掃除機をかける音で目覚めて、始業までワーケーションのふりかえりをしていた。
lambda 関数と sns の連携 定期実行で数百程度の web api 呼び出しを行いたい。これまで定期実行を lambda 関数で実装してきたが、負荷分散を考慮して作ってほしいと言われたので sns を使ってメッセージ分割した上で1つ1つの lambda 関数は sns のメッセージを受け取って実行されるように構成した。lambda 関数を使って pubsub するときは sns を使えばよいらしい。sns はあまり使ったことがないので私自身ノウハウをもっていないし、運用の勘所もよくわかっていない。ドキュメントをいくつか読みながら cdk のコードを書いてた。producer と consumer を lambda 関数で作成し、sns を介してリクエストの負荷分散を図る。lambda 関数は同時並行数を設定できるのでこれがスロットル制限のような役割にもなる。インフラやスクリプトのコードはすぐに実装できたが、lambda 関数の destroy にやたら時間がかかる。権限周りでいくつか設定を試すために destroy しながら検証をしたかった。destroy するのに20分はかかるので deploy や設定の手作業などをやっていると1つの設定を試すのに平気で1時間ぐらいかかってしまう。3回ぐらいやって疲れて検証作業はやや妥協した。
Amazon SNS でのAWS Lambdaの使用 Managing Lambda reserved concurrency Amazon SNS security best practices</description><content>&lt;p>0時に寝て4時半に起きた。21時頃からオフィスで作業してたらそのまま寝落ちした。朝の掃除機をかける音で目覚めて、始業までワーケーションのふりかえりをしていた。&lt;/p>
&lt;h2 id="lambda-関数と-sns-の連携">lambda 関数と sns の連携&lt;/h2>
&lt;p>定期実行で数百程度の web api 呼び出しを行いたい。これまで定期実行を lambda 関数で実装してきたが、負荷分散を考慮して作ってほしいと言われたので sns を使ってメッセージ分割した上で1つ1つの lambda 関数は sns のメッセージを受け取って実行されるように構成した。lambda 関数を使って pubsub するときは sns を使えばよいらしい。sns はあまり使ったことがないので私自身ノウハウをもっていないし、運用の勘所もよくわかっていない。ドキュメントをいくつか読みながら cdk のコードを書いてた。producer と consumer を lambda 関数で作成し、sns を介してリクエストの負荷分散を図る。lambda 関数は同時並行数を設定できるのでこれがスロットル制限のような役割にもなる。インフラやスクリプトのコードはすぐに実装できたが、lambda 関数の destroy にやたら時間がかかる。権限周りでいくつか設定を試すために destroy しながら検証をしたかった。destroy するのに20分はかかるので deploy や設定の手作業などをやっていると1つの設定を試すのに平気で1時間ぐらいかかってしまう。3回ぐらいやって疲れて検証作業はやや妥協した。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.aws.amazon.com/ja_jp/lambda/latest/dg/with-sns.html">Amazon SNS でのAWS Lambdaの使用&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html">Managing Lambda reserved concurrency&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/sns/latest/dg/sns-security-best-practices.html">Amazon SNS security best practices&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>draw.io で描いたインフラ構成図</title><link>/diary/posts/2022/0602/</link><pubDate>Thu, 02 Jun 2022 08:51:06 +0900</pubDate><guid>/diary/posts/2022/0602/</guid><description>0時に寝て4時に起きて7時までだらだらしてた。なんか調子悪い。
draw.io を描いてみた 先日、draw.io で aws 構成図を描く調査 をした。割り込みの作業をやっていてシステム構成図の作成を先延ばししていた。だいたいの調査は終わっていたのであとは根を詰めて描くだけ。次のサンプル構成図をみながら同じように描いていく。
AWS のアーキテクチャ図を描きたい ! でもどうすれば良いの ? 2つの環境があって、そのうちの1つを作成した。新規構築した環境でスクラッチから描いたものの、インフラリソースの構成要素が少なかったのでサンプル構成図を参考にしながらすぐに描けた。半角スペースで文字位置を調整したりすると、github 上で svg 表示したときに文字の位置がずれたりするのでそういうやり方はダメだとわかった。あと draw.io の振る舞いなのか、vscode のプラグインのせいなのかわからないけど、オブジェクトの配置の前後関係をうまく調整できなくてコピペし直したり、なにかの操作をしたタイミングでインフラリソースのアイコンが後ろに隠蔽されていたりもした。リソース間の接続のための線も自動的に繋がるときもあって便利なのだが、誤動作して変な位置にレイアウトされることもあって制御が難しい。私の感覚では、多少の利便性のために自動化されるよりも、自分で思い通りに制御出来る方を好む。draw.io の自動調整機能の制御が難しいなと思った。</description><content>&lt;p>0時に寝て4時に起きて7時までだらだらしてた。なんか調子悪い。&lt;/p>
&lt;h2 id="drawio-を描いてみた">draw.io を描いてみた&lt;/h2>
&lt;p>先日、&lt;a href="/diary/diary/posts/2022/0523/">draw.io で aws 構成図を描く調査&lt;/a> をした。割り込みの作業をやっていてシステム構成図の作成を先延ばししていた。だいたいの調査は終わっていたのであとは根を詰めて描くだけ。次のサンプル構成図をみながら同じように描いていく。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aws.amazon.com/jp/builders-flash/202204/way-to-draw-architecture/">AWS のアーキテクチャ図を描きたい ! でもどうすれば良いの ?&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>2つの環境があって、そのうちの1つを作成した。新規構築した環境でスクラッチから描いたものの、インフラリソースの構成要素が少なかったのでサンプル構成図を参考にしながらすぐに描けた。半角スペースで文字位置を調整したりすると、github 上で svg 表示したときに文字の位置がずれたりするのでそういうやり方はダメだとわかった。あと draw.io の振る舞いなのか、vscode のプラグインのせいなのかわからないけど、オブジェクトの配置の前後関係をうまく調整できなくてコピペし直したり、なにかの操作をしたタイミングでインフラリソースのアイコンが後ろに隠蔽されていたりもした。リソース間の接続のための線も自動的に繋がるときもあって便利なのだが、誤動作して変な位置にレイアウトされることもあって制御が難しい。私の感覚では、多少の利便性のために自動化されるよりも、自分で思い通りに制御出来る方を好む。draw.io の自動調整機能の制御が難しいなと思った。&lt;/p></content></item><item><title>aws サービスと ipv6</title><link>/diary/posts/2022/0530/</link><pubDate>Mon, 30 May 2022 08:22:52 +0900</pubDate><guid>/diary/posts/2022/0530/</guid><description>0時に寝て6時頃に起きて7時半に起きた。
cloudfront と waf と s3 と ipv6 と aws サービスについて ipv6 について調べると、少しずつ対応してきた経緯があって、ipv4 と ipv6 の両対応のことを aws は dualstack と読んでいる。エンドポイントのサブドメインに dualstack があれば、ipv6 対応していると考えてよいと思う。リモートワークしている開発者がテスト環境に接続するときに、その開発者の IP アドレスを登録する aws lambda 関数がある。その lambda 関数にリクスとすると、リクエストしたクライアントの IP アドレスを waf の IP sets に登録することで、パケットフィルタリングのルールを更新している。たまたま、そのインフラを cdk 移行したところ、ある開発者は ipv6 で登録しようとしてエラーになるということがわかった。移行のときに api gateway を使わずに直接 lambda の fuction url を使うようにしたところ、lambda は ipv6 対応しているのでそのまま ipv6 の IP アドレスを登録しようとして waf の設定が ipv4 しか対応していなかったためにエラーになっていた。
AWS Lambda がインバウンド接続用のインターネットプロトコルバージョン 6 (IPv6) エンドポイントのサポートを開始 cloudfront, waf, s3 の ipv6 対応は2016年頃に対応していて、この3つのサービスに対して一緒にブログ記事を書いているのは、これらのサービスを一緒に使うことを想定しているとみなすべきだろう。</description><content>&lt;p>0時に寝て6時頃に起きて7時半に起きた。&lt;/p>
&lt;h2 id="cloudfront-と-waf-と-s3-と-ipv6-と">cloudfront と waf と s3 と ipv6 と&lt;/h2>
&lt;p>aws サービスについて ipv6 について調べると、少しずつ対応してきた経緯があって、ipv4 と ipv6 の両対応のことを aws は dualstack と読んでいる。エンドポイントのサブドメインに dualstack があれば、ipv6 対応していると考えてよいと思う。リモートワークしている開発者がテスト環境に接続するときに、その開発者の IP アドレスを登録する aws lambda 関数がある。その lambda 関数にリクスとすると、リクエストしたクライアントの IP アドレスを waf の IP sets に登録することで、パケットフィルタリングのルールを更新している。たまたま、そのインフラを cdk 移行したところ、ある開発者は ipv6 で登録しようとしてエラーになるということがわかった。移行のときに api gateway を使わずに直接 lambda の fuction url を使うようにしたところ、lambda は ipv6 対応しているのでそのまま ipv6 の IP アドレスを登録しようとして waf の設定が ipv4 しか対応していなかったためにエラーになっていた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aws.amazon.com/jp/about-aws/whats-new/2021/12/aws-lambda-ipv6-endpoints-inbound-connections/">AWS Lambda がインバウンド接続用のインターネットプロトコルバージョン 6 (IPv6) エンドポイントのサポートを開始&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>cloudfront, waf, s3 の ipv6 対応は2016年頃に対応していて、この3つのサービスに対して一緒にブログ記事を書いているのは、これらのサービスを一緒に使うことを想定しているとみなすべきだろう。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aws.amazon.com/jp/blogs/news/ipv6-support-update-cloudfront-waf-and-s3-transfer-acceleration/">IPv6 サポートの更新 – CloudFront、WAF、S3 Transfer Acceleration&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>cdk のコードで cloudfront, waf は ipv6 対応したのだけど、cloudfront のオリジンに当たる s3 の ipv6 対応を cdk からどうやって設定していいかわからなかった。&lt;a href="https://github.com/aws/aws-cdk/tree/master/packages/%40aws-cdk/aws-cloudfront-origins">aws-cloudfront-origins&lt;/a> の &lt;code>S3Origin&lt;/code> のコードを読むと、どうも s3 の ipv6 対応のエンドポイント (dualstack) を考慮して制御しているようにはみえない。バグではないけど、設定方法がわからないので feature request として issue 登録してみた。aws-cdk に issue 登録するのは初めて。余談だけど、&lt;a href="https://github.com/aws/aws-cdk/tree/master/.github/ISSUE_TEMPLATE">ISSUE_TEMPLATE&lt;/a> もよく出来ていて、これを参考にして自分たちのリポジトリにも取り入れてもよさそう。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/20550">(aws-cloudfront-origins): Support dualstack domain name (ipv6) for S3 origin #20550&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>severless framework vs cdk</title><link>/diary/posts/2022/0525/</link><pubDate>Wed, 25 May 2022 08:11:17 +0900</pubDate><guid>/diary/posts/2022/0525/</guid><description>22時に寝て1時に起きて5時半に起きた。前日はあまり寝てなかったので知らないうちに寝てしまった。
severless framework と cdk の比較 serverless framework というツールがある。マルチクラウド対応で aws で言えば lambda のような、サーバーレスのアプリケーションを簡単にデプロイするためのツール。よく使われているようで、私がお手伝いしている会社でも lambda 関数のデプロイにこのツールを使っている。いろいろ調べていたら、serverless framework は cdk と真正面から競合になるツールで、cdk をすでに使っているなら cdk に一元管理した方が保守コストが下がっていいだろうと考え、新たに lambda 関数を作る作業を cdk でやってみた。私はほとんど serverless framework を使ったことがないので正しく理解できていない可能性は高いが、ざっくり比較すると次になる。
serverless framework メリット
cdk より学習コストが低い yaml 設定だけで簡単にデプロイできる デメリット
リソース管理のための s3 バケットを必要とする lambda に関連するリソースしかデプロイできない 開発者が明示的に設定しなくても裏でリソースを勝手に生成するので意図せず適切に管理されていないインフラを作ってしまう懸念がある 大半の実務レベルのアプリケーションでは cf テンプレートの dsl を yaml に設定する必要があり、設定が煩雑になったり見通しが悪くなる cdk メリット
任意の aws インフラのリソースを管理できる プログラミング言語で記述できるので動的なリソースの依存関係を定義できる cf は裏に隠蔽されているが、serverless framework のような dsl を記述する必要はない デメリット
学習コストが高い リファレンス
AWS CDK vs Serverless Framework Serverless Framework vs SAM vs AWS CDK 結論から言うと、将来的に cdk を使うならまず cdk を使った方がよい。本当にシンプルな要件で lambda 関数のインフラしか扱わないなら serverless framework でもいいかもしれない。serverless framework は cdk がない時代に作られたツールだろうからいまから新規に導入する場合は、多少の学習コストを払っても cdk を学んでおけば、将来的に役に立つ場面が多いと思う。</description><content>&lt;p>22時に寝て1時に起きて5時半に起きた。前日はあまり寝てなかったので知らないうちに寝てしまった。&lt;/p>
&lt;h2 id="severless-framework-と-cdk-の比較">severless framework と cdk の比較&lt;/h2>
&lt;p>&lt;a href="https://www.serverless.com/">serverless framework&lt;/a> というツールがある。マルチクラウド対応で aws で言えば lambda のような、サーバーレスのアプリケーションを簡単にデプロイするためのツール。よく使われているようで、私がお手伝いしている会社でも lambda 関数のデプロイにこのツールを使っている。いろいろ調べていたら、serverless framework は cdk と真正面から競合になるツールで、cdk をすでに使っているなら cdk に一元管理した方が保守コストが下がっていいだろうと考え、新たに lambda 関数を作る作業を cdk でやってみた。私はほとんど serverless framework を使ったことがないので正しく理解できていない可能性は高いが、ざっくり比較すると次になる。&lt;/p>
&lt;h4 id="serverless-framework">serverless framework&lt;/h4>
&lt;p>メリット&lt;/p>
&lt;ul>
&lt;li>cdk より学習コストが低い&lt;/li>
&lt;li>yaml 設定だけで簡単にデプロイできる&lt;/li>
&lt;/ul>
&lt;p>デメリット&lt;/p>
&lt;ul>
&lt;li>リソース管理のための s3 バケットを必要とする&lt;/li>
&lt;li>lambda に関連するリソースしかデプロイできない&lt;/li>
&lt;li>開発者が明示的に設定しなくても裏でリソースを勝手に生成するので意図せず適切に管理されていないインフラを作ってしまう懸念がある&lt;/li>
&lt;li>大半の実務レベルのアプリケーションでは cf テンプレートの dsl を yaml に設定する必要があり、設定が煩雑になったり見通しが悪くなる&lt;/li>
&lt;/ul>
&lt;h4 id="cdk">cdk&lt;/h4>
&lt;p>メリット&lt;/p>
&lt;ul>
&lt;li>任意の aws インフラのリソースを管理できる&lt;/li>
&lt;li>プログラミング言語で記述できるので動的なリソースの依存関係を定義できる&lt;/li>
&lt;li>cf は裏に隠蔽されているが、serverless framework のような dsl を記述する必要はない&lt;/li>
&lt;/ul>
&lt;p>デメリット&lt;/p>
&lt;ul>
&lt;li>学習コストが高い&lt;/li>
&lt;/ul>
&lt;p>リファレンス&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.secjuice.com/aws-cdk-vs-serverless-framework/">AWS CDK vs Serverless Framework&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://dev.to/tastefulelk/serverless-framework-vs-sam-vs-aws-cdk-1g9g">Serverless Framework vs SAM vs AWS CDK&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>結論から言うと、将来的に cdk を使うならまず cdk を使った方がよい。本当にシンプルな要件で lambda 関数のインフラしか扱わないなら serverless framework でもいいかもしれない。serverless framework は cdk がない時代に作られたツールだろうからいまから新規に導入する場合は、多少の学習コストを払っても cdk を学んでおけば、将来的に役に立つ場面が多いと思う。&lt;/p></content></item><item><title>ドキュメントをちゃんと書く</title><link>/diary/posts/2022/0519/</link><pubDate>Thu, 19 May 2022 13:33:49 +0900</pubDate><guid>/diary/posts/2022/0519/</guid><description>20時に寝て22時に起きて、それから作業して3時に寝て6時に起きた。
インフラのドキュメント作成 今日からインフラのドキュメント作成に着手した。4月から1ヶ月以上に渡って インフラエンジニア のようなお仕事をしていた。具体的には新しい環境のインフラ構築、ならびに既存インフラのリファクタリングというよりは再構築といった作業をしていた。約1ヶ月で大きなインフラのタスクは完了して、その後もこれまで cdk 管理していなかったインフラリソースの管理なども含め、より再現可能な管理されたインフラとなるように改善してきた。それもだいたい終えてきたので、そろそろ他の開発者にも引き継げるようにドキュメントを書くことにした。私以外は若い開発者が多いせいか、cdk/cf の知識というよりもインフラそのもののやネットワークの知識が少ないメンバーが多い。そういった運用経験の浅い開発者にも適切な教育が行えるよう、ドキュメントやチュートリアルなどを書いていく。数日ぐらいかけてしっかり書いてから勉強会を開催する。それをもって引き継ぎしていくかなぁ。
私が前任者から引き継いだ README に helm の説明が次のように書かれてた。
まず helm がわかってない人はググってくること。
こんな README を私はみたことなくて書いている人が訳分からず作業しているんだなという印象を受けた。私が書くドキュメントには cdk とは何か？から説明している。もちろん aws のドキュメントをすべて読めばよいのだが、それはコストがかかる。私の経験と私が理解した cdk の概念を簡潔に、なるべく自分たちの業務にとって大事なことを要約して書くことに意義があると私は考えている。README にググれみたいなことを書いて誰もなにも言わない開発文化を改善していきたい。</description><content>&lt;p>20時に寝て22時に起きて、それから作業して3時に寝て6時に起きた。&lt;/p>
&lt;h2 id="インフラのドキュメント作成">インフラのドキュメント作成&lt;/h2>
&lt;p>今日からインフラのドキュメント作成に着手した。4月から1ヶ月以上に渡って &lt;a href="/diary/diary/posts/2022/0405/">インフラエンジニア&lt;/a> のようなお仕事をしていた。具体的には新しい環境のインフラ構築、ならびに既存インフラのリファクタリングというよりは再構築といった作業をしていた。約1ヶ月で大きなインフラのタスクは完了して、その後もこれまで cdk 管理していなかったインフラリソースの管理なども含め、より再現可能な管理されたインフラとなるように改善してきた。それもだいたい終えてきたので、そろそろ他の開発者にも引き継げるようにドキュメントを書くことにした。私以外は若い開発者が多いせいか、cdk/cf の知識というよりもインフラそのもののやネットワークの知識が少ないメンバーが多い。そういった運用経験の浅い開発者にも適切な教育が行えるよう、ドキュメントやチュートリアルなどを書いていく。数日ぐらいかけてしっかり書いてから勉強会を開催する。それをもって引き継ぎしていくかなぁ。&lt;/p>
&lt;p>私が前任者から引き継いだ README に helm の説明が次のように書かれてた。&lt;/p>
&lt;blockquote>
&lt;p>まず helm がわかってない人はググってくること。&lt;/p>
&lt;/blockquote>
&lt;p>こんな README を私はみたことなくて書いている人が訳分からず作業しているんだなという印象を受けた。私が書くドキュメントには cdk とは何か？から説明している。もちろん aws のドキュメントをすべて読めばよいのだが、それはコストがかかる。私の経験と私が理解した cdk の概念を簡潔に、なるべく自分たちの業務にとって大事なことを要約して書くことに意義があると私は考えている。README にググれみたいなことを書いて誰もなにも言わない開発文化を改善していきたい。&lt;/p></content></item><item><title>cdk のビルドが難しい話し</title><link>/diary/posts/2022/0518/</link><pubDate>Wed, 18 May 2022 09:55:29 +0900</pubDate><guid>/diary/posts/2022/0518/</guid><description>0時に寝て6時半に起きた。暑くなってきたせいかバテ気味。水曜日は本番リリースの日。3つほど本場環境のインフラ移行作業があったので社員さんの実作業をリモートから指示しながらサポートしていた。私に本番環境の操作権限があれば、この工数はほぼ半分に削減できる。
cdk のパッチ検証 先日 cdk による eks クラスターの helm 管理の調査中断 について書いた。バグっているから適切な設定が反映されないという話しで一時中断していたんだけど、そのときにクラスメソッドの担当者さんとも相談していた。そしたら、その担当者さんが問題を修正して pr を送ってくれた。感謝。
fix(eks): Cluster.FromClusterAttributes ignores KubectlLambdaRole #20373 そこまでやってくれると思ってなかったのですごいなぁと感心しながら、せっかくなのでパッチを当てた cdk で検証してみようと、Contributing to the AWS Cloud Development Kit をみながらローカルでのビルドを試みた。ビルド自体はできたんだけど、パッケージを作るのがうまくいかなくて、cdk はツール自体が大きいので実行時間がかかる。だいたいビルドやパッケージングのそれぞれに20-30分ぐらいかかる。エラーの原因がよく分からなくて面倒になって断念した。私が javascript のパッケージングに疎いせいもあると思うけど、ドキュメントに書いてある通りにうまくいかなかったので早々に諦めた。</description><content>&lt;p>0時に寝て6時半に起きた。暑くなってきたせいかバテ気味。水曜日は本番リリースの日。3つほど本場環境のインフラ移行作業があったので社員さんの実作業をリモートから指示しながらサポートしていた。私に本番環境の操作権限があれば、この工数はほぼ半分に削減できる。&lt;/p>
&lt;h2 id="cdk-のパッチ検証">cdk のパッチ検証&lt;/h2>
&lt;p>先日 &lt;a href="/diary/diary/posts/2022/0516/#eks-クラスターの-helm-管理の調査">cdk による eks クラスターの helm 管理の調査中断&lt;/a> について書いた。バグっているから適切な設定が反映されないという話しで一時中断していたんだけど、そのときにクラスメソッドの担当者さんとも相談していた。そしたら、その担当者さんが問題を修正して pr を送ってくれた。感謝。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/pull/20373">fix(eks): Cluster.FromClusterAttributes ignores KubectlLambdaRole #20373&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>そこまでやってくれると思ってなかったのですごいなぁと感心しながら、せっかくなのでパッチを当てた cdk で検証してみようと、&lt;a href="https://github.com/aws/aws-cdk/blob/master/CONTRIBUTING.md">Contributing to the AWS Cloud Development Kit&lt;/a> をみながらローカルでのビルドを試みた。ビルド自体はできたんだけど、パッケージを作るのがうまくいかなくて、cdk はツール自体が大きいので実行時間がかかる。だいたいビルドやパッケージングのそれぞれに20-30分ぐらいかかる。エラーの原因がよく分からなくて面倒になって断念した。私が javascript のパッケージングに疎いせいもあると思うけど、ドキュメントに書いてある通りにうまくいかなかったので早々に諦めた。&lt;/p></content></item><item><title>helm 調査の一時中断</title><link>/diary/posts/2022/0516/</link><pubDate>Mon, 16 May 2022 08:31:44 +0900</pubDate><guid>/diary/posts/2022/0516/</guid><description>0時に寝て6時半に起きた。
eks クラスターの helm 管理の調査 先週から調査 していて、調査結果から kubectlRoleArn を生成してデプロイを実行してみた。以前発生していた権限エラーは解消したものの、lambda 内からの kubectl と k8s クラスターの認証に失敗する。もう1つ kubectlLambdaRole という設定もあるので、ここに system:masters 権限をもつ iam ロールを設定してみたものの、エラー結果は変わらない。お手上げかなと思ってたら aws-eks: Cluster.FromClusterAttributes ignores KubectlLambdaRole #20008 という issue があって、まさにいま起こっている現象と合致するのでこのせいかもしれない。cdk のバグならそれが解消しないと検証を進められないため、この調査は一旦打ち切って、cdk のバグが修正されて新しいバージョンがリリースされてから再試行することに決めた。
backlog のいろいろ たまたまタイムラインでスライドをみかけて、backlog の開発をしている方の記事をみつけた。scala と play framework で実装されているらしい。もう10年以上開発しているプロダクトなのかな。それ自体はすごいなぁと感心しながらスライドを眺めてた。
ヌーラボ入社後に書いたブログ記事＆プレゼン資料まとめ</description><content>&lt;p>0時に寝て6時半に起きた。&lt;/p>
&lt;h2 id="eks-クラスターの-helm-管理の調査">eks クラスターの helm 管理の調査&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2022/0513/#eks-クラスターの-helm-管理の調査">先週から調査&lt;/a> していて、調査結果から &lt;code>kubectlRoleArn&lt;/code> を生成してデプロイを実行してみた。以前発生していた権限エラーは解消したものの、lambda 内からの kubectl と k8s クラスターの認証に失敗する。もう1つ &lt;code>kubectlLambdaRole&lt;/code> という設定もあるので、ここに &lt;code>system:masters&lt;/code> 権限をもつ iam ロールを設定してみたものの、エラー結果は変わらない。お手上げかなと思ってたら &lt;a href="https://github.com/aws/aws-cdk/issues/20008">aws-eks: Cluster.FromClusterAttributes ignores KubectlLambdaRole #20008&lt;/a> という issue があって、まさにいま起こっている現象と合致するのでこのせいかもしれない。cdk のバグならそれが解消しないと検証を進められないため、この調査は一旦打ち切って、cdk のバグが修正されて新しいバージョンがリリースされてから再試行することに決めた。&lt;/p>
&lt;h2 id="backlog-のいろいろ">backlog のいろいろ&lt;/h2>
&lt;p>たまたまタイムラインでスライドをみかけて、backlog の開発をしている方の記事をみつけた。scala と play framework で実装されているらしい。もう10年以上開発しているプロダクトなのかな。それ自体はすごいなぁと感心しながらスライドを眺めてた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://muziyoshiz.hatenablog.com/entry/2021/08/28/154859">ヌーラボ入社後に書いたブログ記事＆プレゼン資料まとめ&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>マーケティング施策の取り組み開始</title><link>/diary/posts/2022/0513/</link><pubDate>Fri, 13 May 2022 08:42:18 +0900</pubDate><guid>/diary/posts/2022/0513/</guid><description>23時に寝て1時に起きて漫画を読んで6時に起きて漫画読んでた。
隔週の雑談 顧問のはらさんと隔週の打ち合わせ。今日の議題は 先日作成した第4期の展望 について雑談した。あまり深く考えずに起業して初期の頃に作った10ヶ年計画に対してわりとその通りに推移している。来期ぐらいで業務委託のお仕事は終える予定。来期か再来期ぐらいからプロダクト開発の期間に入る。自社プロダクトを作る前から徐々にマーケティングもしていかないといけない。そのため、今期からマーケティング施策を少しずつ増やしていて、まずは会社の信頼度を上げるところからやっていく。売上を上げるためのマーケティングではなく信頼度を上げるためのマーケティングを行う。金森氏が言うようにどんなに優れたプロダクトを作ったとしても売れるかどうかは別問題だ。
eks クラスターの helm 管理の調査 昨日の続き。権限設定がなんもわからんみたいな様相になったので調査のやり方を変えることにした。検証用の eks クラスターを cdk から新規に作成して helm をインストールするときのリソースや権限設定がどうなるのかを調査した。lambda 関数が5個ぐらい、ロールは10個ぐらい作成された。lambda の生成に時間がかかるのか？新規作成するのに25分、削除するときは30分ぐらいかかった。rds もそうだけど、eks のような複雑なインフラを cdk で管理すると実行するのにけっこう時間がかかる。設定が難しくなければ別によいけど、eks のような権限やリソースの設定が複雑なインフラはトライ&amp;amp;エラーで何度も実行する必要があるから、こういうものは cdk で管理するのには向かないインフラだと言えると思う。一定の設定のプラクティスが溜まるまでは eks は cdk で管理しない方がよいかもしれない。
CreationRole というのが設定されて trust relationships に次のような設定が追加される。
{ &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Principal&amp;#34;: { &amp;#34;AWS&amp;#34;: &amp;#34;arn:aws:iam::${accountId}:root&amp;#34; }, &amp;#34;Action&amp;#34;: &amp;#34;sts:AssumeRole&amp;#34; } ] } このロールに含まれるカスタムポリシーには次のような設定がある。たぶんこんな感じのロールを新規に作成して kubectlRoleArn として指定してやればいいんじゃないかと思う。
{ &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Action&amp;#34;: &amp;#34;iam:PassRole&amp;#34;, &amp;#34;Resource&amp;#34;: &amp;#34;arn:aws:iam::${accountId}:role/${EksClusterIamRole}&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: [ &amp;#34;eks:CreateCluster&amp;#34;, &amp;#34;eks:DescribeCluster&amp;#34;, &amp;#34;eks:DescribeUpdate&amp;#34;, &amp;#34;eks:DeleteCluster&amp;#34;, &amp;#34;eks:UpdateClusterVersion&amp;#34;, &amp;#34;eks:UpdateClusterConfig&amp;#34;, &amp;#34;eks:CreateFargateProfile&amp;#34;, &amp;#34;eks:TagResource&amp;#34;, &amp;#34;eks:UntagResource&amp;#34; ], &amp;#34;Resource&amp;#34;: [ &amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:cluster/tmp-test-eks-cluster-by-morimoto&amp;#34;, &amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:cluster/tmp-test-eks-cluster-by-morimoto/*&amp;#34; ], &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: [ &amp;#34;eks:DescribeFargateProfile&amp;#34;, &amp;#34;eks:DeleteFargateProfile&amp;#34; ], &amp;#34;Resource&amp;#34;: &amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:fargateprofile/tmp-test-eks-cluster-by-morimoto/*&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: [ &amp;#34;iam:GetRole&amp;#34;, &amp;#34;iam:listAttachedRolePolicies&amp;#34; ], &amp;#34;Resource&amp;#34;: &amp;#34;*&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: &amp;#34;iam:CreateServiceLinkedRole&amp;#34;, &amp;#34;Resource&amp;#34;: &amp;#34;*&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: [ &amp;#34;ec2:DescribeInstances&amp;#34;, &amp;#34;ec2:DescribeNetworkInterfaces&amp;#34;, &amp;#34;ec2:DescribeSecurityGroups&amp;#34;, &amp;#34;ec2:DescribeSubnets&amp;#34;, &amp;#34;ec2:DescribeRouteTables&amp;#34;, &amp;#34;ec2:DescribeDhcpOptions&amp;#34;, &amp;#34;ec2:DescribeVpcs&amp;#34; ], &amp;#34;Resource&amp;#34;: &amp;#34;*&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; } ] }</description><content>&lt;p>23時に寝て1時に起きて漫画を読んで6時に起きて漫画読んでた。&lt;/p>
&lt;h2 id="隔週の雑談">隔週の雑談&lt;/h2>
&lt;p>顧問のはらさんと隔週の打ち合わせ。今日の議題は &lt;a href="/diary/diary/posts/2022/0503/#打ち合わせ資料の作成">先日作成した第4期の展望&lt;/a> について雑談した。あまり深く考えずに起業して初期の頃に作った10ヶ年計画に対してわりとその通りに推移している。来期ぐらいで業務委託のお仕事は終える予定。来期か再来期ぐらいからプロダクト開発の期間に入る。自社プロダクトを作る前から徐々にマーケティングもしていかないといけない。そのため、今期からマーケティング施策を少しずつ増やしていて、まずは会社の信頼度を上げるところからやっていく。売上を上げるためのマーケティングではなく信頼度を上げるためのマーケティングを行う。金森氏が言うようにどんなに優れたプロダクトを作ったとしても売れるかどうかは別問題だ。&lt;/p>
&lt;iframe width="500" height="250" scrolling="no" src="https://alu.jp/series/%E6%98%A0%E5%83%8F%E7%A0%94%E3%81%AB%E3%81%AF%E6%89%8B%E3%82%92%E5%87%BA%E3%81%99%E3%81%AA%EF%BC%81/crop/embed/X8MNLWdDgPeZ5RIF25sD/0?referer=oembed" style="margin: auto;">&lt;/iframe>
&lt;h2 id="eks-クラスターの-helm-管理の調査">eks クラスターの helm 管理の調査&lt;/h2>
&lt;p>昨日の続き。権限設定がなんもわからんみたいな様相になったので調査のやり方を変えることにした。検証用の eks クラスターを cdk から新規に作成して helm をインストールするときのリソースや権限設定がどうなるのかを調査した。lambda 関数が5個ぐらい、ロールは10個ぐらい作成された。lambda の生成に時間がかかるのか？新規作成するのに25分、削除するときは30分ぐらいかかった。rds もそうだけど、eks のような複雑なインフラを cdk で管理すると実行するのにけっこう時間がかかる。設定が難しくなければ別によいけど、eks のような権限やリソースの設定が複雑なインフラはトライ&amp;amp;エラーで何度も実行する必要があるから、こういうものは cdk で管理するのには向かないインフラだと言えると思う。一定の設定のプラクティスが溜まるまでは eks は cdk で管理しない方がよいかもしれない。&lt;/p>
&lt;p>CreationRole というのが設定されて trust relationships に次のような設定が追加される。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Version&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2012-10-17&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Statement&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Principal&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;AWS&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;arn:aws:iam::${accountId}:root&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;sts:AssumeRole&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>このロールに含まれるカスタムポリシーには次のような設定がある。たぶんこんな感じのロールを新規に作成して &lt;code>kubectlRoleArn&lt;/code> として指定してやればいいんじゃないかと思う。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Version&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2012-10-17&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Statement&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;iam:PassRole&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;arn:aws:iam::${accountId}:role/${EksClusterIamRole}&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:CreateCluster&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:DescribeCluster&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:DescribeUpdate&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:DeleteCluster&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:UpdateClusterVersion&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:UpdateClusterConfig&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:CreateFargateProfile&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:TagResource&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:UntagResource&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:cluster/tmp-test-eks-cluster-by-morimoto&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:cluster/tmp-test-eks-cluster-by-morimoto/*&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:DescribeFargateProfile&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;eks:DeleteFargateProfile&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:fargateprofile/tmp-test-eks-cluster-by-morimoto/*&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;iam:GetRole&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;iam:listAttachedRolePolicies&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;*&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;iam:CreateServiceLinkedRole&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;*&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeInstances&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeNetworkInterfaces&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeSecurityGroups&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeSubnets&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeRouteTables&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeDhcpOptions&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ec2:DescribeVpcs&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;*&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>cdk と eks と lambda と iam がわからん</title><link>/diary/posts/2022/0512/</link><pubDate>Thu, 12 May 2022 11:42:54 +0900</pubDate><guid>/diary/posts/2022/0512/</guid><description>0時に寝て3時に起きて漫画読んで5時に寝て8時に起きた。
eks クラスターの helm 管理 昨日の続き。helm のよさはわかったので dapr を helm で管理しようとしている。その際になるべく cdk で管理できた方がよい。eks は cdk の外部で管理しているのだけど、既存の eks クラスターをインポートする機能も提供されていることに気付いた。
Using existing clusters それなら既存の eks クラスターをインポートして cdk で helm だけ管理しようと思って始めたものの、これがとても難しくて丸1日作業してわからなかった。設定項目は少ないけど、権限の問題で動かない。1回あたりの実行に15分ぐらいかかるのでトライ&amp;amp;エラーするのもなかなか大変。
kubectlRoleArn - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually.</description><content>&lt;p>0時に寝て3時に起きて漫画読んで5時に寝て8時に起きた。&lt;/p>
&lt;h2 id="eks-クラスターの-helm-管理">eks クラスターの helm 管理&lt;/h2>
&lt;p>昨日の続き。helm のよさはわかったので dapr を helm で管理しようとしている。その際になるべく cdk で管理できた方がよい。eks は cdk の外部で管理しているのだけど、既存の eks クラスターをインポートする機能も提供されていることに気付いた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters">Using existing clusters&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>それなら既存の eks クラスターをインポートして cdk で helm だけ管理しようと思って始めたものの、これがとても難しくて丸1日作業してわからなかった。設定項目は少ないけど、権限の問題で動かない。1回あたりの実行に15分ぐらいかかるのでトライ&amp;amp;エラーするのもなかなか大変。&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>kubectlRoleArn&lt;/code> - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually. The trust policy of this role should include the the arn:aws::iam::${accountId}:root principal in order to allow the execution role of the kubectl resource to assume it.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;code>kubectlRoleArn&lt;/code> の設定をどうするかだけなんだが、この説明でどう設定していいか理解できなかった。cdk で新規に eks クラスターを作成するなら自動で作ってくれるけど、既存の eks クラスターの場合は自分で設定しないといけない。ややこしいことに cdk は kubectl の実行を lambda 経由で実行するので eks と lambda と iam のロールやポリシーを適切に設定する必要がある。lambda にどういう権限を設定するのが適切なのかは本当に難しい。サーバーレスはよいアイディアだとは思うけど、lambda は難し過ぎて私はなるべく使いたくないサービスではある。結局わからなくて翌日に持ち越し。&lt;/p></content></item><item><title>障害調査と先入観</title><link>/diary/posts/2022/0427/</link><pubDate>Wed, 27 Apr 2022 07:37:11 +0900</pubDate><guid>/diary/posts/2022/0427/</guid><description>23時に寝て5時過ぎに起きた。
インフラの不具合調査 本番環境に初回デプロイして ecs から secrets manager のアクセスに失敗しているエラーが出ていたので調査した。エラーメッセージでググるとクラスメソッドさんの記事が出てきた。
ECSでコンテナ環境変数をSecretManagerから取得する際にResourceInitializationErrorが発生したときの対処方法 この記事によると、次のどちらかの原因かなと推測していた。
実行 IAM ロールの権限不足 SecretsManager エンドポイントへの不到達 調べども調べどもテスト環境との違いがわからなくてはまってしまった。ある検証をしていたときに、テスト環境を手動で構築した担当者から datadog の api key を設定してないんじゃない？と言われて、まさにそれが原因だった。cdk のコード上は設定済みのものとして ecs の datadog のサイドカーに設定していた。先入観で rds の credential 情報を取得できずにエラーになっていると思い込んでいたが、サイドカーの datadog の api key が原因ではまるべくしてはまったという感じ。
一方で secrets manager に登録するサードパーティの api key をどこで管理するかというのは難しい問題でもある。cdk のコードの中に書いてしまうというのも1つのやり方ではあるが、昨今の github 関連のサードパーティから派生したセキュリティインシデントで private リポジトリのソースコードにアクセスされることも発生しているのでソースコードには書きたくない。で、手動で secrets manager に設定しないといけないから、今回みたいな初回デプロイ時に誰も気付かないみたいなことが起きる。</description><content>&lt;p>23時に寝て5時過ぎに起きた。&lt;/p>
&lt;h2 id="インフラの不具合調査">インフラの不具合調査&lt;/h2>
&lt;p>本番環境に初回デプロイして ecs から secrets manager のアクセスに失敗しているエラーが出ていたので調査した。エラーメッセージでググるとクラスメソッドさんの記事が出てきた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://dev.classmethod.jp/articles/tsnote-ecs-resourceinitializationerror/">ECSでコンテナ環境変数をSecretManagerから取得する際にResourceInitializationErrorが発生したときの対処方法&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>この記事によると、次のどちらかの原因かなと推測していた。&lt;/p>
&lt;ul>
&lt;li>実行 IAM ロールの権限不足&lt;/li>
&lt;li>SecretsManager エンドポイントへの不到達&lt;/li>
&lt;/ul>
&lt;p>調べども調べどもテスト環境との違いがわからなくてはまってしまった。ある検証をしていたときに、テスト環境を手動で構築した担当者から datadog の api key を設定してないんじゃない？と言われて、まさにそれが原因だった。cdk のコード上は設定済みのものとして ecs の datadog のサイドカーに設定していた。先入観で rds の credential 情報を取得できずにエラーになっていると思い込んでいたが、サイドカーの datadog の api key が原因ではまるべくしてはまったという感じ。&lt;/p>
&lt;p>一方で secrets manager に登録するサードパーティの api key をどこで管理するかというのは難しい問題でもある。cdk のコードの中に書いてしまうというのも1つのやり方ではあるが、昨今の github 関連のサードパーティから派生したセキュリティインシデントで private リポジトリのソースコードにアクセスされることも発生しているのでソースコードには書きたくない。で、手動で secrets manager に設定しないといけないから、今回みたいな初回デプロイ時に誰も気付かないみたいなことが起きる。&lt;/p></content></item><item><title>本番環境反映の監督</title><link>/diary/posts/2022/0425/</link><pubDate>Mon, 25 Apr 2022 19:39:39 +0900</pubDate><guid>/diary/posts/2022/0425/</guid><description>昨日は思いっきり昼寝してしまったせいか、夜に眠れなくて3時ぐらいまで起きてて、寝たのか寝てのかよくわからない雰囲気で7時前に起きた。
インフラ作業の本番反映 先週対応した api gateway のコード化 を本番環境に反映した。手動で作成されていた api gateway, vpc link, セキュリティグループとそれに関連するリソース群を cdk 管理のコードで置き換える。既存のリソースを削除してから cdk でデプロイするため、失敗しても切り戻しできないのでプレッシャーがかかる。とはいえ、想定通りに作業が進捗して2時間ほどで完了した。それと並行してテスト環境では、手動で作成された rds を cdk 管理のコードで置き換える移行作業をしていた。これも一筋縄ではいかなくて右往左往しながら作業してた。これを本番環境でやるのもまた億劫だなぁと思いながら発生したエラーや事象を書き綴っていた。あと本番環境で行う大きな移行作業はこれだけ。</description><content>&lt;p>昨日は思いっきり昼寝してしまったせいか、夜に眠れなくて3時ぐらいまで起きてて、寝たのか寝てのかよくわからない雰囲気で7時前に起きた。&lt;/p>
&lt;h2 id="インフラ作業の本番反映">インフラ作業の本番反映&lt;/h2>
&lt;p>先週対応した &lt;a href="/diary/diary/posts/2022/0419/">api gateway のコード化&lt;/a> を本番環境に反映した。手動で作成されていた api gateway, vpc link, セキュリティグループとそれに関連するリソース群を cdk 管理のコードで置き換える。既存のリソースを削除してから cdk でデプロイするため、失敗しても切り戻しできないのでプレッシャーがかかる。とはいえ、想定通りに作業が進捗して2時間ほどで完了した。それと並行してテスト環境では、手動で作成された rds を cdk 管理のコードで置き換える移行作業をしていた。これも一筋縄ではいかなくて右往左往しながら作業してた。これを本番環境でやるのもまた億劫だなぁと思いながら発生したエラーや事象を書き綴っていた。あと本番環境で行う大きな移行作業はこれだけ。&lt;/p></content></item><item><title>rds の再作成</title><link>/diary/posts/2022/0421/</link><pubDate>Thu, 21 Apr 2022 07:37:32 +0900</pubDate><guid>/diary/posts/2022/0421/</guid><description>23時に寝て5時半に起きた。久しぶりによく眠れた気がする。
rds を独立したスタックに分離 昨日の続き。ライフサイクルにあわせたスタックに分割し、スタック間の依存関係を適切に定義することで堅牢なインフラコードになる。最後に残った DatabaseStack を分離するところを朝からやっていた。
DatabaseStack BackendStack GatewayStack FrontendStack rds を壊して、バージョンを aurora postgresql 最新の 13.x にアップグレードして、cf のテンプレートで小細工をしながら既存の環境を移行したりしていた。pg_dump を使ってテキストに dump したデータを、psql を使ってリストアしたりもした。データ量が少なかったらこれで移行してもいいかもしれない。
$ pg_dump --host mydb.xxx.ap-northeast-1.rds.amazonaws.com --username user mydb &amp;gt; mydb.dump $ psql --host mydb.xxx.ap-northeast-1.rds.amazonaws.com --username user --dbname mydb --file ./mydb.dump</description><content>&lt;p>23時に寝て5時半に起きた。久しぶりによく眠れた気がする。&lt;/p>
&lt;h2 id="rds-を独立したスタックに分離">rds を独立したスタックに分離&lt;/h2>
&lt;p>昨日の続き。ライフサイクルにあわせたスタックに分割し、スタック間の依存関係を適切に定義することで堅牢なインフラコードになる。最後に残った DatabaseStack を分離するところを朝からやっていた。&lt;/p>
&lt;ul>
&lt;li>DatabaseStack
&lt;ul>
&lt;li>BackendStack
&lt;ul>
&lt;li>GatewayStack
&lt;ul>
&lt;li>FrontendStack&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>rds を壊して、バージョンを aurora postgresql 最新の 13.x にアップグレードして、cf のテンプレートで小細工をしながら既存の環境を移行したりしていた。&lt;a href="https://www.postgresql.org/docs/13/app-pgdump.html">pg_dump&lt;/a> を使ってテキストに dump したデータを、psql を使ってリストアしたりもした。データ量が少なかったらこれで移行してもいいかもしれない。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ pg_dump --host mydb.xxx.ap-northeast-1.rds.amazonaws.com --username user mydb &amp;gt; mydb.dump
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ psql --host mydb.xxx.ap-northeast-1.rds.amazonaws.com --username user --dbname mydb --file ./mydb.dump
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>堅牢なインフラコード</title><link>/diary/posts/2022/0420/</link><pubDate>Wed, 20 Apr 2022 07:38:23 +0900</pubDate><guid>/diary/posts/2022/0420/</guid><description>23時に寝て5時に起きた。
駐輪場の定期更新 3ヶ月ごとの更新。ちょうどいまのお仕事の契約と同じ更新月になっている。前回と同じ金額だったのでまだ駐輪場の料金は値上げされていない。世界的にインフレしているのに日本が全然インフレしていない理由の1つとして不動産が値上げしていないからというのを日銀の記事で見かけた気がする。どこかのタイミングで不動産関連の値上げも始まるのかもしれない。
インフラコードの抜本的リファクタリング 約2週間かけて、新規インフラ環境の構築、既存インフラの cdk/cf と同期されていなかったインフラ (rds, security group, croudfront, api gateway, waf) の同期など、インフラコードの大きな変更をやり終えた。一部 fromLookup でインポートしているリソースもあるが、いま完全に cdk/cf 管理なインフラとなった。ここからはせっかく cdk でコードを書いているので、モジュール化や共通化など、再利用可能なリソースとして定義して、複数の Stack でコードを再利用するといったリファクタリングをしていく。ひとまずこのことをボーナスステージと呼ぼう。いま完全に同期されたインフラがあるため、インフラ上のリソースの差分が出なければリファクタリングは正しいことが保証される。cdk は Stack 間の依存関係 も定義できるため、適切な依存関係を定義することでより堅牢なインフラコードとなるはずである。具体的には次のような依存関係になる。然るべき堅牢なインフラコードに書き換えていく。
DatabaseStack BackendStack GatewayStack FrontendStack</description><content>&lt;p>23時に寝て5時に起きた。&lt;/p>
&lt;h2 id="駐輪場の定期更新">駐輪場の定期更新&lt;/h2>
&lt;p>3ヶ月ごとの更新。ちょうどいまのお仕事の契約と同じ更新月になっている。前回と同じ金額だったのでまだ駐輪場の料金は値上げされていない。世界的にインフレしているのに日本が全然インフレしていない理由の1つとして不動産が値上げしていないからというのを日銀の記事で見かけた気がする。どこかのタイミングで不動産関連の値上げも始まるのかもしれない。&lt;/p>
&lt;h2 id="インフラコードの抜本的リファクタリング">インフラコードの抜本的リファクタリング&lt;/h2>
&lt;p>約2週間かけて、新規インフラ環境の構築、既存インフラの cdk/cf と同期されていなかったインフラ (rds, security group, croudfront, api gateway, waf) の同期など、インフラコードの大きな変更をやり終えた。一部 &lt;code>fromLookup&lt;/code> でインポートしているリソースもあるが、いま完全に cdk/cf 管理なインフラとなった。ここからはせっかく cdk でコードを書いているので、モジュール化や共通化など、再利用可能なリソースとして定義して、複数の Stack でコードを再利用するといったリファクタリングをしていく。ひとまずこのことをボーナスステージと呼ぼう。いま完全に同期されたインフラがあるため、インフラ上のリソースの差分が出なければリファクタリングは正しいことが保証される。cdk は &lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib-readme.html#dependencies">Stack 間の依存関係&lt;/a> も定義できるため、適切な依存関係を定義することでより堅牢なインフラコードとなるはずである。具体的には次のような依存関係になる。然るべき堅牢なインフラコードに書き換えていく。&lt;/p>
&lt;ul>
&lt;li>DatabaseStack
&lt;ul>
&lt;li>BackendStack
&lt;ul>
&lt;li>GatewayStack
&lt;ul>
&lt;li>FrontendStack&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></content></item><item><title>api gateway のデプロイ検証</title><link>/diary/posts/2022/0419/</link><pubDate>Tue, 19 Apr 2022 07:38:47 +0900</pubDate><guid>/diary/posts/2022/0419/</guid><description>0時に寝て6時に起きた。
api gateway のデプロイ検証 昨日の続き。api gateway を再作成すると、当然、既存のテスト環境は疎通できなくなってしまう。他の開発者を妨害しないようにまた21時から深夜にかけて作業しようと考えていたら、いまは閑散期でテスト環境を使わないといけないような開発の状況にはないと他の開発者から教えてもらった。PO の人たちも検証は一段落しているとのこと。午後からテスト環境壊してもよいという確認が取れたので13時からインフラ作業をしていた。早ければ1時間ぐらい、遅ければ4時間と見積もって、実際にデプロイしてみると、あれやこれやの抜け・漏れ、手動で設定されていたリソースの弊害などもあって、3時間ぐらいかかった。もう2週間ほどインフラの作業ばかりやっているせいか、何が起こっても4時間ぐらいあれば調査とリストアを完了できるぐらいの自信がついてきた。最終的に cdk で再作成した api gateway を使って cloudfront の distribution 経由で web api 呼び出しが繋がった。当たり前の話なんだけど、繋がる瞬間、ローカルで web api のエンドポイントを叩いてレスポンスが返ってくるときが嬉しい。最後の大物だったインフラもたった2日でやっつけることができた。これからはボーナスステージ。</description><content>&lt;p>0時に寝て6時に起きた。&lt;/p>
&lt;h2 id="api-gateway-のデプロイ検証">api gateway のデプロイ検証&lt;/h2>
&lt;p>昨日の続き。api gateway を再作成すると、当然、既存のテスト環境は疎通できなくなってしまう。他の開発者を妨害しないようにまた21時から深夜にかけて作業しようと考えていたら、いまは閑散期でテスト環境を使わないといけないような開発の状況にはないと他の開発者から教えてもらった。PO の人たちも検証は一段落しているとのこと。午後からテスト環境壊してもよいという確認が取れたので13時からインフラ作業をしていた。早ければ1時間ぐらい、遅ければ4時間と見積もって、実際にデプロイしてみると、あれやこれやの抜け・漏れ、手動で設定されていたリソースの弊害などもあって、3時間ぐらいかかった。もう2週間ほどインフラの作業ばかりやっているせいか、何が起こっても4時間ぐらいあれば調査とリストアを完了できるぐらいの自信がついてきた。最終的に cdk で再作成した api gateway を使って cloudfront の distribution 経由で web api 呼び出しが繋がった。当たり前の話なんだけど、繋がる瞬間、ローカルで web api のエンドポイントを叩いてレスポンスが返ってくるときが嬉しい。最後の大物だったインフラもたった2日でやっつけることができた。これからはボーナスステージ。&lt;/p></content></item><item><title>api gateway のコード化</title><link>/diary/posts/2022/0418/</link><pubDate>Mon, 18 Apr 2022 07:39:00 +0900</pubDate><guid>/diary/posts/2022/0418/</guid><description>1時に寝て7時に起きた。
api gateway のコード化 いま cdk で管理していない大きなインフラとして api gateway がある。さらに restful api ではなく http api という新しい仕組みを使っているため、cdk 側も experimental な機能として提供されている。experimental と言っても本番環境で使われいるので十分に production ready と言える。弊害としては cdk のサンプルコードが少なく、公式の api reference に付いているサンプルコードぐらいしか参考になるものがなく、適当にコードを書いてでデプロイし、管理画面で意図した設定になっているかを確認するといったトライ&amp;amp;エラーみたいなやり方しかない。今日の時点では pr まで作ってデプロイと検証は翌日にまわることにした。
さらに cdk の v2 系では -alpha というパッケージが experimental 向けの機能を提供している。バージョン番号ではなく、パッケージ名に alpha が付くという分かりにくさに拍車をかけている。お手伝い先で使っている設定は次の3つのパッケージを使うことで実現できた。api gateway, vpc link, integration, route, authorization, stage など、複数のリソースを連携して設定しないといけないのでちょっとややこしい。
https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-alpha https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-authorizers-alpha https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-integrations-alpha</description><content>&lt;p>1時に寝て7時に起きた。&lt;/p>
&lt;h2 id="api-gateway-のコード化">api gateway のコード化&lt;/h2>
&lt;p>いま cdk で管理していない大きなインフラとして &lt;a href="https://aws.amazon.com/jp/api-gateway/">api gateway&lt;/a> がある。さらに restful api ではなく http api という新しい仕組みを使っているため、cdk 側も experimental な機能として提供されている。experimental と言っても本番環境で使われいるので十分に production ready と言える。弊害としては cdk のサンプルコードが少なく、公式の api reference に付いているサンプルコードぐらいしか参考になるものがなく、適当にコードを書いてでデプロイし、管理画面で意図した設定になっているかを確認するといったトライ&amp;amp;エラーみたいなやり方しかない。今日の時点では pr まで作ってデプロイと検証は翌日にまわることにした。&lt;/p>
&lt;p>さらに cdk の v2 系では &lt;code>-alpha&lt;/code> というパッケージが experimental 向けの機能を提供している。バージョン番号ではなく、パッケージ名に alpha が付くという分かりにくさに拍車をかけている。お手伝い先で使っている設定は次の3つのパッケージを使うことで実現できた。api gateway, vpc link, integration, route, authorization, stage など、複数のリソースを連携して設定しないといけないのでちょっとややこしい。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-alpha">https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-alpha&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-authorizers-alpha">https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-authorizers-alpha&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-integrations-alpha">https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-integrations-alpha&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>コロナワクチン3回目</title><link>/diary/posts/2022/0414/</link><pubDate>Thu, 14 Apr 2022 07:11:44 +0900</pubDate><guid>/diary/posts/2022/0414/</guid><description>22時に寝て3時に起きた6時に起きた。季節の変わり目のせいか、いつも眠い。
ワクチン3回目接種 2回目は昨年の9月27日 に接種した。6ヶ月以上経たないといけないので3月27日以降に接種資格を得て、実際に接種券が届いたのが4月7日だった。神戸市は初回のときにまごまごしたので他の自治体よりも遅れている。これまでファイザーを2回接種したので今回はモデルナを受けてみることにした。モデルナを扱っていてネット予約できるもっとも近くの診療所を予約した。オフィスから自転車で15分ぐらいのところ。16:30の予約なのに16時過ぎぐらいに行ったら普通に受け付けしてくれてすぐに接種もできた。16:05に接種終えて16:20まで待機してオフィスに戻ってきて普通にお仕事してた。その1時間半後に熱を測ったら37.0℃だった。その後もやや熱っぽいけど、平気とは言えば平気。
インフラ移行作業 昨日の続き。フロントエンドのテスト環境が壊れる可能性があるため、他メンバーが使っていない夜に移行作業を行う。POの人たちがQA検証を19-21時ぐらいにやっていることが多いので21時以降に作業すると連絡しておいた。なかなか大変だった。最悪の場合、数時間テスト環境を使えませんと伝えていたものの、その通りで4時間ぐらい検証作業をしていた。cloudfront の distribution 設定を CloudFrontWebDistribution から Distribution へ移行して、新しいやり方であるマネージドポリシーを使うようにした。この設定が意図した振る舞いにならなくて検証作業に時間を割いた。
cloudfront 経由で web api を呼び出すルートがあってキャッシュを無効にしたいのだが、無効にしたキャッシュポリシー (ttl をゼロにする) を作るとヘッダーの設定ができない。次のようなエラーになる。
The parameter HeaderBehavior is invalid for policy with caching disabled. (cloudfront): Cache Policy cannot forward Authorization header. #13441 によると、maxTTL を1秒にして Authorization ヘッダーをオリジンに転送するようには設定できる。キャッシュメソッドは GET と HEAD なので実運用上は問題ないとは思うが、この回避策がないかどうかを調べて検証していた。結果としてはなかった。Configuring CloudFront to forward the Authorization header には Authorization ヘッダーを転送する方法は次の2通りとある。
cache key に含める Managed-AllViewer という origin request policy をすべての viewer requests に含める 最終的には1番目のやり方で対応はしたが、2番目のオリジンリクエストポリシーを設定する方法も検証してみた。オリジンリクエストポリシーを単体で設定することはできなくて、キャッシュポリシーも一緒に設定しないといけないことからキャッシュポリシーの設定の影響を受けて意図したように Authorization ヘッダーの転送はできなかった。</description><content>&lt;p>22時に寝て3時に起きた6時に起きた。季節の変わり目のせいか、いつも眠い。&lt;/p>
&lt;h2 id="ワクチン3回目接種">ワクチン3回目接種&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2021/0927/#コロナワクチン2回目摂取">2回目は昨年の9月27日&lt;/a> に接種した。6ヶ月以上経たないといけないので3月27日以降に接種資格を得て、実際に接種券が届いたのが4月7日だった。神戸市は初回のときにまごまごしたので他の自治体よりも遅れている。これまでファイザーを2回接種したので今回はモデルナを受けてみることにした。モデルナを扱っていてネット予約できるもっとも近くの診療所を予約した。オフィスから自転車で15分ぐらいのところ。16:30の予約なのに16時過ぎぐらいに行ったら普通に受け付けしてくれてすぐに接種もできた。16:05に接種終えて16:20まで待機してオフィスに戻ってきて普通にお仕事してた。その1時間半後に熱を測ったら37.0℃だった。その後もやや熱っぽいけど、平気とは言えば平気。&lt;/p>
&lt;h2 id="インフラ移行作業">インフラ移行作業&lt;/h2>
&lt;p>昨日の続き。フロントエンドのテスト環境が壊れる可能性があるため、他メンバーが使っていない夜に移行作業を行う。POの人たちがQA検証を19-21時ぐらいにやっていることが多いので21時以降に作業すると連絡しておいた。なかなか大変だった。最悪の場合、数時間テスト環境を使えませんと伝えていたものの、その通りで4時間ぐらい検証作業をしていた。cloudfront の distribution 設定を CloudFrontWebDistribution から Distribution へ移行して、新しいやり方であるマネージドポリシーを使うようにした。この設定が意図した振る舞いにならなくて検証作業に時間を割いた。&lt;/p>
&lt;p>cloudfront 経由で web api を呼び出すルートがあってキャッシュを無効にしたいのだが、無効にしたキャッシュポリシー (ttl をゼロにする) を作るとヘッダーの設定ができない。次のようなエラーになる。&lt;/p>
&lt;pre tabindex="0">&lt;code>The parameter HeaderBehavior is invalid for policy with caching disabled.
&lt;/code>&lt;/pre>&lt;p>&lt;a href="https://github.com/aws/aws-cdk/issues/13441">(cloudfront): Cache Policy cannot forward Authorization header. #13441&lt;/a> によると、maxTTL を1秒にして &lt;code>Authorization&lt;/code> ヘッダーをオリジンに転送するようには設定できる。キャッシュメソッドは GET と HEAD なので実運用上は問題ないとは思うが、この回避策がないかどうかを調べて検証していた。結果としてはなかった。&lt;a href="https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/add-origin-custom-headers.html#add-origin-custom-headers-forward-authorization">Configuring CloudFront to forward the Authorization header&lt;/a> には &lt;code>Authorization&lt;/code> ヘッダーを転送する方法は次の2通りとある。&lt;/p>
&lt;ol>
&lt;li>cache key に含める&lt;/li>
&lt;li>Managed-AllViewer という origin request policy をすべての viewer requests に含める&lt;/li>
&lt;/ol>
&lt;p>最終的には1番目のやり方で対応はしたが、2番目のオリジンリクエストポリシーを設定する方法も検証してみた。オリジンリクエストポリシーを単体で設定することはできなくて、キャッシュポリシーも一緒に設定しないといけないことからキャッシュポリシーの設定の影響を受けて意図したように &lt;code>Authorization&lt;/code> ヘッダーの転送はできなかった。&lt;/p>
&lt;ul>
&lt;li>cache policy: Managed-CachingDisabled&lt;/li>
&lt;li>origin request policy: Managed-AllViewer&lt;/li>
&lt;/ul></content></item><item><title>cdk の cloudfront の distribution 設定の移行</title><link>/diary/posts/2022/0413/</link><pubDate>Wed, 13 Apr 2022 15:22:21 +0900</pubDate><guid>/diary/posts/2022/0413/</guid><description>0時に寝て5時過ぎに起きた。
フロントエンドのインフラ作業の続き 昨日、cloudfront の distribution 設定のビヘイビアのキャッシュ設定が誤っていることに気付いたので cdk のコードを修正していく。Amazon CloudFront キャッシュポリシーとオリジンリクエストポリシーを発表 によると、2020年ぐらいに distribution ごとに個別設定していたのをマネージドポリシーというリソースを参照することで一元管理できるようになったらしい。cdk のコードで言えば、次の issue で対応されているが、これらの機能は aws_cloudfront.Distribution のみに追加されている。従来の CloudFrontWebDistribution では使えないので distribution 設定そのものを新しいやり方に移行する必要がある。
https://github.com/aws/aws-cdk/issues/9644 https://github.com/aws/aws-cdk/issues/9647 基本的には同じ設定を行うので新しいクラスのメンバーや構造にあわせて移行するだけなので難しくはないけれど、1つずつ設定内容の移行方法を確認していかないといけないから手間暇はかかる。cdk のドキュメントをみると、型に対してどういった設定をすればよいかが書いてあって、あとはメソッドの定義などもみながら自分たちの設定に近いものを選択していくといった作業になる。難しくはないけど時間はかかる。半日ほどやって移行のための pr を作成した。</description><content>&lt;p>0時に寝て5時過ぎに起きた。&lt;/p>
&lt;h2 id="フロントエンドのインフラ作業の続き">フロントエンドのインフラ作業の続き&lt;/h2>
&lt;p>昨日、cloudfront の distribution 設定のビヘイビアのキャッシュ設定が誤っていることに気付いたので cdk のコードを修正していく。&lt;a href="https://aws.amazon.com/jp/blogs/news/amazon-cloudfront-announces-cache-and-origin-request-policies/">Amazon CloudFront キャッシュポリシーとオリジンリクエストポリシーを発表&lt;/a> によると、2020年ぐらいに distribution ごとに個別設定していたのをマネージドポリシーというリソースを参照することで一元管理できるようになったらしい。cdk のコードで言えば、次の issue で対応されているが、これらの機能は &lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_cloudfront.Distribution.html">aws_cloudfront.Distribution&lt;/a> のみに追加されている。従来の CloudFrontWebDistribution では使えないので distribution 設定そのものを新しいやり方に移行する必要がある。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/9644">https://github.com/aws/aws-cdk/issues/9644&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/9647">https://github.com/aws/aws-cdk/issues/9647&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>基本的には同じ設定を行うので新しいクラスのメンバーや構造にあわせて移行するだけなので難しくはないけれど、1つずつ設定内容の移行方法を確認していかないといけないから手間暇はかかる。cdk のドキュメントをみると、型に対してどういった設定をすればよいかが書いてあって、あとはメソッドの定義などもみながら自分たちの設定に近いものを選択していくといった作業になる。難しくはないけど時間はかかる。半日ほどやって移行のための pr を作成した。&lt;/p></content></item><item><title>cloudfront のキャッシュ設定</title><link>/diary/posts/2022/0412/</link><pubDate>Tue, 12 Apr 2022 07:31:19 +0900</pubDate><guid>/diary/posts/2022/0412/</guid><description>1時に寝て5時に起きた。
フロントエンドのインフラ作業 これまでバックエンドのインフラ作業をしてきたが、そちらの移行作業は完了した。フロントエンドのインフラも積み残しが多々あるのでこの機にリファクタリングする。差分が出ないことを確認して cf のテンプレートのドリフト結果を解消したものをデプロイしたらフロントエンドが壊れた。具体的には xhr の web api 呼び出しに対して認可エラーが返るようになった。数時間ほどの調査の結果、cloudfront の distribution 設定のビヘイビアのキャッシュ設定で web api 呼び出しのときに authorization ヘッダーを転送するための設定が漏れていることがわかった。cdk のコードにはそんな設定がどこにもなく差分も表示されないことから、フロントエンドの cdk コードも全く保守されていないことがわかった。デプロイするときに前回のデプロイが半年前だったのでなんか悪い予感はしたんよね。インフラ担当者の怠慢が度を越し過ぎてもう何が起きても私は驚かないけど。設定に漏れがある前提でフロントエンドのインフラをリファクタリングしていかないといけない。</description><content>&lt;p>1時に寝て5時に起きた。&lt;/p>
&lt;h2 id="フロントエンドのインフラ作業">フロントエンドのインフラ作業&lt;/h2>
&lt;p>これまでバックエンドのインフラ作業をしてきたが、そちらの移行作業は完了した。フロントエンドのインフラも積み残しが多々あるのでこの機にリファクタリングする。差分が出ないことを確認して cf のテンプレートのドリフト結果を解消したものをデプロイしたらフロントエンドが壊れた。具体的には xhr の web api 呼び出しに対して認可エラーが返るようになった。数時間ほどの調査の結果、cloudfront の distribution 設定のビヘイビアのキャッシュ設定で web api 呼び出しのときに authorization ヘッダーを転送するための設定が漏れていることがわかった。cdk のコードにはそんな設定がどこにもなく差分も表示されないことから、フロントエンドの cdk コードも全く保守されていないことがわかった。デプロイするときに前回のデプロイが半年前だったのでなんか悪い予感はしたんよね。インフラ担当者の怠慢が度を越し過ぎてもう何が起きても私は驚かないけど。設定に漏れがある前提でフロントエンドのインフラをリファクタリングしていかないといけない。&lt;/p></content></item><item><title>cdk/cf の Stack とライフサイクル</title><link>/diary/posts/2022/0411/</link><pubDate>Mon, 11 Apr 2022 07:26:57 +0900</pubDate><guid>/diary/posts/2022/0411/</guid><description>0時に寝て5時に起きた。
インフラ変更の本番作業 先週やっていた様々なインフラ構築の改善を本番環境に適用する。cf の changeset は23とかになっていた気がする。大きな括りで次の3つの移行作業をした。
rds をスタックから切り離す cdk の v1 から v2 へのアップグレード ポリシーとセキュリティグループのドリフト解消 私は本番環境へのアクセス権限をもっていないので社員さんの作業内容を伝えてやってもらう。cf のテンプレートの更新やドリフトの解消など、テスト環境で10時間以上は費やした検証結果が功を奏して、本番作業は想定外の事象も発生せず1.5時間で完了した。cdk のコードも意図した設定になるように修正済みだし、なにも問題は発生しない。cdk のコードを書くときは cf のイベントログやドリフト結果と実際のインフラの振る舞いを確認するといった検証には時間がかかるが、それができてしまえば本番環境の構築はうまくできる。それがわかっているインフラ担当者がいなくなると、また1から担当者が検証する必要があって保守は大変かもしれないけど。
同じ Stack でどういったリソースを管理するかというライフサイクルは難しい問題かもしれない。今回 rds を削除したのは、なんらかの理由で手動運用で rds を変更することがあって、アプリケーション Stack から外してしまった方が保守しやすいのではないかという意見が出たため。それが正しいかどうかはわからないが、一度作ったリソースを削除しないもの (vpc や s3 bucket など) をアプリケーション Stack で管理すると、再作成できなくて面倒くさいことがある。というのは、再作成は新規に作成 → 古いリソースを削除の順番で処理されるため、一意な名前をもつリソースは基本的に再作成できない。</description><content>&lt;p>0時に寝て5時に起きた。&lt;/p>
&lt;h2 id="インフラ変更の本番作業">インフラ変更の本番作業&lt;/h2>
&lt;p>先週やっていた様々なインフラ構築の改善を本番環境に適用する。cf の changeset は23とかになっていた気がする。大きな括りで次の3つの移行作業をした。&lt;/p>
&lt;ul>
&lt;li>rds をスタックから切り離す&lt;/li>
&lt;li>cdk の v1 から v2 へのアップグレード&lt;/li>
&lt;li>ポリシーとセキュリティグループのドリフト解消&lt;/li>
&lt;/ul>
&lt;p>私は本番環境へのアクセス権限をもっていないので社員さんの作業内容を伝えてやってもらう。cf のテンプレートの更新やドリフトの解消など、テスト環境で10時間以上は費やした検証結果が功を奏して、本番作業は想定外の事象も発生せず1.5時間で完了した。cdk のコードも意図した設定になるように修正済みだし、なにも問題は発生しない。cdk のコードを書くときは cf のイベントログやドリフト結果と実際のインフラの振る舞いを確認するといった検証には時間がかかるが、それができてしまえば本番環境の構築はうまくできる。それがわかっているインフラ担当者がいなくなると、また1から担当者が検証する必要があって保守は大変かもしれないけど。&lt;/p>
&lt;p>同じ Stack でどういったリソースを管理するかというライフサイクルは難しい問題かもしれない。今回 rds を削除したのは、なんらかの理由で手動運用で rds を変更することがあって、アプリケーション Stack から外してしまった方が保守しやすいのではないかという意見が出たため。それが正しいかどうかはわからないが、一度作ったリソースを削除しないもの (vpc や s3 bucket など) をアプリケーション Stack で管理すると、再作成できなくて面倒くさいことがある。というのは、再作成は新規に作成 → 古いリソースを削除の順番で処理されるため、一意な名前をもつリソースは基本的に再作成できない。&lt;/p></content></item><item><title>壊れた cf スタックのリストアと cdk の再同期</title><link>/diary/posts/2022/0408/</link><pubDate>Fri, 08 Apr 2022 10:13:19 +0900</pubDate><guid>/diary/posts/2022/0408/</guid><description>2時に寝て6時半に起きた。インフラエンジニアになったのでみんなが作業していない時間にインフラの保守作業をするようにしている。昼はアプリケーションエンジニア、夜はインフラエンジニアみたいな生活になっていてしんどい。
壊れた cf スタックの更新 テスト環境の cf スタックを手動で更新して壊れているのを cdk で管理できるように直した。壊れていたのは次の3つ。
rds をスナップショットからリストアしたので cf が管理している rds リソースが存在しない iam の acl 設定が異なる セキュリティグループのインバウンドルールが異なる aws 的にもそういった状況は認識していて cdk で同期できなくなった cf スタックを更新する手順を提供している。
AWS CloudFormation スタックを削除したときにリソースの一部を保持する方法を教えてください。 ざっくり手順をまとめると次になる。
対象のリソースに DeletetionPolicy=Retain にセットする テンプレートからリソースを削除して、スタックの更新を実行する テンプレート内のリソースの実際の状態を describe して、スタック内に既存のリソースをインポートする リソースの設定ぐらいなら既存のリソースからインポートしなくても cf のテンプレートを直接書き換えたものをアップロードしてスタックを更新するのでも大丈夫だったりする。しかし、cdk もそのテンプレートにあうように修正しないといけないため、cdk のコードとテンプレートのコードの両方をチェックしながら検証する必要がある。cdk でリソース管理ができるようになったからといって、それが変更前の既存のリソースの設定と同じかどうかは人間が目でみて検証しないといけない。これがあちこちで参照されているリソースだと追跡するのが面倒くさいといった手間暇がかかる。
cdk がよいものかどうか、私はまだ判断がつかないけど、cf を抽象化して便利になっているところは認めるものの、cf のスタックが壊れたときのトラブルシューティングが必要以上に複雑で厄介というのも事実ではある。一方で壊れた cf スタックを5時間ぐらいかけて直したのではまりポイントはいくつかも学ぶことができた。しんどかったけど。例えば、あるセキュリティグループのインバウンドルールに別のセキュリティグループを関連付けるとき、1つの設定ではうまくいかなくて次の2つの設定を追加した。これが適切かどうかわからないが、この設定で cdk でデプロイしたスタックの環境と既存リソースとの環境が整合した状態 (ドリフトが解消される) になった。こういうのが cdk の抽象化による訳のわからないところの1つ。
otherSecurityGroup.addIngressRule( ec2.SecurityGroup.fromSecurityGroupId(this, &amp;#39;my security group&amp;#39;, mySgId), ec2.Port.tcp(80), &amp;#34;my inboud rule&amp;#34;, ) otherSecurityGroup.addIngressRule( ec2.Peer.securityGroupId(mySgId), ec2.Port.tcp(80), &amp;#34;my inboud rule&amp;#34;, )</description><content>&lt;p>2時に寝て6時半に起きた。インフラエンジニアになったのでみんなが作業していない時間にインフラの保守作業をするようにしている。昼はアプリケーションエンジニア、夜はインフラエンジニアみたいな生活になっていてしんどい。&lt;/p>
&lt;h2 id="壊れた-cf-スタックの更新">壊れた cf スタックの更新&lt;/h2>
&lt;p>テスト環境の cf スタックを手動で更新して壊れているのを cdk で管理できるように直した。壊れていたのは次の3つ。&lt;/p>
&lt;ul>
&lt;li>rds をスナップショットからリストアしたので cf が管理している rds リソースが存在しない&lt;/li>
&lt;li>iam の acl 設定が異なる&lt;/li>
&lt;li>セキュリティグループのインバウンドルールが異なる&lt;/li>
&lt;/ul>
&lt;p>aws 的にもそういった状況は認識していて cdk で同期できなくなった cf スタックを更新する手順を提供している。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aws.amazon.com/jp/premiumsupport/knowledge-center/delete-cf-stack-retain-resources/">AWS CloudFormation スタックを削除したときにリソースの一部を保持する方法を教えてください。&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>ざっくり手順をまとめると次になる。&lt;/p>
&lt;ol>
&lt;li>対象のリソースに DeletetionPolicy=Retain にセットする&lt;/li>
&lt;li>テンプレートからリソースを削除して、スタックの更新を実行する&lt;/li>
&lt;li>テンプレート内のリソースの実際の状態を describe して、スタック内に既存のリソースをインポートする&lt;/li>
&lt;/ol>
&lt;p>リソースの設定ぐらいなら既存のリソースからインポートしなくても cf のテンプレートを直接書き換えたものをアップロードしてスタックを更新するのでも大丈夫だったりする。しかし、cdk もそのテンプレートにあうように修正しないといけないため、cdk のコードとテンプレートのコードの両方をチェックしながら検証する必要がある。cdk でリソース管理ができるようになったからといって、それが変更前の既存のリソースの設定と同じかどうかは人間が目でみて検証しないといけない。これがあちこちで参照されているリソースだと追跡するのが面倒くさいといった手間暇がかかる。&lt;/p>
&lt;p>cdk がよいものかどうか、私はまだ判断がつかないけど、cf を抽象化して便利になっているところは認めるものの、cf のスタックが壊れたときのトラブルシューティングが必要以上に複雑で厄介というのも事実ではある。一方で壊れた cf スタックを5時間ぐらいかけて直したのではまりポイントはいくつかも学ぶことができた。しんどかったけど。例えば、あるセキュリティグループのインバウンドルールに別のセキュリティグループを関連付けるとき、1つの設定ではうまくいかなくて次の2つの設定を追加した。これが適切かどうかわからないが、この設定で cdk でデプロイしたスタックの環境と既存リソースとの環境が整合した状態 (ドリフトが解消される) になった。こういうのが cdk の抽象化による訳のわからないところの1つ。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-typescript" data-lang="typescript">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">otherSecurityGroup&lt;/span>.&lt;span style="color:#a6e22e">addIngressRule&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ec2&lt;/span>.&lt;span style="color:#a6e22e">SecurityGroup&lt;/span>.&lt;span style="color:#a6e22e">fromSecurityGroupId&lt;/span>(&lt;span style="color:#66d9ef">this&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;my security group&amp;#39;&lt;/span>, &lt;span style="color:#a6e22e">mySgId&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ec2&lt;/span>.&lt;span style="color:#a6e22e">Port&lt;/span>.&lt;span style="color:#a6e22e">tcp&lt;/span>(&lt;span style="color:#ae81ff">80&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;my inboud rule&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">otherSecurityGroup&lt;/span>.&lt;span style="color:#a6e22e">addIngressRule&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ec2&lt;/span>.&lt;span style="color:#a6e22e">Peer&lt;/span>.&lt;span style="color:#a6e22e">securityGroupId&lt;/span>(&lt;span style="color:#a6e22e">mySgId&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ec2&lt;/span>.&lt;span style="color:#a6e22e">Port&lt;/span>.&lt;span style="color:#a6e22e">tcp&lt;/span>(&lt;span style="color:#ae81ff">80&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;my inboud rule&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>cdk のメジャーバージョンのマイグレーション</title><link>/diary/posts/2022/0407/</link><pubDate>Thu, 07 Apr 2022 06:10:00 +0900</pubDate><guid>/diary/posts/2022/0407/</guid><description>0時に寝て5時に起きた。開発者にインフラ変更の影響を出さないように6時半からインフラのお仕事してた。
cdk v1 と v2 の違い AWS CDK Versions には v1 と v2 の2つがある。新規で作るものは v2 を選択すればよいけど、既存のスタックが v1 だとマイグレーションが必要になる。cdk は bootstrap したときに CDKToolkit というスタックを生成する。cdk をアップグレードするというのはこのスタックの設定も更新する必要がある。デフォルト設定をそのまま使っていればマイグレーションはそんなに難しくはないはずだけど、設定をカスタマイズしていたりするといくつかパラメーターを調整したりしなかったりしてややこしいかもしれない。
Migrating to AWS CDK v2 Bootstrapping また v2 は v1 の experimental な機能は移行されていないため、v1 のライブラリを直接使うか、自前でその機能を実装するといったことも必要になる可能性がある。
cdk v2 を使っていて v1 にある機能が v2 になくてあれー？って感じで調べてたら experimental な機能はまだ移行されてないらしい。https://t.co/SbRZ5ddrTj
&amp;mdash; Tetsuya Morimoto (@t2y) April 7, 2022 例えば、v1 の apigwv2.VpcLink というメソッドは experimental で v2 に移行されていないため、v2 に移行されている stable な CfnVpcLink という機能を使って次のように実装した。これは v1 の cdk の実装をみて同じように実装しただけ。</description><content>&lt;p>0時に寝て5時に起きた。開発者にインフラ変更の影響を出さないように6時半からインフラのお仕事してた。&lt;/p>
&lt;h2 id="cdk-v1-と-v2-の違い">cdk v1 と v2 の違い&lt;/h2>
&lt;p>&lt;a href="https://docs.aws.amazon.com/cdk/api/versions.html">AWS CDK Versions&lt;/a> には v1 と v2 の2つがある。新規で作るものは v2 を選択すればよいけど、既存のスタックが v1 だとマイグレーションが必要になる。cdk は bootstrap したときに CDKToolkit というスタックを生成する。cdk をアップグレードするというのはこのスタックの設定も更新する必要がある。デフォルト設定をそのまま使っていればマイグレーションはそんなに難しくはないはずだけど、設定をカスタマイズしていたりするといくつかパラメーターを調整したりしなかったりしてややこしいかもしれない。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.aws.amazon.com/cdk/v2/guide/migrating-v2.html">Migrating to AWS CDK v2&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/cdk/v2/guide/bootstrapping.html">Bootstrapping&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>また v2 は v1 の experimental な機能は移行されていないため、v1 のライブラリを直接使うか、自前でその機能を実装するといったことも必要になる可能性がある。&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="ja" dir="ltr">cdk v2 を使っていて v1 にある機能が v2 になくてあれー？って感じで調べてたら experimental な機能はまだ移行されてないらしい。&lt;a href="https://t.co/SbRZ5ddrTj">https://t.co/SbRZ5ddrTj&lt;/a>&lt;/p>&amp;mdash; Tetsuya Morimoto (@t2y) &lt;a href="https://twitter.com/t2y/status/1511924087450640386?ref_src=twsrc%5Etfw">April 7, 2022&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>例えば、v1 の apigwv2.VpcLink というメソッドは experimental で v2 に移行されていないため、v2 に移行されている stable な CfnVpcLink という機能を使って次のように実装した。これは v1 の cdk の実装をみて同じように実装しただけ。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/pull/10531/files#diff-1cf3aaf7b2b6b2e72123b93cb3108eb9e9a3291e588d62eb2cd34dd0509d3045">https://github.com/aws/aws-cdk/pull/10531/files#diff-1cf3aaf7b2b6b2e72123b93cb3108eb9e9a3291e588d62eb2cd34dd0509d3045&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-diff" data-lang="diff">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- const apiGwVpcLink = new apigwv2.VpcLink(this, &amp;#39;ApiGwVpcLink&amp;#39;, {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- vpc: vpc,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- vpcLinkName: &amp;#39;my-vpc-link&amp;#39;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- securityGroups: [mySecurityGroup]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&lt;/span>&lt;span style="color:#a6e22e">+ const apiGwVpcLink = new apigwv2.CfnVpcLink(this, &amp;#39;ApiGwVpcLink&amp;#39;, {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">+ name: &amp;#39;my-vpc-link&amp;#39;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">+ subnetIds: vpc.privateSubnets.map(sb =&amp;gt; sb.subnetId),
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">+ securityGroupIds: [mySecurityGroup.securityGroupId]
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>ecs の draining とタスクの停止時間</title><link>/diary/posts/2022/0406/</link><pubDate>Wed, 06 Apr 2022 06:30:16 +0900</pubDate><guid>/diary/posts/2022/0406/</guid><description>0時に寝て4時に起きた。なんか起きてから sns のタイムラインを眺めてた。6時半にはオフィスについて cdk のコードを読み始めた。
ecs の draining に時間がかかる？ cdk でインフラのデプロイをしていて、ecs のタスクの置き換えにやたら時間がかかっているのに気付いた。調べてみると、aws のドキュメントがすぐにヒットした。デフォルトでは停止するまでに5分ぐらいかかってしまうようだけど、それを調整したかったらいくつかパラメーターがある。
コンテナインスタンスが DRAINING に設定されているときに、Amazon ECS タスクが停止するのに時間がかかるトラブルシューティング方法を教えてください。 ecs サービスの deployment configuration minimumHealthyPercent: 同時に停止できるタスクの割合設定 maximumPercent: draining されるタスクが停止するまで置き換えるタスクを開始するかどうかの設定？ ロードバランサーの deregistration delay deregistrationDelay: elb(nlb) が登録解除処理が完了するまでに待つ時間。タスクが draining の状態になってこの時間が過ぎた後に登録解除して target が未使用になる ecs タスク定義の stop timeout stopTimeout: コンテナーが正常終了しないときに ecs が強制的にプロセスを kill するまでの待ち時間 それぞれのインフラの状況にあわせて適切なパラメーターを変更すればよい。私が管理しているのは次の2つを変更した。
maximumPercent: 100 -&amp;gt; 200 (%) deregistrationDelay: 300 -&amp;gt; 30 (秒) これで18分ほどかかっていたデプロイ時間を8分ぐらいまで短縮できた。テスト環境の設定なので多少のエラーが発生したとしても速い方がよい。</description><content>&lt;p>0時に寝て4時に起きた。なんか起きてから sns のタイムラインを眺めてた。6時半にはオフィスについて cdk のコードを読み始めた。&lt;/p>
&lt;h2 id="ecs-の-draining-に時間がかかる">ecs の draining に時間がかかる？&lt;/h2>
&lt;p>cdk でインフラのデプロイをしていて、ecs のタスクの置き換えにやたら時間がかかっているのに気付いた。調べてみると、aws のドキュメントがすぐにヒットした。デフォルトでは停止するまでに5分ぐらいかかってしまうようだけど、それを調整したかったらいくつかパラメーターがある。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aws.amazon.com/jp/premiumsupport/knowledge-center/ecs-tasks-stop-delayed-draining/">コンテナインスタンスが DRAINING に設定されているときに、Amazon ECS タスクが停止するのに時間がかかるトラブルシューティング方法を教えてください。&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="ecs-サービスの-deployment-configuration">ecs サービスの deployment configuration&lt;/h4>
&lt;ul>
&lt;li>minimumHealthyPercent: 同時に停止できるタスクの割合設定&lt;/li>
&lt;li>maximumPercent: draining されるタスクが停止するまで置き換えるタスクを開始するかどうかの設定？&lt;/li>
&lt;/ul>
&lt;h4 id="ロードバランサーの-deregistration-delay">ロードバランサーの deregistration delay&lt;/h4>
&lt;ul>
&lt;li>deregistrationDelay: elb(nlb) が登録解除処理が完了するまでに待つ時間。タスクが draining の状態になってこの時間が過ぎた後に登録解除して target が未使用になる&lt;/li>
&lt;/ul>
&lt;h4 id="ecs-タスク定義の-stop-timeout">ecs タスク定義の stop timeout&lt;/h4>
&lt;ul>
&lt;li>stopTimeout: コンテナーが正常終了しないときに ecs が強制的にプロセスを kill するまでの待ち時間&lt;/li>
&lt;/ul>
&lt;p>それぞれのインフラの状況にあわせて適切なパラメーターを変更すればよい。私が管理しているのは次の2つを変更した。&lt;/p>
&lt;ul>
&lt;li>maximumPercent: 100 -&amp;gt; 200 (%)&lt;/li>
&lt;li>deregistrationDelay: 300 -&amp;gt; 30 (秒)&lt;/li>
&lt;/ul>
&lt;p>これで18分ほどかかっていたデプロイ時間を8分ぐらいまで短縮できた。テスト環境の設定なので多少のエラーが発生したとしても速い方がよい。&lt;/p></content></item><item><title>再びのインフラエンジニア</title><link>/diary/posts/2022/0405/</link><pubDate>Tue, 05 Apr 2022 06:30:11 +0900</pubDate><guid>/diary/posts/2022/0405/</guid><description>0時に寝て7時に起きた。
インフラタスクに専念 本当はインフラ担当者が別途いるのだけど、多忙過ぎて、インフラタスクが1ヶ月近く遅延していて、プロジェクト内で合意を得て私がすべて巻き取ることにした。内容の如何に依らず、その一切合切をすべて巻き取ると宣言した。過去に働いた会社でも他の担当者ができなかった業務を後からリカバリするのはよくやってたのでそれ自体は構わない。ただ他人のタスクを肩代わりしても評価されないことも多くて、もともと私のタスクではないから誰がやったかなんか忘れてしまうんよね。私もとくにアピールしないからそう認識されても構わないのだけど、そういう業務が増えてくるとその職場を辞めるきっかけにもなってた。
インフラ担当者や他の社員さんにヒアリングしながら現時点でも十数個のタスクがある。過去のインフラの負債も含めて2-3週間ぐらい、私が集中的にやればすべて片がつくのではないかと考えている。今日たまたまスクラムのリファインメントやってて、業務の人から他の機能開発が遅れているのに2-3週間もインフラ作業に専念するってどういうこと？インフラタスクってインフラ担当者にやってもらえないんですか？と質問を受けて、できるんならその方が望ましいけど、過去の実績からまったく進捗しないのでこちらでやるざるを得ない状況というのを説明した。業務の人からみたらインフラなんか何をやっているかわからないからそんなもんよね。これから2-3週間経って蓄積したインフラタスクをすべて解決した後で少し時間が経つとインフラ担当者が全部やったように外部からはみえてしまうというのを、過去に何度も経験した。</description><content>&lt;p>0時に寝て7時に起きた。&lt;/p>
&lt;h2 id="インフラタスクに専念">インフラタスクに専念&lt;/h2>
&lt;p>本当はインフラ担当者が別途いるのだけど、多忙過ぎて、インフラタスクが1ヶ月近く遅延していて、プロジェクト内で合意を得て私がすべて巻き取ることにした。内容の如何に依らず、その一切合切をすべて巻き取ると宣言した。過去に働いた会社でも他の担当者ができなかった業務を後からリカバリするのはよくやってたのでそれ自体は構わない。ただ他人のタスクを肩代わりしても評価されないことも多くて、もともと私のタスクではないから誰がやったかなんか忘れてしまうんよね。私もとくにアピールしないからそう認識されても構わないのだけど、そういう業務が増えてくるとその職場を辞めるきっかけにもなってた。&lt;/p>
&lt;p>インフラ担当者や他の社員さんにヒアリングしながら現時点でも十数個のタスクがある。過去のインフラの負債も含めて2-3週間ぐらい、私が集中的にやればすべて片がつくのではないかと考えている。今日たまたまスクラムのリファインメントやってて、業務の人から他の機能開発が遅れているのに2-3週間もインフラ作業に専念するってどういうこと？インフラタスクってインフラ担当者にやってもらえないんですか？と質問を受けて、できるんならその方が望ましいけど、過去の実績からまったく進捗しないのでこちらでやるざるを得ない状況というのを説明した。業務の人からみたらインフラなんか何をやっているかわからないからそんなもんよね。これから2-3週間経って蓄積したインフラタスクをすべて解決した後で少し時間が経つとインフラ担当者が全部やったように外部からはみえてしまうというのを、過去に何度も経験した。&lt;/p></content></item></channel></rss>