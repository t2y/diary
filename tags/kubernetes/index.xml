<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>kubernetes on forest nook</title><link>/diary/tags/kubernetes/</link><description>Recent content in kubernetes on forest nook</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><copyright>© 2021 Tetsuya Morimoto</copyright><lastBuildDate>Wed, 11 May 2022 08:31:36 +0900</lastBuildDate><atom:icon>/diary/favicon.ico</atom:icon><icon>/diary/favicon.ico</icon><atom:link href="/diary/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>helm を調べた</title><link>/diary/posts/2022/0511/</link><pubDate>Wed, 11 May 2022 08:31:36 +0900</pubDate><guid>/diary/posts/2022/0511/</guid><description>1時に寝て6時半に起きて8時に起きた。前日は資料づくりで遅くまでオフィスに残っていたせいか、なんか寝坊した。
helm 調査 k8s 上の datadog-agent が helm で管理されていて、あるバージョンから dapr も helm 管理できる ようになった。dapr は cli からもインストールできるけど、helm のことをよくわかってなかったので調べることにした。そんなたくさん記事をみたわけではないけど、いくつか記事を読んで quora のやり取りが一番よかった。
When should you use Kubernetes Helm and not use it? ざっくりまとめるとこうかな。
helm は oss 且つ cncf の公式プロジェクトだからまぁ安心 helm はサードパーティのパッケージのインストールや設定の利便性を高める k8s はテンプレート機能が弱いので共通設定と特定環境向けの設定を管理するのがあまり得意ではない セキュリティを考慮した k8s 設定は自分でやるよりコミュニティに任せた方がよい場合もある パッケージなのでバージョン管理は得意 helm は k8s 向けのパッケージマネージャとレポジトリマネージャーとマーケットプレイスを組み合わせたみたいなもの k8s 上でサードパーティのパッケージを自分で設定したい特別な理由がない限りは helm を使うのがよさそうという結論になった。</description><content>&lt;p>1時に寝て6時半に起きて8時に起きた。前日は資料づくりで遅くまでオフィスに残っていたせいか、なんか寝坊した。&lt;/p>
&lt;h2 id="helm-調査">helm 調査&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2022/0201/#kubernetes-のログ管理と-datadog-agent-のログ連携不具合">k8s 上の datadog-agent&lt;/a> が &lt;a href="https://helm.sh/">helm&lt;/a> で管理されていて、あるバージョンから &lt;a href="https://docs.dapr.io/operations/hosting/kubernetes/kubernetes-deploy/#install-with-helm-advanced">dapr も helm 管理できる&lt;/a> ようになった。dapr は cli からもインストールできるけど、helm のことをよくわかってなかったので調べることにした。そんなたくさん記事をみたわけではないけど、いくつか記事を読んで quora のやり取りが一番よかった。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.quora.com/When-should-you-use-Kubernetes-Helm-and-not-use-it">When should you use Kubernetes Helm and not use it?&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>ざっくりまとめるとこうかな。&lt;/p>
&lt;ul>
&lt;li>helm は oss 且つ cncf の公式プロジェクトだからまぁ安心&lt;/li>
&lt;li>helm はサードパーティのパッケージのインストールや設定の利便性を高める
&lt;ul>
&lt;li>k8s はテンプレート機能が弱いので共通設定と特定環境向けの設定を管理するのがあまり得意ではない&lt;/li>
&lt;li>セキュリティを考慮した k8s 設定は自分でやるよりコミュニティに任せた方がよい場合もある&lt;/li>
&lt;li>パッケージなのでバージョン管理は得意&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>helm は k8s 向けのパッケージマネージャとレポジトリマネージャーとマーケットプレイスを組み合わせたみたいなもの&lt;/li>
&lt;/ul>
&lt;p>k8s 上でサードパーティのパッケージを自分で設定したい特別な理由がない限りは helm を使うのがよさそうという結論になった。&lt;/p></content></item><item><title>spring boot の環境とログ設定</title><link>/diary/posts/2022/0324/</link><pubDate>Thu, 24 Mar 2022 07:54:35 +0900</pubDate><guid>/diary/posts/2022/0324/</guid><description>0時に寝て4時に起きて6時に起きた。
spring のプロファイル設定 spring の Profiles の仕組みを使って環境ごとの設定を作る。デプロイは k8s で管理しているため、spring boot の Externalized Configuration の仕組みを使って、環境変数から application.yml に定義された設定を書き換える。k8s は kustomize で管理していて prod, test, dev の3つの環境で任意の設定を記述できる。
問題はログ出力の設定を環境ごとに変えたい。具体的には datadog に連携されるログは構造化ログ (json lines) を、ローカルの開発ではコンソールログをみたい。Log4j Spring Boot Support によると、1つの設定ファイルに複数のプロファイル設定を記述できるようにもみえるけど、実際にやってみたらうまく動かなかった。xml ではなく yml を使っているせいかもしれないし、私の記述方法が誤っているのかもしれない。いずれにしても yml で複数のプロファイルを設定しているサンプルをみつけられなかった。
そこで Different Log4j2 Configurations per Spring Profile をみて、環境ごとにログ設定ファイルも分割することにした。application.yml には次のように記述する。
spring: profiles: active: dev logging: config: classpath:log4j2-${spring.profiles.active}.yml ローカル開発向けの lgo4j2-dev.yml は次のようになる。
Configuration: status: warn name: YAMLConfig appenders: Console: name: STDOUT target: SYSTEM_OUT PatternLayout: Pattern: &amp;#34;%d{yyyy-MM-dd HH:mm:ss.</description><content>&lt;p>0時に寝て4時に起きて6時に起きた。&lt;/p>
&lt;h2 id="spring-のプロファイル設定">spring のプロファイル設定&lt;/h2>
&lt;p>spring の &lt;a href="https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.profiles">Profiles&lt;/a> の仕組みを使って環境ごとの設定を作る。デプロイは k8s で管理しているため、spring boot の &lt;a href="https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.external-config">Externalized Configuration&lt;/a> の仕組みを使って、環境変数から application.yml に定義された設定を書き換える。k8s は kustomize で管理していて prod, test, dev の3つの環境で任意の設定を記述できる。&lt;/p>
&lt;p>問題はログ出力の設定を環境ごとに変えたい。具体的には datadog に連携されるログは構造化ログ (json lines) を、ローカルの開発ではコンソールログをみたい。&lt;a href="https://logging.apache.org/log4j/2.x/log4j-spring-boot/index.html">Log4j Spring Boot Support&lt;/a> によると、1つの設定ファイルに複数のプロファイル設定を記述できるようにもみえるけど、実際にやってみたらうまく動かなかった。xml ではなく yml を使っているせいかもしれないし、私の記述方法が誤っているのかもしれない。いずれにしても yml で複数のプロファイルを設定しているサンプルをみつけられなかった。&lt;/p>
&lt;p>そこで &lt;a href="https://www.baeldung.com/spring-log4j2-config-per-profile">Different Log4j2 Configurations per Spring Profile&lt;/a> をみて、環境ごとにログ設定ファイルも分割することにした。application.yml には次のように記述する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yml" data-lang="yml">&lt;span style="color:#f92672">spring&lt;/span>:
&lt;span style="color:#f92672">profiles&lt;/span>:
&lt;span style="color:#f92672">active&lt;/span>: &lt;span style="color:#ae81ff">dev&lt;/span>
&lt;span style="color:#f92672">logging&lt;/span>:
&lt;span style="color:#f92672">config&lt;/span>: &lt;span style="color:#ae81ff">classpath:log4j2-${spring.profiles.active}.yml&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>ローカル開発向けの lgo4j2-dev.yml は次のようになる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yml" data-lang="yml">&lt;span style="color:#f92672">Configuration&lt;/span>:
&lt;span style="color:#f92672">status&lt;/span>: &lt;span style="color:#ae81ff">warn&lt;/span>
&lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">YAMLConfig&lt;/span>
&lt;span style="color:#f92672">appenders&lt;/span>:
&lt;span style="color:#f92672">Console&lt;/span>:
&lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">STDOUT&lt;/span>
&lt;span style="color:#f92672">target&lt;/span>: &lt;span style="color:#ae81ff">SYSTEM_OUT&lt;/span>
&lt;span style="color:#f92672">PatternLayout&lt;/span>:
&lt;span style="color:#f92672">Pattern&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;%d{yyyy-MM-dd HH:mm:ss.SSS}[%t]%-5level %logger{36} - %msg%n&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>k8s のマニフェストで環境変数を次のように定義すれば prod というプロファイルが設定される。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yml" data-lang="yml">&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>
&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment&lt;/span>
&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">my-service&lt;/span>
&lt;span style="color:#f92672">spec&lt;/span>:
&lt;span style="color:#f92672">template&lt;/span>:
&lt;span style="color:#f92672">spec&lt;/span>:
&lt;span style="color:#f92672">containers&lt;/span>:
- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">my-service&lt;/span>
&lt;span style="color:#f92672">env&lt;/span>:
- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">spring.profiles.active&lt;/span>
&lt;span style="color:#f92672">value&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;prod&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>クラウド環境向けの log4j2-prod.yml は次のようになる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yml" data-lang="yml">&lt;span style="color:#f92672">Configuration&lt;/span>:
&lt;span style="color:#f92672">status&lt;/span>: &lt;span style="color:#ae81ff">warn&lt;/span>
&lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">YAMLConfig&lt;/span>
&lt;span style="color:#f92672">appenders&lt;/span>:
&lt;span style="color:#f92672">Console&lt;/span>:
&lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">STDOUT&lt;/span>
&lt;span style="color:#f92672">target&lt;/span>: &lt;span style="color:#ae81ff">SYSTEM_OUT&lt;/span>
&lt;span style="color:#f92672">EcsLayout&lt;/span>:
&lt;span style="color:#f92672">serviceName&lt;/span>: &lt;span style="color:#ae81ff">my-service&lt;/span>
&lt;span style="color:#f92672">serviceNodeName&lt;/span>: &lt;span style="color:#66d9ef">null&lt;/span>
&lt;span style="color:#f92672">includeMarkers&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;span style="color:#f92672">KeyValuePair&lt;/span>:
- &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">type&lt;/span>
&lt;span style="color:#f92672">value&lt;/span>: &lt;span style="color:#ae81ff">app&lt;/span>
&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>k8s のロールバック</title><link>/diary/posts/2022/0313/</link><pubDate>Sun, 13 Mar 2022 12:07:00 +0900</pubDate><guid>/diary/posts/2022/0313/</guid><description>0時に寝て7時に起きた。
k8s のロールバック Rolling Back to a Previous Revision をみながらすぐできた。ロールバックもこれまでと同様、github actions の workflow dispatch で管理できるようにした。基本的にはこれだけでロールバックできる。
$ kubectl rollout undo deployment/my-app-deploy ちょっと工夫したこととして、デプロイ時に kubernetes.io/change-cause というアノテーションに git のリビジョンもセットしておくと確認するときにちょっと楽ができる。apply した後の deployment リソースに docker イメージのタグ情報 (= git のリビジョン) を書き込んでおく。
$ kubectl apply -k ${{ env.DEPLOYMENT_ENV }} $ kubectl annotate deployment my-app-deploy kubernetes.io/change-cause=${{ env.IMAGE_TAG }} --overwrite=true kubectl から履歴をみたときに k8s のリビジョンがどの git のリビジョンを使っているかがわかりやすい。デフォルトでは何も設定されていないかもしれない。
$ kubectl rollout history deployment/my-app-deploy deployment.apps/my-app-deploy REVISION CHANGE-CAUSE 15 &amp;lt;none&amp;gt; 16 &amp;lt;none&amp;gt; 17 &amp;lt;none&amp;gt; 18 &amp;lt;none&amp;gt; 19 &amp;lt;none&amp;gt; 20 &amp;lt;none&amp;gt; 21 &amp;lt;none&amp;gt; 22 &amp;lt;none&amp;gt; 24 1f17a22a6659ea0714a21fca034645cd191e189b 27 a84e113d8b7c124178b58e2f40f57b00aae65485 28 dcf3552db0668d416ed880f6e896455d7bab194c</description><content>&lt;p>0時に寝て7時に起きた。&lt;/p>
&lt;h2 id="k8s-のロールバック">k8s のロールバック&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-to-a-previous-revision">Rolling Back to a Previous Revision&lt;/a> をみながらすぐできた。ロールバックもこれまでと同様、github actions の workflow dispatch で管理できるようにした。基本的にはこれだけでロールバックできる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl rollout undo deployment/my-app-deploy
&lt;/code>&lt;/pre>&lt;/div>&lt;p>ちょっと工夫したこととして、デプロイ時に &lt;a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#change-cause">kubernetes.io/change-cause&lt;/a> というアノテーションに git のリビジョンもセットしておくと確認するときにちょっと楽ができる。apply した後の deployment リソースに docker イメージのタグ情報 (= git のリビジョン) を書き込んでおく。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl apply -k &lt;span style="color:#e6db74">${&lt;/span>{ env.DEPLOYMENT_ENV &lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#f92672">}&lt;/span>
$ kubectl annotate deployment my-app-deploy kubernetes.io/change-cause&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>{ env.IMAGE_TAG &lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#f92672">}&lt;/span> --overwrite&lt;span style="color:#f92672">=&lt;/span>true
&lt;/code>&lt;/pre>&lt;/div>&lt;p>kubectl から履歴をみたときに k8s のリビジョンがどの git のリビジョンを使っているかがわかりやすい。デフォルトでは何も設定されていないかもしれない。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl rollout history deployment/my-app-deploy
deployment.apps/my-app-deploy
REVISION CHANGE-CAUSE
&lt;span style="color:#ae81ff">15&lt;/span> &amp;lt;none&amp;gt;
&lt;span style="color:#ae81ff">16&lt;/span> &amp;lt;none&amp;gt;
&lt;span style="color:#ae81ff">17&lt;/span> &amp;lt;none&amp;gt;
&lt;span style="color:#ae81ff">18&lt;/span> &amp;lt;none&amp;gt;
&lt;span style="color:#ae81ff">19&lt;/span> &amp;lt;none&amp;gt;
&lt;span style="color:#ae81ff">20&lt;/span> &amp;lt;none&amp;gt;
&lt;span style="color:#ae81ff">21&lt;/span> &amp;lt;none&amp;gt;
&lt;span style="color:#ae81ff">22&lt;/span> &amp;lt;none&amp;gt;
&lt;span style="color:#ae81ff">24&lt;/span> 1f17a22a6659ea0714a21fca034645cd191e189b
&lt;span style="color:#ae81ff">27&lt;/span> a84e113d8b7c124178b58e2f40f57b00aae65485
&lt;span style="color:#ae81ff">28&lt;/span> dcf3552db0668d416ed880f6e896455d7bab194c
&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>kustomize の動的な設定</title><link>/diary/posts/2022/0228/</link><pubDate>Mon, 28 Feb 2022 08:11:25 +0900</pubDate><guid>/diary/posts/2022/0228/</guid><description>23時に寝て2時に起きてそこから断続的に寝たり起きたりを繰り返して6時に半に起きた。ウクライナ情勢のニュースや記事ばかり読んでてあてられた。
kustomize の動的な設定 kustomize で管理している image のタグをデプロイ処理の中で動的に変更したい。パラメーター渡しなり環境変数なり、なにかしらあるだろうとは予測している。Demo: change image names and tags のサンプルによると、次のように実行すればよいみたい。
$ kustomize edit set image busybox=alpine:3.6 次のような kustomization.yaml をセットしてくれるみたい。
images: - name: busybox newName: alpine newTag: 3.6 タグのところをリビジョン指定できればいいだけなのでとくに SECRET を使う必要もない。このやり方で書き換えた newTag が POD のデプロイ対象になってくれればよいはず。</description><content>&lt;p>23時に寝て2時に起きてそこから断続的に寝たり起きたりを繰り返して6時に半に起きた。ウクライナ情勢のニュースや記事ばかり読んでてあてられた。&lt;/p>
&lt;h2 id="kustomize-の動的な設定">kustomize の動的な設定&lt;/h2>
&lt;p>kustomize で管理している image のタグをデプロイ処理の中で動的に変更したい。パラメーター渡しなり環境変数なり、なにかしらあるだろうとは予測している。&lt;a href="https://github.com/kubernetes-sigs/kustomize/blob/master/examples/image.md">Demo: change image names and tags&lt;/a> のサンプルによると、次のように実行すればよいみたい。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kustomize edit set image busybox&lt;span style="color:#f92672">=&lt;/span>alpine:3.6
&lt;/code>&lt;/pre>&lt;/div>&lt;p>次のような &lt;code>kustomization.yaml&lt;/code> をセットしてくれるみたい。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yml" data-lang="yml">&lt;span style="color:#f92672">images&lt;/span>:
- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">busybox&lt;/span>
&lt;span style="color:#f92672">newName&lt;/span>: &lt;span style="color:#ae81ff">alpine&lt;/span>
&lt;span style="color:#f92672">newTag&lt;/span>: &lt;span style="color:#ae81ff">3.6&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>タグのところをリビジョン指定できればいいだけなのでとくに SECRET を使う必要もない。このやり方で書き換えた &lt;code>newTag&lt;/code> が POD のデプロイ対象になってくれればよいはず。&lt;/p></content></item><item><title>wiki のドキュメント整理</title><link>/diary/posts/2022/0201/</link><pubDate>Tue, 01 Feb 2022 07:28:46 +0900</pubDate><guid>/diary/posts/2022/0201/</guid><description>23時に寝て4時半に起きた。昨日の帰りに自転車でこけて胸を強打してひたすら痛い。起き上がるのも痛い。安静にしてた。
kubernetes のログ管理と datadog-agent のログ連携不具合 先日、datadog にログ連携されていない不具合 が発生していて、その1次調査を終えたことについて書いた。緊急対応としては datadog-agent を再起動することで改善することはわかっていたので、その後、kubernetes のログ管理と datadog-agent がどうやって kubernetes クラスター上で実行されているアプリケーションのログを収集しているかを調査していた。今日は wiki に調査してわかったことなどをまとめていた。
kubernetes クラスターはコンテナランタイムに docker を使っていて、アプリケーションの stdout/stderr を docker の logging driver にリダイレクトし、JSON Lines に設定された logging driver が kubernetes ノード上にログファイルとして出力する。datadog-agent は autodiscovery 機能で pod の情報を常にポーリングしていて、pod が新たにデプロイされたらログファイルを pod 内にマウントして、そのマウントしたログファイルを読み込んでログ収集していると思われる。datadog-agent から pod の情報を取得するには kubernetes のサービスアカウントを使っていて、その credential が projected volume としてマウントされて pod 内から利用できる。その credential を使って kubelet api にリクエストすることで pod の情報を取得している。
文章で書けばたったこれだけのことなんだけど、たったこれだけのことを理解するのに次のドキュメントを読んだ。実際の調査のときはわからなかったのでもっと多くのドキュメントを読んでいる。いま書いたことを理解するならこのドキュメントを読めば理解できるはず。
https://kubernetes.io/docs/concepts/overview/components/ https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/ https://kubernetes.io/docs/concepts/cluster-administration/logging/ https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/ https://kubernetes.io/docs/concepts/storage/projected-volumes/ https://docs.datadoghq.com/agent/kubernetes/log/?tab=helm https://docs.</description><content>&lt;p>23時に寝て4時半に起きた。昨日の帰りに自転車でこけて胸を強打してひたすら痛い。起き上がるのも痛い。安静にしてた。&lt;/p>
&lt;h2 id="kubernetes-のログ管理と-datadog-agent-のログ連携不具合">kubernetes のログ管理と datadog-agent のログ連携不具合&lt;/h2>
&lt;p>先日、&lt;a href="/diary/diary/posts/2022/0127/#ログ連携の不具合調査">datadog にログ連携されていない不具合&lt;/a> が発生していて、その1次調査を終えたことについて書いた。緊急対応としては datadog-agent を再起動することで改善することはわかっていたので、その後、kubernetes のログ管理と datadog-agent がどうやって kubernetes クラスター上で実行されているアプリケーションのログを収集しているかを調査していた。今日は wiki に調査してわかったことなどをまとめていた。&lt;/p>
&lt;p>kubernetes クラスターはコンテナランタイムに docker を使っていて、アプリケーションの stdout/stderr を docker の logging driver にリダイレクトし、JSON Lines に設定された logging driver が kubernetes ノード上にログファイルとして出力する。datadog-agent は autodiscovery 機能で pod の情報を常にポーリングしていて、pod が新たにデプロイされたらログファイルを pod 内にマウントして、そのマウントしたログファイルを読み込んでログ収集していると思われる。datadog-agent から pod の情報を取得するには kubernetes のサービスアカウントを使っていて、その credential が projected volume としてマウントされて pod 内から利用できる。その credential を使って kubelet api にリクエストすることで pod の情報を取得している。&lt;/p>
&lt;p>文章で書けばたったこれだけのことなんだけど、たったこれだけのことを理解するのに次のドキュメントを読んだ。実際の調査のときはわからなかったのでもっと多くのドキュメントを読んでいる。いま書いたことを理解するならこのドキュメントを読めば理解できるはず。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/overview/components/">https://kubernetes.io/docs/concepts/overview/components/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/">https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/">https://kubernetes.io/docs/concepts/cluster-administration/logging/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/projected-volumes/">https://kubernetes.io/docs/concepts/storage/projected-volumes/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.datadoghq.com/agent/kubernetes/log/?tab=helm">https://docs.datadoghq.com/agent/kubernetes/log/?tab=helm&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.datadoghq.com/getting_started/agent/autodiscovery/?tab=kubernetes">https://docs.datadoghq.com/getting_started/agent/autodiscovery/?tab=kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>ドキュメントに書いてあることを深く理解するために、kubernetes と datadog-agent のソースコードも読んだ。どちらも go 言語で実装されている。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes">https://github.com/kubernetes/kubernetes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/DataDog/datadog-agent">https://github.com/DataDog/datadog-agent&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;code>kubectl logs&lt;/code> の振る舞いを確認するだけでも、ソースコードからは実際のログファイルを open してストリームを返しているところはわからなかった。api 呼び出しが連携されて抽象化されていて、コンポーネントの役割分担があって、何も知らずにコードを読んでいてもわからなかった。Kubernetes の低レイヤーのところは Container Runtime Interface (CRI) という標準化を行い、1.20 から docker は非推奨となり、将来的に CRI を提供する実装に置き換わるらしい。ログファイルを open する役割は CRI の実装が担うんじゃないかと思うけど、そこまでは調べきれなかった。また機会があれば CRI の実装も読んでみる。&lt;/p>
&lt;figure>&lt;img src="/diary/diary/img/2022/0201_kubectl-logs.png"/>
&lt;/figure></content></item><item><title>ヘルスチェックのレスポンスのステータスコード</title><link>/diary/posts/2022/0131/</link><pubDate>Mon, 31 Jan 2022 07:29:38 +0900</pubDate><guid>/diary/posts/2022/0131/</guid><description>1時に寝て6時に起きた。
404 のレスポンスをヘルスチェック ここ数日はお仕事でインフラ周りの調査をしていた。たまたまログをみていて、ELB のヘルスチェックが 404 になっているのを大量にみつけた。てっきりヘルスチェックを使ってないのだろうと思ってインフラ担当者に問い合わせたら、404 が返ってくることをヘルスチェックしているという。404 をチェックすることになんの意味もなく、ただ急ぎで設定する必要があったからとりあえず動かせるためにそう設定したという。一方でアプリケーション側は spring boot で開発していて、Spring Boot Actuator も導入されているので /actuator/health にアクセスすれば 200 が返ってくるようになっている。どういう経緯だったかはわからないけど、開発者に一言聞けば 404 のレスポンスをヘルスチェックすることは何もない状態でずっと運用されていたことがわかった。
アプリケーション側の Kubernetes: Ingress のマニフェストにもそういった設定が入っていて、インフラ側の CDK のコードにもそういったマニフェストがあって、両方の設定変更が必要なのか、アプリケーション側のものだけでいいのか、少し調査が必要ということになった。私も Ingress というのがなにものなのか、よくわかってないので調べて理解する機会としよう。</description><content>&lt;p>1時に寝て6時に起きた。&lt;/p>
&lt;h2 id="404-のレスポンスをヘルスチェック">404 のレスポンスをヘルスチェック&lt;/h2>
&lt;p>ここ数日はお仕事でインフラ周りの調査をしていた。たまたまログをみていて、ELB のヘルスチェックが 404 になっているのを大量にみつけた。てっきりヘルスチェックを使ってないのだろうと思ってインフラ担当者に問い合わせたら、404 が返ってくることをヘルスチェックしているという。404 をチェックすることになんの意味もなく、ただ急ぎで設定する必要があったからとりあえず動かせるためにそう設定したという。一方でアプリケーション側は spring boot で開発していて、&lt;a href="https://www.baeldung.com/spring-boot-actuators">Spring Boot Actuator&lt;/a> も導入されているので &lt;code>/actuator/health&lt;/code> にアクセスすれば 200 が返ってくるようになっている。どういう経緯だったかはわからないけど、開発者に一言聞けば 404 のレスポンスをヘルスチェックすることは何もない状態でずっと運用されていたことがわかった。&lt;/p>
&lt;p>アプリケーション側の &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Kubernetes: Ingress&lt;/a> のマニフェストにもそういった設定が入っていて、インフラ側の CDK のコードにもそういったマニフェストがあって、両方の設定変更が必要なのか、アプリケーション側のものだけでいいのか、少し調査が必要ということになった。私も Ingress というのがなにものなのか、よくわかってないので調べて理解する機会としよう。&lt;/p></content></item><item><title>datadog-agent のログ連携の不具合調査</title><link>/diary/posts/2022/0127/</link><pubDate>Thu, 27 Jan 2022 07:47:57 +0900</pubDate><guid>/diary/posts/2022/0127/</guid><description>0時に寝て4時に起きた。朝から1時間ほどドラクエタクトやってた。
ログ連携の不具合調査 少し前に本番環境で datadog-agent からログが (クラウドの) datadog に連携されていないことがわかった。kubectl logs のコマンドで確認すると、アプリケーションのログは出力されているので datadog-agent から datadog にログを送信するところの問題であるように推測された。たまたま今日、同じような現象をテスト環境で確認できた。ちょうどスクラムのプランニングでログ調査のための作業をするチケットの承認を得たところだった。満を持して発生したような障害だったので私が調査すると明言して調査してた。半日ぐらい調査して、pod 内の credential 情報が置き換わってしまうことが原因っぽいと特定できたが、なぜ置き換わってしまうのかはまだわからない。もう少し調査して解決したら会社のテックブログにいいなと思ったので、日記に書いてた内容を移行することにした。</description><content>&lt;p>0時に寝て4時に起きた。朝から1時間ほどドラクエタクトやってた。&lt;/p>
&lt;h2 id="ログ連携の不具合調査">ログ連携の不具合調査&lt;/h2>
&lt;p>少し前に本番環境で &lt;a href="https://github.com/DataDog/datadog-agent">datadog-agent&lt;/a> からログが (クラウドの) datadog に連携されていないことがわかった。kubectl logs のコマンドで確認すると、アプリケーションのログは出力されているので datadog-agent から datadog にログを送信するところの問題であるように推測された。たまたま今日、同じような現象をテスト環境で確認できた。ちょうどスクラムのプランニングでログ調査のための作業をするチケットの承認を得たところだった。満を持して発生したような障害だったので私が調査すると明言して調査してた。半日ぐらい調査して、pod 内の credential 情報が置き換わってしまうことが原因っぽいと特定できたが、なぜ置き換わってしまうのかはまだわからない。もう少し調査して解決したら会社のテックブログにいいなと思ったので、日記に書いてた内容を移行することにした。&lt;/p></content></item><item><title>頭文字Dを読了</title><link>/diary/posts/2021/1205/</link><pubDate>Sun, 05 Dec 2021 11:47:08 +0900</pubDate><guid>/diary/posts/2021/1205/</guid><description>0時に寝て7時に起きてだらだらやってて午前中は 頭文字D のアニメをみてた。漫画 (アニメも？) はすでに完結しているのでいつか読もうと思いつつ最後まで読んでいない。ゴッドフットやゴッドアームが出てくるぐらいまでは読んだ気がする。その後どうなったのかを知らない。イニシャルDをみていると、ストーリーも絵も演出もまったく派手さはなくて普通なんだけど、なぜかおもしろくて続きをみてしまうという人間の娯楽の本質をついている気がしてくる。なんでなんだろうなぁ。
頭文字D たまたま思い出したので夜に漫画喫茶行って頭文字Dを最後まで読んできた。全48巻で、31巻ぐらいから読み始めて3-4時間ぐらいで読み終えた。漫画なので仕方ないけど、対戦相手がどんどん強くなっていって勝ち方が玄人好みというのか、単純に抜いた・抜かれたの話しではなく、タイヤマネージメントがどうこうとか、恐怖に対する心理がどうこうとか、ドライバーと車のセッティングも含めた駆け引きが強くなっていって、どちらが速いかというよりは戦略通りの展開にもっていって最後はそれがうまくはまるみたいな、これまでもずっとそうだったんだけど、ここからはよりトップレベルのほんの僅かな差が勝敗を分けるといった描き方になっていったように思う。それはそれで現実に近い気はするけど、漫画的には派手な演出にならないので玄人好みなストーリーになっていった気がする。但し、そこまでやってきて最後の対戦相手だけは、個人的には納得感がなくて、ここまで緻密に作り上げてきた理論や個々のドライバーの修練の積み重ねが圧倒的天才の前にひれ伏すみたいな切り口が急展開していて、頭の切り替えができなかった感じがした。とはいえ、最後まで読み終えられてよかったし、作品としてはすごくおもしろかった。作者はモータースポーツが本当に好きなんだろうなというのが伝わってくる漫画だと思う。
ふるさと納税 あまり欲しいものもないし、ふるさと納税の行政手続きも一通り理解したから今年はやらなくてもいいかとも思っていた。しかし、paypayボーナスキャンペーン をみてやってみるかという気になった。paypay はいろんなものと連携していて見かけるたびにすごいなと思う。お得だからと必要もないものを買うことはないけど、ふるさと納税はやらなかったとしても、どのみち納税は必要なものなので還元があるということは節税につながるのかな？理屈はよくわからないけど、言いたいことは paypay はすごいという話でした。
dapr の api トークンを使った認証 Enable API token authentication in Dapr を一通り読んだ。内容はとくに難しくなく、こんな風に dapr の manifest を書けば JWT トークンを設定できますということを書いてある。私はずっとサーバーサイドばっかりやってきたからフロントエンドで使われる技術や仕組みに弱い。JWT トークンもその1つで、自分でちゃんと実装したことがないからちゃんとよく分かってない。これが OAuth2 なら provider を実装したこともあるからその仕組みも意図も理解できる。一度どこかで自分で JWT も実装してみないといけないのだろうな。
少し前にお仕事で kubernetes の secret の移行作業をやった。既存の secret にキーバリューを追加するときは patch を使う。
$ kubectl patch secret mydata -p=&amp;#39;{&amp;#34;stringData&amp;#34;:{&amp;#34;mykey&amp;#34;: &amp;#34;myvalue&amp;#34;}}&amp;#39; secret の内容を確認するときも2つのやり方がある。キーだけを確認するならこれでよい。
$ kubectl describe secrets mydata キーに対応する値もデコードして確認するならこうする。但し、閲覧注意。
$ kubectl get secret mydata -o json | jq &amp;#39;.</description><content>&lt;p>0時に寝て7時に起きてだらだらやってて午前中は &lt;a href="https://ja.wikipedia.org/wiki/%E9%A0%AD%E6%96%87%E5%AD%97D">頭文字D&lt;/a> のアニメをみてた。漫画 (アニメも？) はすでに完結しているのでいつか読もうと思いつつ最後まで読んでいない。ゴッドフットやゴッドアームが出てくるぐらいまでは読んだ気がする。その後どうなったのかを知らない。イニシャルDをみていると、ストーリーも絵も演出もまったく派手さはなくて普通なんだけど、なぜかおもしろくて続きをみてしまうという人間の娯楽の本質をついている気がしてくる。なんでなんだろうなぁ。&lt;/p>
&lt;h2 id="頭文字d">頭文字D&lt;/h2>
&lt;p>たまたま思い出したので夜に漫画喫茶行って頭文字Dを最後まで読んできた。全48巻で、31巻ぐらいから読み始めて3-4時間ぐらいで読み終えた。漫画なので仕方ないけど、対戦相手がどんどん強くなっていって勝ち方が玄人好みというのか、単純に抜いた・抜かれたの話しではなく、タイヤマネージメントがどうこうとか、恐怖に対する心理がどうこうとか、ドライバーと車のセッティングも含めた駆け引きが強くなっていって、どちらが速いかというよりは戦略通りの展開にもっていって最後はそれがうまくはまるみたいな、これまでもずっとそうだったんだけど、ここからはよりトップレベルのほんの僅かな差が勝敗を分けるといった描き方になっていったように思う。それはそれで現実に近い気はするけど、漫画的には派手な演出にならないので玄人好みなストーリーになっていった気がする。但し、そこまでやってきて最後の対戦相手だけは、個人的には納得感がなくて、ここまで緻密に作り上げてきた理論や個々のドライバーの修練の積み重ねが圧倒的天才の前にひれ伏すみたいな切り口が急展開していて、頭の切り替えができなかった感じがした。とはいえ、最後まで読み終えられてよかったし、作品としてはすごくおもしろかった。作者はモータースポーツが本当に好きなんだろうなというのが伝わってくる漫画だと思う。&lt;/p>
&lt;h2 id="ふるさと納税">ふるさと納税&lt;/h2>
&lt;p>あまり欲しいものもないし、ふるさと納税の行政手続きも一通り理解したから今年はやらなくてもいいかとも思っていた。しかし、&lt;a href="https://www.satofull.jp/static/campaign/202112_pcp.php">paypayボーナスキャンペーン&lt;/a> をみてやってみるかという気になった。paypay はいろんなものと連携していて見かけるたびにすごいなと思う。お得だからと必要もないものを買うことはないけど、ふるさと納税はやらなかったとしても、どのみち納税は必要なものなので還元があるということは節税につながるのかな？理屈はよくわからないけど、言いたいことは paypay はすごいという話でした。&lt;/p>
&lt;h2 id="dapr-の-api-トークンを使った認証">dapr の api トークンを使った認証&lt;/h2>
&lt;p>&lt;a href="https://docs.dapr.io/operations/security/api-token/">Enable API token authentication in Dapr&lt;/a> を一通り読んだ。内容はとくに難しくなく、こんな風に dapr の manifest を書けば &lt;a href="https://jwt.io/">JWT&lt;/a> トークンを設定できますということを書いてある。私はずっとサーバーサイドばっかりやってきたからフロントエンドで使われる技術や仕組みに弱い。JWT トークンもその1つで、自分でちゃんと実装したことがないからちゃんとよく分かってない。これが OAuth2 なら provider を実装したこともあるからその仕組みも意図も理解できる。一度どこかで自分で JWT も実装してみないといけないのだろうな。&lt;/p>
&lt;p>少し前にお仕事で kubernetes の secret の移行作業をやった。既存の secret にキーバリューを追加するときは patch を使う。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl patch secret mydata -p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;{&amp;#34;stringData&amp;#34;:{&amp;#34;mykey&amp;#34;: &amp;#34;myvalue&amp;#34;}}&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>secret の内容を確認するときも2つのやり方がある。キーだけを確認するならこれでよい。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl describe secrets mydata
&lt;/code>&lt;/pre>&lt;/div>&lt;p>キーに対応する値もデコードして確認するならこうする。但し、閲覧注意。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl get secret mydata -o json | jq &lt;span style="color:#e6db74">&amp;#39;.data | map_values(@base64d)&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>kustomize のパッチ適用の違い</title><link>/diary/posts/2021/1122/</link><pubDate>Mon, 22 Nov 2021 13:40:40 +0900</pubDate><guid>/diary/posts/2021/1122/</guid><description>22時ぐらいには寝て6時半に起きた。昨日はお出かけしてきてバテたんで19時頃からうたた寝を繰り返してずっと寝てた。実家に帰っていた期間を除いて、土日のどちらかを休むのはここ3ヶ月はなかったと思うし、土日の2日間ほとんど仕事をしなかったのは半年ぐらいはなかったと思う。久しぶりに土日に仕事しなかったなという印象で、その理由は業務委託のお仕事の契約が決まって余裕があるからだと思う。
kustomize の Inline Patch Inline Patch に次の3つのやり方が説明されている。
patchesStrategicMerge: Strategic Merge Patch として解析されるパッチファイルのリスト patchesJSON6902: 1つのターゲットリソースのみに適用可能な JSON Patch として解析されるパッチと関連付けされるターゲットのリスト patches: 関連付けされるターゲットとパッチのリスト。このパッチは複数のオブジェクトに適用でき、パッチが Strategic Merge Patch なのか JSON Patch かは自動的に検出 patches は patchesStrategicMerge と patchesJSON6902 の両方を記述できる。運用上は patchesStrategicMerge か patchesJSON6902 を適用したいパッチの内容によって使い分けることになる。おそらく前者は base にない要素を追加したり、base の要素をすべて置き換えたりするときに使う。後者は base にある map や list の一部の要素のみを限定して置き換えたり、削除したりするときに使う。ちなみに patchesJSON6902 の 6902 というのは RFC 6902 JavaScript Object Notation (JSON) Patch に由来する。
patchesJson6902 の例として次のような設定にパッチを適用する。base から読まれた metadata の要素から namespace のみを削除したり、spec.metadata の1番目のリストの secretKeyRef が参照する Secret を my-secret で置き換えたりできる。こういったパッチを patchesStrategicMerge で実現することはできないのではないかと思う (詳しくないので私が間違っているかもしれない) 。</description><content>&lt;p>22時ぐらいには寝て6時半に起きた。昨日はお出かけしてきてバテたんで19時頃からうたた寝を繰り返してずっと寝てた。実家に帰っていた期間を除いて、土日のどちらかを休むのはここ3ヶ月はなかったと思うし、土日の2日間ほとんど仕事をしなかったのは半年ぐらいはなかったと思う。久しぶりに土日に仕事しなかったなという印象で、その理由は業務委託のお仕事の契約が決まって余裕があるからだと思う。&lt;/p>
&lt;h2 id="kustomize-の-inline-patch">kustomize の Inline Patch&lt;/h2>
&lt;p>&lt;a href="https://kubectl.docs.kubernetes.io/guides/example/inline_patch/">Inline Patch&lt;/a> に次の3つのやり方が説明されている。&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>patchesStrategicMerge: Strategic Merge Patch として解析されるパッチファイルのリスト&lt;/li>
&lt;li>patchesJSON6902: 1つのターゲットリソースのみに適用可能な JSON Patch として解析されるパッチと関連付けされるターゲットのリスト&lt;/li>
&lt;li>patches: 関連付けされるターゲットとパッチのリスト。このパッチは複数のオブジェクトに適用でき、パッチが Strategic Merge Patch なのか JSON Patch かは自動的に検出&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>patches は patchesStrategicMerge と patchesJSON6902 の両方を記述できる。運用上は patchesStrategicMerge か patchesJSON6902 を適用したいパッチの内容によって使い分けることになる。おそらく前者は base にない要素を追加したり、base の要素をすべて置き換えたりするときに使う。後者は base にある map や list の一部の要素のみを限定して置き換えたり、削除したりするときに使う。ちなみに patchesJSON6902 の 6902 というのは &lt;a href="https://datatracker.ietf.org/doc/html/rfc6902">RFC 6902 JavaScript Object Notation (JSON) Patch&lt;/a> に由来する。&lt;/p>
&lt;p>patchesJson6902 の例として次のような設定にパッチを適用する。base から読まれた metadata の要素から namespace のみを削除したり、spec.metadata の1番目のリストの secretKeyRef が参照する Secret を my-secret で置き換えたりできる。こういったパッチを patchesStrategicMerge で実現することはできないのではないかと思う (詳しくないので私が間違っているかもしれない) 。&lt;/p>
&lt;h5 id="baseyml">base.yml&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>
&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Component&lt;/span>
&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">my-component&lt;/span>
&lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">default&lt;/span>
&lt;span style="color:#f92672">spec&lt;/span>:
&lt;span style="color:#f92672">metadata&lt;/span>:
- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">username&lt;/span>
&lt;span style="color:#f92672">value&lt;/span>: &lt;span style="color:#ae81ff">user&lt;/span>
- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">password&lt;/span>
&lt;span style="color:#f92672">secretKeyRef&lt;/span>:
&lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">base-secret&lt;/span>
&lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">password&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="kustomizationyml">kustomization.yml&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">...
&lt;span style="color:#f92672">patchesJson6902&lt;/span>:
- &lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#ae81ff">my-patch.yaml&lt;/span>
&lt;span style="color:#f92672">target&lt;/span>:
&lt;span style="color:#f92672">group&lt;/span>: &lt;span style="color:#ae81ff">apps&lt;/span>
&lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Component&lt;/span>
&lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">my-component&lt;/span>
...
&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="my-patchyaml">my-patch.yaml&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">- &lt;span style="color:#f92672">op&lt;/span>: &lt;span style="color:#ae81ff">remove&lt;/span>
&lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#ae81ff">/metadata/namespace&lt;/span>
- &lt;span style="color:#f92672">op&lt;/span>: &lt;span style="color:#ae81ff">replace&lt;/span>
&lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#ae81ff">/spec/metadata/1/secretKeyRef/name&lt;/span>
&lt;span style="color:#f92672">value&lt;/span>: &lt;span style="color:#ae81ff">my-secret&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="リーンキャンバスレビュー-前半">リーンキャンバスレビュー (前半)&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2021/1114/#リーンキャンバス">前に作ったリーンキャンバス&lt;/a> を使って友だちにプロダクトの設計をレビューしてもらった。私がリーンキャンバスを作ったことがなかったので、この項目にはどういった内容を書くか、それぞれの項目がどういった関連付けや粒度で整理するかといった、リーンキャンバスの書き方そのものも含めて教えてもらった。&lt;/p>
&lt;p>私が設計のために作った40枚のスライドを話すと2時間必要とするが、リーンキャンバスを使えば要点のみ15分で話せるようになるのが狙いになるみたい。とはいえ、リーンキャンバスの書いてある内容の半分を確認するだけで今日は2時間弱かかってしまった。議事録を取りながらだったので話すだけならもっと短くなったかもしれないし、その背景や根拠を細かくツッコミしていくとそれなりの時間はかかるのかもしれない。リーンキャンバス上は数枚の付箋で簡潔に書いてあるが、これどういうこと？みたいな問いになると詳細を説明しないといけないので時間がかかったように思う。リーンキャンバスの精度や品質が上がれば、読み手が詳細を確認しなくても意図を理解しやすくて詳細のツッコミが不要になるのかもしれない。これまで使ったことがないツールでおもしろいので週末に後半を行う。課題管理の背景には実践知、認知心理学、情報共有、組織論といった様々な分野にまたがるのでそのコンテキストを共有するのはなかなか難しいのではないかという思いもある。&lt;/p></content></item><item><title>お仕事しかしなかった一日</title><link>/diary/posts/2021/1115/</link><pubDate>Mon, 15 Nov 2021 13:13:44 +0900</pubDate><guid>/diary/posts/2021/1115/</guid><description>2時に寝て6時半に起きた。寝る前にウォーキングしてくるとよく眠れる気がする。お仕事で簡単に終わると思ってた作業にちょっとはまってトラブルシューティングしてたら疲れた。原因はわかって自己解決できたのはよかったけど、消耗して早くお仕事を終えて帰ってくつろいでた。
ローカル開発環境の整備 お仕事でローカルの k8s 環境の保守の作業をしている。minikube でローカルの k8s クラスターを作成して kubectl コマンドで制御する。k8s の yaml の設定ファイルのことをマニフェストと呼ぶのかな？そのテンプレート？ジェネレーター的なツールに kustomize を使っている。先週から1週間触っていたので cli の操作にはだいぶ慣れてきた。
まだ基本的な delete &amp;amp; apply みたいなことしかやってないけど、また余裕のあるときに細かいコマンドやロールバックのやり方なども学習しようと思う。デプロイで一番重要なのはロールバック、次にローリングアップデート、ローリングアップデートができればカナリアリリースもできるかな？その2つがあれば運用は大幅にコスト削減できるし、開発のアジリティも上げられる。たぶん k8s を使えば簡単にできるんだろうなというのは delete &amp;amp; apply だけみてもそう受け取れる。k8s クラスターさえマネージドならよく出来た仕組みだなと感心した。</description><content>&lt;p>2時に寝て6時半に起きた。寝る前にウォーキングしてくるとよく眠れる気がする。お仕事で簡単に終わると思ってた作業にちょっとはまってトラブルシューティングしてたら疲れた。原因はわかって自己解決できたのはよかったけど、消耗して早くお仕事を終えて帰ってくつろいでた。&lt;/p>
&lt;h2 id="ローカル開発環境の整備">ローカル開発環境の整備&lt;/h2>
&lt;p>お仕事でローカルの k8s 環境の保守の作業をしている。&lt;a href="https://minikube.sigs.k8s.io/docs/">minikube&lt;/a> でローカルの k8s クラスターを作成して &lt;a href="https://kubernetes.io/docs/reference/kubectl/overview/">kubectl&lt;/a> コマンドで制御する。k8s の yaml の設定ファイルのことをマニフェストと呼ぶのかな？そのテンプレート？ジェネレーター的なツールに &lt;a href="https://kustomize.io/">kustomize&lt;/a> を使っている。先週から1週間触っていたので cli の操作にはだいぶ慣れてきた。&lt;/p>
&lt;p>まだ基本的な delete &amp;amp; apply みたいなことしかやってないけど、また余裕のあるときに細かいコマンドやロールバックのやり方なども学習しようと思う。デプロイで一番重要なのはロールバック、次にローリングアップデート、ローリングアップデートができればカナリアリリースもできるかな？その2つがあれば運用は大幅にコスト削減できるし、開発のアジリティも上げられる。たぶん k8s を使えば簡単にできるんだろうなというのは delete &amp;amp; apply だけみてもそう受け取れる。k8s クラスターさえマネージドならよく出来た仕組みだなと感心した。&lt;/p></content></item><item><title>Kubernetes 使い始めの雑感</title><link>/diary/posts/2021/1108/</link><pubDate>Mon, 08 Nov 2021 08:40:51 +0900</pubDate><guid>/diary/posts/2021/1108/</guid><description>1時に寝て7時に起きた。夜にウォーキングし始めてからよく眠れるようになった気がする。
udemy: Kubernetes入門 昨日 の続き。今日はセクション6から最後まで。CI/CD のセクションだけスキップして、他は一通り目を通した。
セクション6 Kubernetes実践 1つずつ書くのは大変だけど、数をこなして徐々に覚えていけばよい。手で書くのもよいが、別のやり方としてクライアント側で dry run すると、設定のひな型を作ってくれるのでそれに必要な設定を足すのもよい。
$ kubectl create deploy mysql --image=mysql:5.7 --dry-run=client -o yaml $ kubectl exec -n database -it mysql-787f86d65c-nflxx -- mysql -uroot -ppassword データベースとアプリケーションを異なる namespace にデプロイして、それらが通信できるような設定を行う。基本的には --dry-run=client でひな型を作りつつ、必要な設定を追加していくやり方が簡単そうにみえた。とはいえ、実際に設定していくときはどういう設定を追加するとどういう振る舞いになるかを調べながら作業すると思うのでこんな簡単にはできないとは思う。次のようなアプリケーションをデプロイする一覧の流れを理解できた。
namespace 作成 Deployment 作成 ConfigMap 作成 Secret 作成 Deployment, ConfigMap, Secret 適用 Service 適用 port-forward でローカルからアクセス (作成したリソースをすべて削除) セクション7 KubernetesのDebug 基本は pod のステータスを確認しながら問題があれば、その箇所を追いかけていって原因を調査する。
$ kubectl get pod -A $ kubectl get pod -A --selector run=nginx k8s 上で実行しているアプリケーションの依存先へ接続できない場合は Service の確認が必要となる。kubectl の get, describe, logs などのサブコマンドをあれこれみながらエラーの原因を把握して、yaml の設定を変更していく。k8s のアーキテクチャとコマンドを覚えていないとなかなか難しそう。</description><content>&lt;p>1時に寝て7時に起きた。夜にウォーキングし始めてからよく眠れるようになった気がする。&lt;/p>
&lt;h2 id="udemy-kubernetes入門">udemy: Kubernetes入門&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2021/1107/#udemy-kubernetes入門">昨日&lt;/a> の続き。今日はセクション6から最後まで。CI/CD のセクションだけスキップして、他は一通り目を通した。&lt;/p>
&lt;h3 id="セクション6-kubernetes実践">セクション6 Kubernetes実践&lt;/h3>
&lt;p>1つずつ書くのは大変だけど、数をこなして徐々に覚えていけばよい。手で書くのもよいが、別のやり方としてクライアント側で dry run すると、設定のひな型を作ってくれるのでそれに必要な設定を足すのもよい。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl create deploy mysql --image&lt;span style="color:#f92672">=&lt;/span>mysql:5.7 --dry-run&lt;span style="color:#f92672">=&lt;/span>client -o yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl exec -n database -it mysql-787f86d65c-nflxx -- mysql -uroot -ppassword
&lt;/code>&lt;/pre>&lt;/div>&lt;p>データベースとアプリケーションを異なる namespace にデプロイして、それらが通信できるような設定を行う。基本的には &lt;code>--dry-run=client&lt;/code> でひな型を作りつつ、必要な設定を追加していくやり方が簡単そうにみえた。とはいえ、実際に設定していくときはどういう設定を追加するとどういう振る舞いになるかを調べながら作業すると思うのでこんな簡単にはできないとは思う。次のようなアプリケーションをデプロイする一覧の流れを理解できた。&lt;/p>
&lt;ol>
&lt;li>namespace 作成&lt;/li>
&lt;li>Deployment 作成&lt;/li>
&lt;li>ConfigMap 作成&lt;/li>
&lt;li>Secret 作成&lt;/li>
&lt;li>Deployment, ConfigMap, Secret 適用&lt;/li>
&lt;li>Service 適用&lt;/li>
&lt;li>port-forward でローカルからアクセス&lt;/li>
&lt;li>(作成したリソースをすべて削除)&lt;/li>
&lt;/ol>
&lt;h3 id="セクション7-kubernetesのdebug">セクション7 KubernetesのDebug&lt;/h3>
&lt;p>基本は pod のステータスを確認しながら問題があれば、その箇所を追いかけていって原因を調査する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl get pod -A
$ kubectl get pod -A --selector run&lt;span style="color:#f92672">=&lt;/span>nginx
&lt;/code>&lt;/pre>&lt;/div>&lt;p>k8s 上で実行しているアプリケーションの依存先へ接続できない場合は Service の確認が必要となる。kubectl の get, describe, logs などのサブコマンドをあれこれみながらエラーの原因を把握して、yaml の設定を変更していく。k8s のアーキテクチャとコマンドを覚えていないとなかなか難しそう。&lt;/p>
&lt;p>とりあえず動かした後にまとめて全部削除できるのがテストやデバッグに便利そう。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl delete -f .
&lt;/code>&lt;/pre>&lt;/div>&lt;p>もしくは開発用に独自の namespace を作成して、あとで丸ごと namespace を削除するのでもよさそう。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl delete ns mynamespace
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="k8s-の調査">k8s の調査&lt;/h2>
&lt;p>業務のアプリケーションを minikube で作ったローカル k8s クラスターで動かしてみた。ローカルの開発環境の構築方法をメンテナンスして、自分でも一通り k8s の yaml を書いて、デプロイして、振る舞いを確認したりしていた。最初なのでおもしろい。自分で一通りやってみて、k8s が難しいとみんなが言っているのは k8s クラスターを自前で構築するのが難しいのだとようやく理解できた。k8s クラスターがすでにある状態なら kubectl の使い方を覚えるだけで全く難しくない。GKE や EKS を使って運用するなら k8s の運用コストは大したことがないと理解できた。k8s クラスター向けの yaml はたくさん書かないといけないけど、どうせ ECS や EC2 でやっていても CDK や Terraform などのインフラ設定を書くのは同じなのでそこはあまり差がない。k8s はコンテナオーケストレーションをやってくれるメリットが大きいので minikube と EKS の環境の差異があまり問題にならないようなアプリケーション開発であれば、普通に使っていって問題ないように思えた。ローカルで環境作るのが大変なんじゃないかという先入観があったけど、全然そんなことはなかった。コンテナのイメージをビルドしないといけないのが追加のコストかな。&lt;/p></content></item><item><title>普通の休日の翌日</title><link>/diary/posts/2021/1107/</link><pubDate>Sun, 07 Nov 2021 11:21:21 +0900</pubDate><guid>/diary/posts/2021/1107/</guid><description>5時に寝て10時に起きた。昨日は夕方に2-3時間寝てたのでその分、夜に調べものをしていた。休みたい気持ちもあるけど、調べるものが多過ぎて全然時間が足りない。
bizpy 勉強会の資料作り 昨日 の続き。昨日サンプルコードを実装したので、その設定や要点を 資料 に作成した。現時点で Python で Slack のインテグレーションをやってみる勉強会 #2 の参加者は10人。連続シリーズは回を重ねるごとに減っていくものなのでこんなもんかな。あともう1回やったら終わりにする。
udemy: Kubernetes入門 友だちから udemy の k8s のコースがよいと聞いたんだけど、そのコースはいまは提供されていなくて、せっかくなので適当に検索してヒットした Kubernetes入門 を受講することに決めた。本当は英語の本格的なコースを受講した方がよいのだろうけど、余裕のあるときはそれでいいけど、いま数日で概要を把握して使えるようにしたいので日本語のコースにしてみた。
Udemy の Learning Docker and Kubernetes by Lab がとてもよい Docker, Kubernetes 学習とツールとコンピュータサイエンス 昨日インストールした minikube のクラスターを使って「Kubernetes入門」のセクション1からセクション5までやった。だいたい半分ぐらい。所感としては、全く何も知らない人には要点をかいつまんで教えてくれるのと、最初に覚えるとよい基本的な CLI のコマンドとその振る舞いや設定を紹介してくれるのでよかった。初めて k8s に挑戦する自分にとってはちょうどよいレベル感だった。全体像の概念を捉えてコンテキストに沿って順番にハンズオン形式で学習していくスタイル。nakamasato/kubernetes-basics を使って自分でも CLI でコマンドを打ちながら進めてみた。yaml ファイルを定義するのもこれはこれで面倒だけど、この辺は慣れの問題かな？とも思う。いくつか学んだことを整理しておく。
セクション1 Introduction k8s には2つのコンポーネントがあり、これを k8s クラスターと呼んでいる。
Control Plane (API サーバー) 複数の Worker (Kubelet) yaml で設定する Desired State (理想状態) と呼ばれる設定が登録されると、Control Plane の API サーバーと Worker の kubelet が通信してそれを実現しようとする。pod とは k8s のデプロイの最小単位となる。コンテナ、ポート、レプリカ数などを設定する。pod をそれぞれの Worker にデプロイしたり、Worker がダウンしたときに別の Worker で起動させたりする。</description><content>&lt;p>5時に寝て10時に起きた。昨日は夕方に2-3時間寝てたのでその分、夜に調べものをしていた。休みたい気持ちもあるけど、調べるものが多過ぎて全然時間が足りない。&lt;/p>
&lt;h2 id="bizpy-勉強会の資料作り">bizpy 勉強会の資料作り&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2021/1106/#slack-apps-の調査">昨日&lt;/a> の続き。昨日サンプルコードを実装したので、その設定や要点を &lt;a href="https://github.com/t2y/python-study/tree/master/BizPy/slack/20211027">資料&lt;/a> に作成した。現時点で &lt;a href="https://bizpy.connpass.com/event/229091/">Python で Slack のインテグレーションをやってみる勉強会 #2&lt;/a> の参加者は10人。連続シリーズは回を重ねるごとに減っていくものなのでこんなもんかな。あともう1回やったら終わりにする。&lt;/p>
&lt;h2 id="udemy-kubernetes入門">udemy: Kubernetes入門&lt;/h2>
&lt;p>友だちから udemy の k8s のコースがよいと聞いたんだけど、そのコースはいまは提供されていなくて、せっかくなので適当に検索してヒットした &lt;a href="https://www.udemy.com/course/kubernetes-basics-2021/">Kubernetes入門&lt;/a> を受講することに決めた。本当は英語の本格的なコースを受講した方がよいのだろうけど、余裕のあるときはそれでいいけど、いま数日で概要を把握して使えるようにしたいので日本語のコースにしてみた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://blog.ayakumo.net/entry/2018/01/27/010000">Udemy の Learning Docker and Kubernetes by Lab がとてもよい&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.ayakumo.net/entry/2018/02/15/232918">Docker, Kubernetes 学習とツールとコンピュータサイエンス&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>昨日インストールした minikube のクラスターを使って「Kubernetes入門」のセクション1からセクション5までやった。だいたい半分ぐらい。所感としては、全く何も知らない人には要点をかいつまんで教えてくれるのと、最初に覚えるとよい基本的な CLI のコマンドとその振る舞いや設定を紹介してくれるのでよかった。初めて k8s に挑戦する自分にとってはちょうどよいレベル感だった。全体像の概念を捉えてコンテキストに沿って順番にハンズオン形式で学習していくスタイル。&lt;a href="https://github.com/nakamasato/kubernetes-basics">nakamasato/kubernetes-basics&lt;/a> を使って自分でも CLI でコマンドを打ちながら進めてみた。yaml ファイルを定義するのもこれはこれで面倒だけど、この辺は慣れの問題かな？とも思う。いくつか学んだことを整理しておく。&lt;/p>
&lt;h3 id="セクション1-introduction">セクション1 Introduction&lt;/h3>
&lt;p>k8s には2つのコンポーネントがあり、これを k8s クラスターと呼んでいる。&lt;/p>
&lt;ul>
&lt;li>Control Plane (API サーバー)&lt;/li>
&lt;li>複数の Worker (Kubelet)&lt;/li>
&lt;/ul>
&lt;p>yaml で設定する Desired State (理想状態) と呼ばれる設定が登録されると、Control Plane の API サーバーと Worker の kubelet が通信してそれを実現しようとする。pod とは k8s のデプロイの最小単位となる。コンテナ、ポート、レプリカ数などを設定する。pod をそれぞれの Worker にデプロイしたり、Worker がダウンしたときに別の Worker で起動させたりする。&lt;/p>
&lt;h3 id="セクション2-kubernets-概要">セクション2 Kubernets 概要&lt;/h3>
&lt;p>k8s はコンテナ化したアプリケーションのデプロイ、スケーリング、管理を行うためのオープンソースのコンテナオーケストレーションシステムである。&lt;/p>
&lt;ul>
&lt;li>コンテナ
&lt;ul>
&lt;li>独立した環境でアプリケーションを実行する仕組み&lt;/li>
&lt;li>コンテナの実態はプロセス&lt;/li>
&lt;li>Kernel Namespaces を利用し、プロセスID、ネットワークインターフェース、リソースなどを分離してコンテナ間で干渉しない&lt;/li>
&lt;li>ホストマシンへの依存度を最小化してアプリケーションをどこでも実行可能にする
&lt;ul>
&lt;li>従来のやり方の最大の違いはライブラリがホストマシンにインストールされるのではなく、コンテナの内部にインストールされる&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>オーケストレーション
&lt;ul>
&lt;li>デプロイ、スケーリング、管理などの仕組み&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>1つのアプリケーションは複数のマシン上で動かすことで可用性を高めたいが、コンテナを動かすために考えることが増えていくと管理コストも増えていく。コンテナオーケストレーション機能により次のようなシステム管理者が行っていたことが自動化される。&lt;/p>
&lt;ul>
&lt;li>デプロイメント&lt;/li>
&lt;li>スケジューリング&lt;/li>
&lt;li>オートスケーリング
&lt;ul>
&lt;li>負荷に応じてコンテナ数やマシン数を増減させる&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ネットワーク&lt;/li>
&lt;li>リソースマネジメント&lt;/li>
&lt;li>セキュリティ
&lt;ul>
&lt;li>ネットワークポリシーやリソースの権限定義&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>k8s クラスターの構造は次になる。&lt;/p>
&lt;ul>
&lt;li>Control Plane
&lt;ul>
&lt;li>api: kubelet と通信するサーバー&lt;/li>
&lt;li>etcd: 設定などを格納するキーバリューストア&lt;/li>
&lt;li>shed: kube スケジューラー&lt;/li>
&lt;li>c-m: コントロールマネージャー&lt;/li>
&lt;li>c-c-m: クラウドプロバイダと api 連携する
&lt;ul>
&lt;li>ローカルで使うときは必要ない&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Worker ノードはコンテナランタイムをいインストールしておく必要がある
&lt;ul>
&lt;li>kubelet は Control Plane と通信するためのエージェントとして動作する&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>一番需要なこととして、k8s は理想状態と現実状態を比較して、理想状態に近づけようとする。app.yaml の理想状態を kubectl を用いて api サーバーを介して etcd に格納する。現実状態は kubelet から api サーバーを介して etcd に格納される。c-m は理想状態と現実状態のチェックを行い、異なっていれば理想状態に近づけることをしていく。&lt;/p>
&lt;h3 id="セクション4-kubectl">セクション4 kubectl&lt;/h3>
&lt;p>minikube で最初に起動しているのは Control Plane を起動していることが理解できた。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ minikube start
$ kubectl config current-context
minikube
&lt;/code>&lt;/pre>&lt;/div>&lt;p>同時に ~/.kube/config に kubectl の設定も追加される。&lt;code>minikube&lt;/code> という名前でクラスター、ユーザー、コンテキストが設定される。&lt;/p>
&lt;p>リソース一覧の確認。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl api-resources
&lt;/code>&lt;/pre>&lt;/div>&lt;p>出力フォーマットも様々。例えば、デフォルトの表示は次になる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl get node
NAME STATUS ROLES AGE VERSION
minikube Ready control-plane,master 7m21s v1.22.2
$ kubectl get node -o wide
NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME
minikube Ready control-plane,master 8m38s v1.22.2 192.168.49.2 &amp;lt;none&amp;gt; Ubuntu 20.04.2 LTS 5.11.0-38-generic docker://20.10.8
&lt;/code>&lt;/pre>&lt;/div>&lt;p>より詳細な情報をそれぞれのフォーマットで表示する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl get node -o json
$ kubectl get node -o yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>namespace を確認する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl get namespace
NAME STATUS AGE
default Active 10m
kube-node-lease Active 10m
kube-public Active 10m
kube-system Active 10m
&lt;/code>&lt;/pre>&lt;/div>&lt;p>namespace を指定して pod 一覧を取得する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl get pod --namespace kube-system
NAME READY STATUS RESTARTS AGE
coredns-78fcd69978-qxqbn 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 11m
etcd-minikube 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 11m
kube-apiserver-minikube 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 11m
kube-controller-manager-minikube 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 11m
kube-proxy-g55hg 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 11m
kube-scheduler-minikube 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 11m
storage-provisioner 1/1 Running &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">(&lt;/span>10m ago&lt;span style="color:#f92672">)&lt;/span> 11m
&lt;/code>&lt;/pre>&lt;/div>&lt;p>グローバルな CLI のオプションを確認する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl options
&lt;/code>&lt;/pre>&lt;/div>&lt;p>ノードの詳細を表示する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl describe node
&lt;/code>&lt;/pre>&lt;/div>&lt;p>describe は名前の接頭辞を指定できるので namespace ならこんな感じに実行できる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl describe namespace kube-
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="セクション5-kubernetes-リソース">セクション5 Kubernetes リソース&lt;/h3>
&lt;p>pod とは k8s 上のデプロイの最小単位である。&lt;/p>
&lt;ul>
&lt;li>1つまたは複数のコンテナをもつ&lt;/li>
&lt;li>ネットワークやストレージを共有リソースとしてもつ&lt;/li>
&lt;li>コンテナの実行方法に関する仕様をもつ&lt;/li>
&lt;/ul>
&lt;p>pod が使えなくなった場合に他のノードにデプロイされることもある。1つのアプリケーションを複数の pod でデプロイすることが多い。なるべく複数のアプリケーションを1つの pod に入れない。個別の pod を直接操作しない。&lt;/p>
&lt;p>共有コンテキスト&lt;/p>
&lt;ul>
&lt;li>同一 pod 内のコンテナは同じストレージにアクセスできる&lt;/li>
&lt;li>同一 pod 内のコンテナは ip アドレスとポートを含むネットワーク名前空間を共有する&lt;/li>
&lt;/ul>
&lt;p>k8s オブジェクト&lt;/p>
&lt;ul>
&lt;li>クラスタの状態を表現する&lt;/li>
&lt;li>2つのフィールドをもつ
&lt;ul>
&lt;li>spec: 理想状態 (desired status)&lt;/li>
&lt;li>status: 現実状態 (current status)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>pod の作成は k8s オブジェクトを作成している。オブジェクト作成時の必須フィールドが4つある。&lt;/p>
&lt;ul>
&lt;li>apiVersion&lt;/li>
&lt;li>kind&lt;/li>
&lt;li>metadata&lt;/li>
&lt;li>spec&lt;/li>
&lt;/ul>
&lt;p>namespace は同一クラスター上で複数の仮想クラスターの動作をサポートする。&lt;/p>
&lt;ul>
&lt;li>仮想クラスターとは、物理的には同じマシンで動いているかもしれないが、仮想的に環境を分離している
&lt;ul>
&lt;li>1つのクラスターを論理的にわける&lt;/li>
&lt;li>チームや部署ごとにわけて使い分けたりすることも多い&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>namespace を使うメリットは次になる。&lt;/p>
&lt;ul>
&lt;li>pod やコンテナのリソースの範囲設定&lt;/li>
&lt;li>namespace 全体の総リソース制限&lt;/li>
&lt;li>権限管理&lt;/li>
&lt;/ul>
&lt;p>初期の namespace として4つあるが、初心者は最初の2つだけをまず覚えておく。&lt;/p>
&lt;ul>
&lt;li>default:&lt;/li>
&lt;li>kube-system:&lt;/li>
&lt;li>kube-public:&lt;/li>
&lt;li>kube-node-lease&lt;/li>
&lt;/ul>
&lt;p>namespace と cluster の違い。&lt;/p>
&lt;ul>
&lt;li>Namespace-scoped リソース
&lt;ul>
&lt;li>namespace に属しているリソース&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Cluster-scoped リソース
&lt;ul>
&lt;li>クラスター全体で使われるもの&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>次のコマンドで確認できる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl api-resources --namespaced&lt;span style="color:#f92672">=&lt;/span>true
&lt;/code>&lt;/pre>&lt;/div>&lt;p>namespace の作成&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl create namespace my-namespace
&lt;/code>&lt;/pre>&lt;/div>&lt;p>ワークロードリソースとは複数の pod を作成・管理するためのリソース。ワークロードリソースは pod テンプレートを使って pod を作成する。&lt;/p>
&lt;ul>
&lt;li>ReplicaSet
&lt;ul>
&lt;li>常に指定したレプリカ数の pod を保つ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Deployment
&lt;ul>
&lt;li>ローリングアップデートやロールバックなどのアップデート機能を提供&lt;/li>
&lt;li>ReplicaSet のロールアウト&lt;/li>
&lt;li>不安定な場合の前のバージョンへロールバック&lt;/li>
&lt;li>使用頻度が高い
&lt;ul>
&lt;li>ほとんどのアプリケーションは Deployment で管理&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Secret
&lt;ul>
&lt;li>機密情報を保存・管理し、Pod から参照可能&lt;/li>
&lt;li>主な使用方法としてコンテナの環境変数の設定
&lt;ul>
&lt;li>アプリケーションの DB のパスワードなどに使う&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Service
&lt;ul>
&lt;li>pod の集合を抽象化して公開する
&lt;ul>
&lt;li>pod の集合に対する DNS 名&lt;/li>
&lt;li>pod の集合に対する負荷分散&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></content></item></channel></rss>