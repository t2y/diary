<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>kubernetes on forest nook</title><link>/diary/tags/kubernetes/</link><description>Recent content in kubernetes on forest nook</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><copyright>© 2021 Tetsuya Morimoto</copyright><lastBuildDate>Wed, 20 Jul 2022 07:41:35 +0900</lastBuildDate><atom:icon>/diary/favicon.ico</atom:icon><icon>/diary/favicon.ico</icon><atom:link href="/diary/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>ノードグループと nodeSelector</title><link>/diary/posts/2022/0720/</link><pubDate>Wed, 20 Jul 2022 07:41:35 +0900</pubDate><guid>/diary/posts/2022/0720/</guid><description>1時に寝て6時に起きた。
EKS クラスターのノードグループと k8s の nodeSelector 先日 k8s の nodeSelector の調査について書いた。いまテスト環境でその調査結果の運用検証中。当初は k8s の機能だけを使いたいと考えていたが、いま eksctl コマンドで EKS クラスターを管理していて、k8s ノードの実体である ec2 のプロビジョニングはノードグループがもつ起動テンプレートとオートスケールポリシーにより制御される。そのため、ノードグループを分割してそれぞれにノードラベルが必ず付与されるように管理する方が簡単だとわかってきた。要はアプリケーションサーバー向けのノードグループとバッチ処理向けのノードグループの2つを作った。あと覚えておくとよいのが、Adding a custom instance role で任意のポリシーもノードグループの iam ロールに追加できる。設定例としてはこんな感じ。
iam: attachPolicyARNs: - arn:aws:iam::${accountId}:policy/my-custom-policy - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore ノードグループの準備が整ったら nodeSelector を指定した pod をデプロイするだけ。k8s ノードがどんなノードラベルをもっているかは次のようにして確認できる。
$ kubectl get node --show-labels 意図したノードラベルが付与された k8s ノードに pod がデプロイされたかどうかは次のようにして確認できる。
$ kubectl get pod --output wide --field-selector spec.nodeName=${ノードラベルをもつノード名} これらの環境構築、検証、wiki にドキュメントを書いて本番作業手順もまとめた。一通りきれいにまとまったインフラ作業を完遂できて気分がよい。</description><content>&lt;p>1時に寝て6時に起きた。&lt;/p>
&lt;h2 id="eks-クラスターのノードグループと-k8s-の-nodeselector">EKS クラスターのノードグループと k8s の nodeSelector&lt;/h2>
&lt;p>先日 &lt;a href="/diary/diary/posts/2022/0711/">k8s の nodeSelector&lt;/a> の調査について書いた。いまテスト環境でその調査結果の運用検証中。当初は k8s の機能だけを使いたいと考えていたが、いま eksctl コマンドで EKS クラスターを管理していて、k8s ノードの実体である ec2 のプロビジョニングはノードグループがもつ起動テンプレートとオートスケールポリシーにより制御される。そのため、ノードグループを分割してそれぞれにノードラベルが必ず付与されるように管理する方が簡単だとわかってきた。要はアプリケーションサーバー向けのノードグループとバッチ処理向けのノードグループの2つを作った。あと覚えておくとよいのが、&lt;a href="https://eksctl.io/usage/iam-policies/#adding-a-custom-instance-role">Adding a custom instance role&lt;/a> で任意のポリシーもノードグループの iam ロールに追加できる。設定例としてはこんな感じ。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yml" data-lang="yml">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">iam&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">attachPolicyARNs&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">arn:aws:iam::${accountId}:policy/my-custom-policy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ノードグループの準備が整ったら nodeSelector を指定した pod をデプロイするだけ。k8s ノードがどんなノードラベルをもっているかは次のようにして確認できる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get node --show-labels
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>意図したノードラベルが付与された k8s ノードに pod がデプロイされたかどうかは次のようにして確認できる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get pod --output wide --field-selector spec.nodeName&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>ノードラベルをもつノード名&lt;span style="color:#e6db74">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>これらの環境構築、検証、wiki にドキュメントを書いて本番作業手順もまとめた。一通りきれいにまとまったインフラ作業を完遂できて気分がよい。&lt;/p></content></item><item><title>nodeSelector を試す</title><link>/diary/posts/2022/0711/</link><pubDate>Mon, 11 Jul 2022 08:45:04 +0900</pubDate><guid>/diary/posts/2022/0711/</guid><description>4時に寝て8時に起きた。久しぶりに寝坊した。
k8s の nodeSelector 先日 定期/バッチ処理を k8s の cronjob にすべて移行 した。すでに本番運用もしていて調子もよさそうにみえる。あと残課題としてバッチ処理とアプリケーションサーバーの pod がデプロイされる k8s ノードを分割したい。現時点では、バッチ処理の負荷は小さいから同じスペックのインスタンスの k8s ノード上で混在させて運用している。しかし、いずれ運用上の問題になる懸念がある。そのため、バッチ処理のみを実行する k8s ノードを管理したい。次のドキュメントに書いてある。
Assigning Pods to Nodes k8s のドキュメントによると、大きく分けて nodeSelector と Affinity という2つのやり方がある。前者はラベルでフィルターするシンプルな仕組み、後者はさらに複雑な要件に対応するもの。いまのところ、ただ分割できればよいのでシンプルな nodeSelector で実装してみることにした。
nodeSelector Affinity and anti-affinity 余談だが、nodeSelector はいずれ Affinity に置き換わるので deprecated だと一時期ドキュメントに書かれていたらしい。具体的に決まっていることでもないため、nodeSelector: when will it be deprecated? #82184 によると deprecated という文言を含む文章がその後に削除された。Affinity は高機能且つ高コストであることから、(現時点では) nodeSelector はシンプルで推奨すべき方法とまで書いてあるのですぐになくなるわけではなさそう。
minikube で nodeSelector の検証を始めたんだけど、いくつかうまくいかないことがあって断念した。multi-node 機能を使って controll plane と worker ノードの2つを起動できたけど、worker ノードから docker host にアクセスできなかった。何かしら設定が必要なのか、別途レジストリが必要になるのかよくわからなかった。あと node にラベル設定したときに worker ノードにラベル設定しても minikube を再起動するとそのラベルが消えてしまっていて保持されないようだった。ちょっと調べてローカルの環境を作るのが面倒になったので早々に断念した。</description><content>&lt;p>4時に寝て8時に起きた。久しぶりに寝坊した。&lt;/p>
&lt;h2 id="k8s-の-nodeselector">k8s の nodeSelector&lt;/h2>
&lt;p>先日 &lt;a href="/diary/diary/posts/2022/0706/">定期/バッチ処理を k8s の cronjob にすべて移行&lt;/a> した。すでに本番運用もしていて調子もよさそうにみえる。あと残課題としてバッチ処理とアプリケーションサーバーの pod がデプロイされる k8s ノードを分割したい。現時点では、バッチ処理の負荷は小さいから同じスペックのインスタンスの k8s ノード上で混在させて運用している。しかし、いずれ運用上の問題になる懸念がある。そのため、バッチ処理のみを実行する k8s ノードを管理したい。次のドキュメントに書いてある。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">Assigning Pods to Nodes&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>k8s のドキュメントによると、大きく分けて nodeSelector と Affinity という2つのやり方がある。前者はラベルでフィルターするシンプルな仕組み、後者はさらに複雑な要件に対応するもの。いまのところ、ただ分割できればよいのでシンプルな nodeSelector で実装してみることにした。&lt;/p>
&lt;ul>
&lt;li>nodeSelector&lt;/li>
&lt;li>Affinity and anti-affinity&lt;/li>
&lt;/ul>
&lt;p>余談だが、nodeSelector はいずれ Affinity に置き換わるので deprecated だと一時期ドキュメントに書かれていたらしい。具体的に決まっていることでもないため、&lt;a href="https://github.com/kubernetes/kubernetes/issues/82184">nodeSelector: when will it be deprecated? #82184&lt;/a> によると deprecated という文言を含む文章がその後に削除された。Affinity は高機能且つ高コストであることから、(現時点では) nodeSelector はシンプルで推奨すべき方法とまで書いてあるのですぐになくなるわけではなさそう。&lt;/p>
&lt;p>minikube で nodeSelector の検証を始めたんだけど、いくつかうまくいかないことがあって断念した。multi-node 機能を使って controll plane と worker ノードの2つを起動できたけど、worker ノードから docker host にアクセスできなかった。何かしら設定が必要なのか、別途レジストリが必要になるのかよくわからなかった。あと node にラベル設定したときに worker ノードにラベル設定しても minikube を再起動するとそのラベルが消えてしまっていて保持されないようだった。ちょっと調べてローカルの環境を作るのが面倒になったので早々に断念した。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Using Multi-Node Clusters&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>lambda から cronjob へ</title><link>/diary/posts/2022/0706/</link><pubDate>Wed, 06 Jul 2022 19:27:28 +0900</pubDate><guid>/diary/posts/2022/0706/</guid><description>0時に6時に起きた。今日は晴れたので自転車通勤。
優雅にドキュメントを書きながら障害対応 昨日は凸凹しながら乗り切ってサービスイン2日目。今日から外部システムとの連携なども絡んでくる。昨日の今日なんで何か起こるだろうと思いつつ、暇だったらドキュメント書くタスクがいくつも溜まっているのでそれを片付けるかと業務を始めた。私ぐらいの人間になると、いつ凸凹が発生してもよいように、この日のために取っておいたようなドキュメントタスクがいくつも溜まっている。午前中に私の出番はなく、優雅にドキュメントの1つを完成させた。
以前から定期実行やバッチ処理は lambda 関数をトリガーに作られていた。それらを serverless framework から cdk へ移行した んだけど、その後にバッチ処理を k8s の cronjob で実装した 。この cronjob が思いの外、うまくいって、私がフルスクラッチで cli を作っているのだから、私からみてさいきょうのばっちしょりの土台を実装している。定期実行もすべて cronjob でやればいいやんと気付いて、過去に lambda 関数 (cdk/python) で実装したものをすべて移行することに決めた。昨日の流れからわかるように過去に作成済みの一連の lambda 関数はまだ本番環境にデプロイされていない。m1 chip macbook 問題 で同僚のマシンからデプロイできないという不運もあったんだけど、もうデプロイしなくていいよ、すべて cronjob で置き換えるからと伝えて2つ移行した。あと半日もあれば完了できそうな見通し。
lambda 関数から cronjob への移行作業をしていると、本番環境でのバッチ処理の一部のロジックが誤っているとわかってそれを修正したり、当然のようにテスト環境のデータも誤っているので正しいかどうかは本番環境にデプロイするしかないみたいな凸凹した状況を横切りながら本日の作業を終えた。いくつか残課題は残っているものの、明日中には平常業務に戻れるぐらいの状況にはなりつつあるのかもしれない。サービスイン2日目を終えて致命的な問題は起こっていないようにみえる。ひとまずはよかったという感じ。</description><content>&lt;p>0時に6時に起きた。今日は晴れたので自転車通勤。&lt;/p>
&lt;h2 id="優雅にドキュメントを書きながら障害対応">優雅にドキュメントを書きながら障害対応&lt;/h2>
&lt;p>昨日は凸凹しながら乗り切ってサービスイン2日目。今日から外部システムとの連携なども絡んでくる。昨日の今日なんで何か起こるだろうと思いつつ、暇だったらドキュメント書くタスクがいくつも溜まっているのでそれを片付けるかと業務を始めた。私ぐらいの人間になると、いつ凸凹が発生してもよいように、この日のために取っておいたようなドキュメントタスクがいくつも溜まっている。午前中に私の出番はなく、優雅にドキュメントの1つを完成させた。&lt;/p>
&lt;p>以前から定期実行やバッチ処理は lambda 関数をトリガーに作られていた。それらを &lt;a href="/diary/diary/posts/2022/0609/#serverless-framework-から-cdk-移行の背景">serverless framework から cdk へ移行した&lt;/a> んだけど、その後にバッチ処理を &lt;a href="/diary/diary/posts/2022/0622/">k8s の cronjob で実装した&lt;/a> 。この cronjob が思いの外、うまくいって、私がフルスクラッチで cli を作っているのだから、私からみてさいきょうのばっちしょりの土台を実装している。定期実行もすべて cronjob でやればいいやんと気付いて、過去に lambda 関数 (cdk/python) で実装したものをすべて移行することに決めた。昨日の流れからわかるように過去に作成済みの一連の lambda 関数はまだ本番環境にデプロイされていない。&lt;a href="/diary/diary/posts/2022/0704/">m1 chip macbook 問題&lt;/a> で同僚のマシンからデプロイできないという不運もあったんだけど、もうデプロイしなくていいよ、すべて cronjob で置き換えるからと伝えて2つ移行した。あと半日もあれば完了できそうな見通し。&lt;/p>
&lt;p>lambda 関数から cronjob への移行作業をしていると、本番環境でのバッチ処理の一部のロジックが誤っているとわかってそれを修正したり、当然のようにテスト環境のデータも誤っているので正しいかどうかは本番環境にデプロイするしかないみたいな凸凹した状況を横切りながら本日の作業を終えた。いくつか残課題は残っているものの、明日中には平常業務に戻れるぐらいの状況にはなりつつあるのかもしれない。サービスイン2日目を終えて致命的な問題は起こっていないようにみえる。ひとまずはよかったという感じ。&lt;/p></content></item><item><title>cdk で既存の eks クラスターを管理すべきか</title><link>/diary/posts/2022/0629/</link><pubDate>Wed, 29 Jun 2022 08:19:59 +0900</pubDate><guid>/diary/posts/2022/0629/</guid><description>0時に寝て6時に起きた。
cdk から既存の eks クラスターを制御する 1ヶ月ほど前に検証していた cdk による eks クラスターの helm 管理 を再検証した。kubectlRoleArn にどういった権限をもつ iam role を設定したらよいかがよくわからなくて苦労していた。最終的にそれが理解できて helm 管理もできるようになったのでまとめておく。
kubectlRoleArn - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually.</description><content>&lt;p>0時に寝て6時に起きた。&lt;/p>
&lt;h2 id="cdk-から既存の-eks-クラスターを制御する">cdk から既存の eks クラスターを制御する&lt;/h2>
&lt;p>1ヶ月ほど前に検証していた &lt;a href="/diary/diary/posts/2022/0518/#cdk-のパッチ検証">cdk による eks クラスターの helm 管理&lt;/a> を再検証した。kubectlRoleArn にどういった権限をもつ iam role を設定したらよいかがよくわからなくて苦労していた。最終的にそれが理解できて helm 管理もできるようになったのでまとめておく。&lt;/p>
&lt;blockquote>
&lt;p>kubectlRoleArn - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually. The trust policy of this role should include the the arn:aws::iam::${accountId}:root principal in order to allow the execution role of the kubectl resource to assume it.&lt;/p>
&lt;p>&lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters">https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>aws-auth の configmap に設定されている system:masters に所属している iam role を調べる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl describe configmap -n kube-system aws-auth
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>この iam role には &lt;code>sts:AssumeRole&lt;/code> 権限を与え、trust relationships に &lt;code>arn:aws:iam::${accountId}:root&lt;/code> といった root ユーザーを含める必要がある。この root ユーザーの設定がないと次のような権限エラーが発生する。この権限エラーの修正方法がわからなくて苦労していた。結果的には関係なかった kubectlLambdaRole の設定も必要なんじゃないかと検証していたのが前回の作業の中心だった。&lt;/p>
&lt;pre tabindex="0">&lt;code>An error occurred (AccessDenied) when calling the AssumeRole operation:
User: arn:aws:sts::${accountId}:assumed-role/xxx is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::${accountId}:role/myrole
Error: Kubernetes cluster unreachable: Get &amp;#34;https://xxx.gr7.ap-northeast-1.eks.amazonaws.com/version
&lt;/code>&lt;/pre>&lt;p>ようやく cdk で既存の eks クラスターをインポートして helm パッケージを管理できるようになった。とはいえ、cdk/cf の実行時間を測ってみると次のようになった。&lt;/p>
&lt;ul>
&lt;li>helm パッケージの新規インストール: 約5分&lt;/li>
&lt;li>helm パッケージのアンインストール: 約25分&lt;/li>
&lt;/ul>
&lt;p>これは cdk が helm パッケージを管理するための lambda 環境を構築/削除するときの時間になる。cdk はアプリケーションの stack から nested stack を作成して、そこに lambda や iam role などをまとめて作成する。一度作成してしまえば、バージョンのアップグレードは30秒ほどで完了した。&lt;/p>
&lt;p>この振る舞いを検証した上で、cdk で eks クラスターをインポートする管理はやめようとチームに提案した。正しい設定を作ってしまえば運用は楽になると言える一面もあるが、新規に helm パッケージを追加するときのちょっとした typo や設定ミスなどがあると、1回の試行に30分かかる。私がこの検証に1週間以上のデバッグ時間を割いている理由がそれに相当する。お手伝い先の運用ではテスト/本番環境ともにローカルから接続できる状態なので helm コマンドを直接実行した方が遥かに管理コストや保守コストを下げると言える。cdk を使って嬉しいことは helm コマンドでわかるバージョン情報と設定内容が cdk のコードとして管理されているぐらいでしかない。ドキュメントと helm コマンドで管理する方が現状ではよいだろうと私は結論付けた。同じような理由で eks クラスターも cdk ではなく eksctl コマンドで管理されている。&lt;/p>
&lt;p>1週間以上の労力と時間を費やしてやらない方がよいとわかったという、一般的には失敗と呼ばれる作業に終わったわけだけど、eks/cdk の勉強にはなった。&lt;/p></content></item><item><title>k8s のアップグレードをやってみた</title><link>/diary/posts/2022/0625/</link><pubDate>Sat, 25 Jun 2022 16:34:05 +0900</pubDate><guid>/diary/posts/2022/0625/</guid><description>0時に寝て6時半に起きた。起きてから1時間ほどだらだらしてた。
ストレッチ 今日の開脚幅は開始前161cmで、ストレッチ後163cmだった。先週と同じなので現状維持とも言えるし、よい状態を維持しているとも言えるかもしれない。もう1年以上通っているせいか、なにかポイントが溜まっていて使わないといけないという話しで今日は20分延長でやってくれた。とは言っても、基本的なストレッチ項目が変わるわけではなく、いつもより伸ばす時間や手順が少し増えているぐらいだった気がする。今週はとくに腰の負荷もあまり感じなかったせいか、いつもの右腰の張りもなかったように思う。トレーナーさんに聞くと、暑くなると筋肉は伸びやすくなるので季節要因でストレッチをしたときの伸び具合が変わるのは普通とのこと。調子がよくなってきたのでこのまま好調を維持したい。
eks (k8s) のアップグレード お手伝い先のお仕事がもうすぐサービスインなのでそれまでにリスクのある作業をやっとこうみたいな状況にある。たまたま eks (k8s) のバージョンを 1.21 から 1.22 にあげようと思い立って、木曜日に提案したら、どんな障害が起きるかわからないので他メンバーがテスト環境を使っていない時間帯で作業した方がよいだろうという話になって土日にやることにした。
Updating an Amazon EKS cluster Kubernetes version 何が起きるか分からなくても、土曜日から始めて致命的なトラブルに見舞われても1日もあれば解決できるだろうという見通しで作業を始めた。その見通しも「私がやれば」という前提に成り立っている。良くも悪くも私がやろうと言ったことに反対されることはほとんどないが、それは私が言ったことは一定時間に私がすべてやり切るという信頼に基づいている。本当の意味でできるかどうか分からないことを必要以上に抱え込んでしまうときもあるのでバランス感覚は必要かもしれない。言わばサービス休日出勤だし、なぜ私がやっているかと言うと、システムの運用や保守の展望を考えたら、サービスインの前にインフラのバージョンを上げておく方が将来の保守コストを下げることに繋がるという1点のみに重要性を見い出していて、それをもっとも強く主張しているのが私だからという理由。
結論から言って2時間でアップグレード作業を完了した。1つ手順漏れがあって、アプリケーションの pod がすべてエラーになるというトラブルに見舞われたものの、すぐ手順漏れに気付いて難なく復旧できた。今日はテスト環境のアップグレードをしたわけだけど、また後日、本番向けの作業手順書を作れば、ほぼタウンタイムなしで1時間もあればアップグレード作業を完了できそうな見通しではある。
実際はミスもあったので次の順番でやったわけではないが、おそらくこの手順でやれば正しいはず。
aws cli と eksctl コマンドのインストール aws のアップグレードドキュメンを読む cert-manager のアップグレード (1.1.1 から 1.5.4) aws-load-balancer-controller のアップグレード (2.2.0 から 2.4.2) k8s control plane のアップグレード (1.21 から 1.22) (オプション: 不要) autoscaler のアップグレード (オプション: 不要) gpu サポートノードのアップグレード vpc cni プラグインのアップグレード (1.7.5 から 1.11.2) coredns プラグインのアップグレード (1.8.4 から 1.8.7) kube-proxy のアップグレード (1.</description><content>&lt;p>0時に寝て6時半に起きた。起きてから1時間ほどだらだらしてた。&lt;/p>
&lt;h2 id="ストレッチ">ストレッチ&lt;/h2>
&lt;p>今日の開脚幅は開始前161cmで、ストレッチ後163cmだった。先週と同じなので現状維持とも言えるし、よい状態を維持しているとも言えるかもしれない。もう1年以上通っているせいか、なにかポイントが溜まっていて使わないといけないという話しで今日は20分延長でやってくれた。とは言っても、基本的なストレッチ項目が変わるわけではなく、いつもより伸ばす時間や手順が少し増えているぐらいだった気がする。今週はとくに腰の負荷もあまり感じなかったせいか、いつもの右腰の張りもなかったように思う。トレーナーさんに聞くと、暑くなると筋肉は伸びやすくなるので季節要因でストレッチをしたときの伸び具合が変わるのは普通とのこと。調子がよくなってきたのでこのまま好調を維持したい。&lt;/p>
&lt;h2 id="eks-k8s-のアップグレード">eks (k8s) のアップグレード&lt;/h2>
&lt;p>お手伝い先のお仕事がもうすぐサービスインなのでそれまでにリスクのある作業をやっとこうみたいな状況にある。たまたま eks (k8s) のバージョンを 1.21 から 1.22 にあげようと思い立って、木曜日に提案したら、どんな障害が起きるかわからないので他メンバーがテスト環境を使っていない時間帯で作業した方がよいだろうという話になって土日にやることにした。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html">Updating an Amazon EKS cluster Kubernetes version&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>何が起きるか分からなくても、土曜日から始めて致命的なトラブルに見舞われても1日もあれば解決できるだろうという見通しで作業を始めた。その見通しも「私がやれば」という前提に成り立っている。良くも悪くも私がやろうと言ったことに反対されることはほとんどないが、それは私が言ったことは一定時間に私がすべてやり切るという信頼に基づいている。本当の意味でできるかどうか分からないことを必要以上に抱え込んでしまうときもあるのでバランス感覚は必要かもしれない。言わばサービス休日出勤だし、なぜ私がやっているかと言うと、システムの運用や保守の展望を考えたら、サービスインの前にインフラのバージョンを上げておく方が将来の保守コストを下げることに繋がるという1点のみに重要性を見い出していて、それをもっとも強く主張しているのが私だからという理由。&lt;/p>
&lt;p>結論から言って2時間でアップグレード作業を完了した。1つ手順漏れがあって、アプリケーションの pod がすべてエラーになるというトラブルに見舞われたものの、すぐ手順漏れに気付いて難なく復旧できた。今日はテスト環境のアップグレードをしたわけだけど、また後日、本番向けの作業手順書を作れば、ほぼタウンタイムなしで1時間もあればアップグレード作業を完了できそうな見通しではある。&lt;/p>
&lt;p>実際はミスもあったので次の順番でやったわけではないが、おそらくこの手順でやれば正しいはず。&lt;/p>
&lt;ol>
&lt;li>aws cli と eksctl コマンドのインストール&lt;/li>
&lt;li>aws のアップグレードドキュメンを読む&lt;/li>
&lt;li>cert-manager のアップグレード (1.1.1 から 1.5.4)&lt;/li>
&lt;li>aws-load-balancer-controller のアップグレード (2.2.0 から 2.4.2)&lt;/li>
&lt;li>k8s control plane のアップグレード (1.21 から 1.22)&lt;/li>
&lt;li>(オプション: 不要) autoscaler のアップグレード&lt;/li>
&lt;li>(オプション: 不要) gpu サポートノードのアップグレード&lt;/li>
&lt;li>vpc cni プラグインのアップグレード (1.7.5 から 1.11.2)&lt;/li>
&lt;li>coredns プラグインのアップグレード (1.8.4 から 1.8.7)&lt;/li>
&lt;li>kube-proxy のアップグレード (1.21.2 から 1.22.6)&lt;/li>
&lt;li>k8s nodegroup のアップグレード (1.21 から 1.22)
&lt;ul>
&lt;li>k8s ノードが存在する nodegroup をアップグレードするとそのインスタンスが再作成されて pod が再デプロイされる&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>細かい手順は aws のドキュメントの指示に従いながらやったらできた。add-on と self-managed add-on の種別の違いがあったり、helm と k8s manifest の手順が別々だったり、どのバージョンからのアップグレードかで作業手順が異なったりと、ドキュメントをちゃんと読まないと正しい作業手順がわからない。基本的にはドキュメント通りの作業で完了できた。&lt;/p>
&lt;h2 id="もくもく会">もくもく会&lt;/h2>
&lt;p>アップグレード作業を終えてから1時間ほど残っていたので16時から &lt;a href="https://kobe-sannomiya-dev.connpass.com/event/251117/">【三宮.dev ＆ KELab 共催】もくもく会&lt;/a> に参加した。今回は &lt;a href="https://kobe-engr-lab.studio.site/">Kobe Engineers Lab&lt;/a> さんと共催ということで &lt;a href="https://120workplace.jp/">120 WORKPLACE KOBE&lt;/a> で開催された。Kobe Engineers Lab の主催者の会社が 120 workplace でオフィスを借りているため、会議室を5時間/月まで無料で借りられるという。私も過去に何度か 120 workplace のコワーキングスペースで作業したこともあった。久しぶりに行ってよい場所だとは思う。会議室は初めて入ったけど、10人ぐらいは余裕で作業できる大きなテーブルがあって広くてよかった。終わってからわたなべさんと3時間ほど立ち呑みしてた。&lt;/p>
&lt;h2 id="はんなりビジネス">はんなりビジネス&lt;/h2>
&lt;p>21時から &lt;a href="https://hannari-python.connpass.com/event/250916/">はんなりビジネス #0&lt;/a> に参加した。おがわさんがまた新しいことやるんだなと思って興味本位で参加してみた。現実の課題に対してコミュニティの有志を募ってチームで取り組んでみたら、問題解決能力も身についてプログラミングの知識を活かしてより実践的なスキルが身に着いてよいのではないかといったところから始まった企画らしい。今日は初回だったので参加者でどういう取り組みがよいのかを雑談してた。まだまだこの先どうなるかわからないけど、私はあまりこの手の取り組みには懐疑的かなぁ。自分たちにとってちょうどよい課題レベルの対象をみつけるのは難しいし、誰でも参加できるオープンなビジネスコンテストやアイディアソンが本当に大事な問題を扱っているかも怪しい。現実の課題はお仕事でいくらでもあるので、それをコミュニティでやろうと思うとニッチな何かになるか、価値があるかどうかよりも本人がやりたいかどうかの目的になってしまうような気もする。とはいえ、私自身、ビジネス力はまったくないのでなにかしらやっているうちに価値に気付くこともあるかもしれない。もうしばらく様子をみてみる。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://chomoku.notion.site/6872dcf2ba8d4f6887cb16f71879f4ca">https://chomoku.notion.site/6872dcf2ba8d4f6887cb16f71879f4ca&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>k8s の cronjob を検証中</title><link>/diary/posts/2022/0622/</link><pubDate>Wed, 22 Jun 2022 18:58:20 +0900</pubDate><guid>/diary/posts/2022/0622/</guid><description>0時に寝て6時に起きた。寝不足を解消して体調が戻ってきた。
k8s の cronjob バッチ処理を Kubernetes: CronJob で作る。一通り設定して minikube で検証して eks 上でも動くようになった。
apiVersion: batch/v1 kind: CronJob metadata: name: my-app-hourly-job spec: schedule: &amp;#34;5 */1 * * *&amp;#34; concurrencyPolicy: Forbid startingDeadlineSeconds: 600 jobTemplate: spec: backoffLimit: 0 template: metadata: labels: app: my-app-hourly annotations: dapr.io/enabled: &amp;#34;true&amp;#34; dapr.io/app-id: &amp;#34;my-app-hourly&amp;#34; spec: containers: - name: my-app-hourly-job image: my-app-image imagePullPolicy: Always env: - name: BATCH_ENV value: &amp;#34;dev&amp;#34; command: - &amp;#34;/bin/sh&amp;#34; - &amp;#34;/app/scripts/my-app.sh&amp;#34; - &amp;#34;param1&amp;#34; - &amp;#34;param2&amp;#34; restartPolicy: Never command の設定がわかりにくい。さらに k8s のドキュメントのサンプル設定も誤解を招くような例になっている。どうも実行できるのは1つの cli だけで、複数コマンドを指定できるわけではない。シェルスクリプトを docker イメージに含めて、そこで任意のスクリプトを実装した方がよいだろう。</description><content>&lt;p>0時に寝て6時に起きた。寝不足を解消して体調が戻ってきた。&lt;/p>
&lt;h2 id="k8s-の-cronjob">k8s の cronjob&lt;/h2>
&lt;p>バッチ処理を &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">Kubernetes: CronJob&lt;/a> で作る。一通り設定して minikube で検証して eks 上でも動くようになった。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yml" data-lang="yml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">batch/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">CronJob&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">my-app-hourly-job&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">schedule&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;5 */1 * * *&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">concurrencyPolicy&lt;/span>: &lt;span style="color:#ae81ff">Forbid&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">startingDeadlineSeconds&lt;/span>: &lt;span style="color:#ae81ff">600&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">jobTemplate&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">backoffLimit&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">template&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">my-app-hourly&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">annotations&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dapr.io/enabled&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dapr.io/app-id&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;my-app-hourly&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">my-app-hourly-job&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">my-app-image&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">imagePullPolicy&lt;/span>: &lt;span style="color:#ae81ff">Always&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">env&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">BATCH_ENV&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">value&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;dev&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">command&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;/bin/sh&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;/app/scripts/my-app.sh&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;param1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;param2&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">restartPolicy&lt;/span>: &lt;span style="color:#ae81ff">Never&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>command の設定がわかりにくい。さらに k8s のドキュメントのサンプル設定も誤解を招くような例になっている。どうも実行できるのは1つの cli だけで、複数コマンドを指定できるわけではない。シェルスクリプトを docker イメージに含めて、そこで任意のスクリプトを実装した方がよいだろう。&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;/bin/sh&amp;rdquo;&lt;/li>
&lt;li>&amp;ldquo;/app/scripts/my-app.sh&amp;rdquo;&lt;/li>
&lt;li>&amp;ldquo;param1&amp;rdquo;&lt;/li>
&lt;li>&amp;ldquo;param2&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>この設定は次の cli として実行される。&lt;/p>
&lt;blockquote>
&lt;p>/bin/sh /app/scripts/my-app.sh param1 param2&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://stackoverflow.com/questions/51657105/how-to-ensure-kubernetes-cronjob-does-not-restart-on-failure">How to ensure kubernetes cronjob does not restart on failure&lt;/a> によると、バッチ処理が失敗したときに再実行したくないときは次の3つの設定をする。&lt;/p>
&lt;ul>
&lt;li>concurrencyPolicy: Forbid&lt;/li>
&lt;li>backoffLimit: 0&lt;/li>
&lt;li>restartPolicy: Never&lt;/li>
&lt;/ul>
&lt;p>restartPolicy が Never 以外だと、エラーが発生すると永遠に再実行されてしまうので障害時に2次被害を増やしてしまう懸念があったような気がする。&lt;/p>
&lt;p>あと、うちの環境は dapr 経由で他の pod サービスと通信しているので dapr を有効にしないと pod 間通信ができない。dapr はデーモンでずっと起動しているからバッチ処理の終了時に daprd も shutdown してやらないといけない。&lt;a href="https://docs.dapr.io/operations/hosting/kubernetes/kubernetes-job/">Running Dapr with a Kubernetes Job&lt;/a> にその方法が書いてある。daprd を shutdown しないと、pod のステータスが NotReady のままで Completed にならない。&lt;/p>
&lt;p>まだまだよくわかってないので &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Jobs&lt;/a> のドキュメントに一通り目を通そうと思っている。&lt;/p></content></item><item><title>法人として消費税を納めた</title><link>/diary/posts/2022/0524/</link><pubDate>Tue, 24 May 2022 01:14:51 +0900</pubDate><guid>/diary/posts/2022/0524/</guid><description>5時に寝て7時過ぎに起きた。前日の夜から法人決算の電子申告に取り組み始めた。本当は紙でやるつもりだったんだけど、eltax が快適だったので e-tax も衝動的にやってみたくなった。
消費税と地方消費税の申告 法人決算 の一部。今回が初めての消費税と地方消費税の申告になる。簡易課税で支払う。
消費税は、国税（国に納付する税金）であり消費税の納税義務がある事業者が納付します。地方消費税とは、 消費税と同様で商品の販売やサービスの提供などの取引にかかる税金 です。消費税との違いは、 地方消費税は国税ではなく地方税（都道府県や市町村に納付する税金）という点です。 しかし実際に納付するときは消費税と分けて納付はせずに、 消費税と一緒に地方消費税を所管税務署へ納付します。
消費税と地方消費税の違いは？納付対象者や納付方法、計算の仕方まで徹底解説！
freee で出力した書類をみながら e-tax の画面で同じ書類の項目を埋めていくだけの作業。1つだけバリデーションエラーが発生して、何度やり直しても数値は正しいようにみえるので無視して処理を継続することにした。メッセージにも値が正しければ継続してくださいと書いてあるのでバリデーションがバグっているのだろうと推測する。書類を作成して、署名して、送信して、納付情報が返ってきて、pay-easy で納付額を振り込む。1時間ほどで完了できた。
eks (k8s) から alb の管理 eks (k8s) に aws-load-balancer-controller をインストールすると k8s 上のリソースとして alb を管理できるようになる。
AWS Load Balancer Controller アドオンのインストール 具体的には k8s の Ingress と Nodepoint リソースから次の3つのリソースを生成してくれる。
application load balancer http listener target groups alb からのヘルスチェックは次のようにエンドポイントを記述する。spring boot だと Actuator という web api がヘルスチェックの機能を提供している。
alb.ingress.kubernetes.io/healthcheck-path: /actuator/health alb.ingress.kubernetes.io/scheme の設定で alb を配置するサブネットを指定できる。デフォルトは internal になる。
private subnet に配置するとき</description><content>&lt;p>5時に寝て7時過ぎに起きた。前日の夜から法人決算の電子申告に取り組み始めた。本当は紙でやるつもりだったんだけど、eltax が快適だったので e-tax も衝動的にやってみたくなった。&lt;/p>
&lt;h2 id="消費税と地方消費税の申告">消費税と地方消費税の申告&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2022/0505/#法人決算">法人決算&lt;/a> の一部。今回が初めての消費税と地方消費税の申告になる。簡易課税で支払う。&lt;/p>
&lt;blockquote>
&lt;p>消費税は、国税（国に納付する税金）であり消費税の納税義務がある事業者が納付します。地方消費税とは、 &lt;strong>消費税と同様で商品の販売やサービスの提供などの取引にかかる税金&lt;/strong> です。消費税との違いは、 &lt;strong>地方消費税は国税ではなく地方税（都道府県や市町村に納付する税金）という点です。&lt;/strong> しかし実際に納付するときは消費税と分けて納付はせずに、 &lt;strong>消費税と一緒に地方消費税を所管税務署へ納付します。&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://biz.moneyforward.com/tax_return/basic/70/#i">消費税と地方消費税の違いは？納付対象者や納付方法、計算の仕方まで徹底解説！&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>freee で出力した書類をみながら e-tax の画面で同じ書類の項目を埋めていくだけの作業。1つだけバリデーションエラーが発生して、何度やり直しても数値は正しいようにみえるので無視して処理を継続することにした。メッセージにも値が正しければ継続してくださいと書いてあるのでバリデーションがバグっているのだろうと推測する。書類を作成して、署名して、送信して、納付情報が返ってきて、pay-easy で納付額を振り込む。1時間ほどで完了できた。&lt;/p>
&lt;h2 id="eks-k8s-から-alb-の管理">eks (k8s) から alb の管理&lt;/h2>
&lt;p>eks (k8s) に aws-load-balancer-controller をインストールすると k8s 上のリソースとして alb を管理できるようになる。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/aws-load-balancer-controller.html">AWS Load Balancer Controller アドオンのインストール&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>具体的には k8s の Ingress と Nodepoint リソースから次の3つのリソースを生成してくれる。&lt;/p>
&lt;ul>
&lt;li>application load balancer&lt;/li>
&lt;li>http listener&lt;/li>
&lt;li>target groups&lt;/li>
&lt;/ul>
&lt;p>alb からのヘルスチェックは次のようにエンドポイントを記述する。spring boot だと &lt;a href="https://docs.spring.io/spring-boot/docs/current/actuator-api/htmlsingle/">Actuator&lt;/a> という web api がヘルスチェックの機能を提供している。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">alb.ingress.kubernetes.io/healthcheck-path&lt;/span>: &lt;span style="color:#ae81ff">/actuator/health&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>alb.ingress.kubernetes.io/scheme&lt;/code> の設定で alb を配置するサブネットを指定できる。デフォルトは &lt;code>internal&lt;/code> になる。&lt;/p>
&lt;p>private subnet に配置するとき&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">alb.ingress.kubernetes.io/scheme&lt;/span>: &lt;span style="color:#ae81ff">internal&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>public subnet に配置するとき&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">alb.ingress.kubernetes.io/scheme&lt;/span>: &lt;span style="color:#ae81ff">internet-facing&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>helm を調べた</title><link>/diary/posts/2022/0511/</link><pubDate>Wed, 11 May 2022 08:31:36 +0900</pubDate><guid>/diary/posts/2022/0511/</guid><description>1時に寝て6時半に起きて8時に起きた。前日は資料づくりで遅くまでオフィスに残っていたせいか、なんか寝坊した。
helm 調査 k8s 上の datadog-agent が helm で管理されていて、あるバージョンから dapr も helm 管理できる ようになった。dapr は cli からもインストールできるけど、helm のことをよくわかってなかったので調べることにした。そんなたくさん記事をみたわけではないけど、いくつか記事を読んで quora のやり取りが一番よかった。
When should you use Kubernetes Helm and not use it? ざっくりまとめるとこうかな。
helm は oss 且つ cncf の公式プロジェクトだからまぁ安心 helm はサードパーティのパッケージのインストールや設定の利便性を高める k8s はテンプレート機能が弱いので共通設定と特定環境向けの設定を管理するのがあまり得意ではない セキュリティを考慮した k8s 設定は自分でやるよりコミュニティに任せた方がよい場合もある パッケージなのでバージョン管理は得意 helm は k8s 向けのパッケージマネージャとレポジトリマネージャーとマーケットプレイスを組み合わせたみたいなもの k8s 上でサードパーティのパッケージを自分で設定したい特別な理由がない限りは helm を使うのがよさそうという結論になった。</description><content>&lt;p>1時に寝て6時半に起きて8時に起きた。前日は資料づくりで遅くまでオフィスに残っていたせいか、なんか寝坊した。&lt;/p>
&lt;h2 id="helm-調査">helm 調査&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2022/0201/#kubernetes-のログ管理と-datadog-agent-のログ連携不具合">k8s 上の datadog-agent&lt;/a> が &lt;a href="https://helm.sh/">helm&lt;/a> で管理されていて、あるバージョンから &lt;a href="https://docs.dapr.io/operations/hosting/kubernetes/kubernetes-deploy/#install-with-helm-advanced">dapr も helm 管理できる&lt;/a> ようになった。dapr は cli からもインストールできるけど、helm のことをよくわかってなかったので調べることにした。そんなたくさん記事をみたわけではないけど、いくつか記事を読んで quora のやり取りが一番よかった。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.quora.com/When-should-you-use-Kubernetes-Helm-and-not-use-it">When should you use Kubernetes Helm and not use it?&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>ざっくりまとめるとこうかな。&lt;/p>
&lt;ul>
&lt;li>helm は oss 且つ cncf の公式プロジェクトだからまぁ安心&lt;/li>
&lt;li>helm はサードパーティのパッケージのインストールや設定の利便性を高める
&lt;ul>
&lt;li>k8s はテンプレート機能が弱いので共通設定と特定環境向けの設定を管理するのがあまり得意ではない&lt;/li>
&lt;li>セキュリティを考慮した k8s 設定は自分でやるよりコミュニティに任せた方がよい場合もある&lt;/li>
&lt;li>パッケージなのでバージョン管理は得意&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>helm は k8s 向けのパッケージマネージャとレポジトリマネージャーとマーケットプレイスを組み合わせたみたいなもの&lt;/li>
&lt;/ul>
&lt;p>k8s 上でサードパーティのパッケージを自分で設定したい特別な理由がない限りは helm を使うのがよさそうという結論になった。&lt;/p></content></item><item><title>spring boot の環境とログ設定</title><link>/diary/posts/2022/0324/</link><pubDate>Thu, 24 Mar 2022 07:54:35 +0900</pubDate><guid>/diary/posts/2022/0324/</guid><description>0時に寝て4時に起きて6時に起きた。
spring のプロファイル設定 spring の Profiles の仕組みを使って環境ごとの設定を作る。デプロイは k8s で管理しているため、spring boot の Externalized Configuration の仕組みを使って、環境変数から application.yml に定義された設定を書き換える。k8s は kustomize で管理していて prod, test, dev の3つの環境で任意の設定を記述できる。
問題はログ出力の設定を環境ごとに変えたい。具体的には datadog に連携されるログは構造化ログ (json lines) を、ローカルの開発ではコンソールログをみたい。Log4j Spring Boot Support によると、1つの設定ファイルに複数のプロファイル設定を記述できるようにもみえるけど、実際にやってみたらうまく動かなかった。xml ではなく yml を使っているせいかもしれないし、私の記述方法が誤っているのかもしれない。いずれにしても yml で複数のプロファイルを設定しているサンプルをみつけられなかった。
そこで Different Log4j2 Configurations per Spring Profile をみて、環境ごとにログ設定ファイルも分割することにした。application.yml には次のように記述する。
spring: profiles: active: dev logging: config: classpath:log4j2-${spring.profiles.active}.yml ローカル開発向けの lgo4j2-dev.yml は次のようになる。
Configuration: status: warn name: YAMLConfig appenders: Console: name: STDOUT target: SYSTEM_OUT PatternLayout: Pattern: &amp;#34;%d{yyyy-MM-dd HH:mm:ss.</description><content>&lt;p>0時に寝て4時に起きて6時に起きた。&lt;/p>
&lt;h2 id="spring-のプロファイル設定">spring のプロファイル設定&lt;/h2>
&lt;p>spring の &lt;a href="https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.profiles">Profiles&lt;/a> の仕組みを使って環境ごとの設定を作る。デプロイは k8s で管理しているため、spring boot の &lt;a href="https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.external-config">Externalized Configuration&lt;/a> の仕組みを使って、環境変数から application.yml に定義された設定を書き換える。k8s は kustomize で管理していて prod, test, dev の3つの環境で任意の設定を記述できる。&lt;/p>
&lt;p>問題はログ出力の設定を環境ごとに変えたい。具体的には datadog に連携されるログは構造化ログ (json lines) を、ローカルの開発ではコンソールログをみたい。&lt;a href="https://logging.apache.org/log4j/2.x/log4j-spring-boot/index.html">Log4j Spring Boot Support&lt;/a> によると、1つの設定ファイルに複数のプロファイル設定を記述できるようにもみえるけど、実際にやってみたらうまく動かなかった。xml ではなく yml を使っているせいかもしれないし、私の記述方法が誤っているのかもしれない。いずれにしても yml で複数のプロファイルを設定しているサンプルをみつけられなかった。&lt;/p>
&lt;p>そこで &lt;a href="https://www.baeldung.com/spring-log4j2-config-per-profile">Different Log4j2 Configurations per Spring Profile&lt;/a> をみて、環境ごとにログ設定ファイルも分割することにした。application.yml には次のように記述する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yml" data-lang="yml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spring&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">profiles&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">active&lt;/span>: &lt;span style="color:#ae81ff">dev&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">logging&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">config&lt;/span>: &lt;span style="color:#ae81ff">classpath:log4j2-${spring.profiles.active}.yml&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ローカル開発向けの lgo4j2-dev.yml は次のようになる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yml" data-lang="yml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">Configuration&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">status&lt;/span>: &lt;span style="color:#ae81ff">warn&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">YAMLConfig&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">appenders&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">Console&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">STDOUT&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">target&lt;/span>: &lt;span style="color:#ae81ff">SYSTEM_OUT&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">PatternLayout&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">Pattern&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;%d{yyyy-MM-dd HH:mm:ss.SSS}[%t]%-5level %logger{36} - %msg%n&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>k8s のマニフェストで環境変数を次のように定義すれば prod というプロファイルが設定される。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yml" data-lang="yml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">my-service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">template&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">my-service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">env&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">spring.profiles.active&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">value&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;prod&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>クラウド環境向けの log4j2-prod.yml は次のようになる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yml" data-lang="yml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">Configuration&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">status&lt;/span>: &lt;span style="color:#ae81ff">warn&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">YAMLConfig&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">appenders&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">Console&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">STDOUT&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">target&lt;/span>: &lt;span style="color:#ae81ff">SYSTEM_OUT&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">EcsLayout&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">serviceName&lt;/span>: &lt;span style="color:#ae81ff">my-service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">serviceNodeName&lt;/span>: &lt;span style="color:#66d9ef">null&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">includeMarkers&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">KeyValuePair&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">type&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">value&lt;/span>: &lt;span style="color:#ae81ff">app&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>k8s のロールバック</title><link>/diary/posts/2022/0313/</link><pubDate>Sun, 13 Mar 2022 12:07:00 +0900</pubDate><guid>/diary/posts/2022/0313/</guid><description>0時に寝て7時に起きた。
k8s のロールバック Rolling Back to a Previous Revision をみながらすぐできた。ロールバックもこれまでと同様、github actions の workflow dispatch で管理できるようにした。基本的にはこれだけでロールバックできる。
$ kubectl rollout undo deployment/my-app-deploy ちょっと工夫したこととして、デプロイ時に kubernetes.io/change-cause というアノテーションに git のリビジョンもセットしておくと確認するときにちょっと楽ができる。apply した後の deployment リソースに docker イメージのタグ情報 (= git のリビジョン) を書き込んでおく。
$ kubectl apply -k ${{ env.DEPLOYMENT_ENV }} $ kubectl annotate deployment my-app-deploy kubernetes.io/change-cause=${{ env.IMAGE_TAG }} --overwrite=true kubectl から履歴をみたときに k8s のリビジョンがどの git のリビジョンを使っているかがわかりやすい。デフォルトでは何も設定されていないかもしれない。
$ kubectl rollout history deployment/my-app-deploy deployment.apps/my-app-deploy REVISION CHANGE-CAUSE 15 &amp;lt;none&amp;gt; 16 &amp;lt;none&amp;gt; 17 &amp;lt;none&amp;gt; 18 &amp;lt;none&amp;gt; 19 &amp;lt;none&amp;gt; 20 &amp;lt;none&amp;gt; 21 &amp;lt;none&amp;gt; 22 &amp;lt;none&amp;gt; 24 1f17a22a6659ea0714a21fca034645cd191e189b 27 a84e113d8b7c124178b58e2f40f57b00aae65485 28 dcf3552db0668d416ed880f6e896455d7bab194c</description><content>&lt;p>0時に寝て7時に起きた。&lt;/p>
&lt;h2 id="k8s-のロールバック">k8s のロールバック&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-to-a-previous-revision">Rolling Back to a Previous Revision&lt;/a> をみながらすぐできた。ロールバックもこれまでと同様、github actions の workflow dispatch で管理できるようにした。基本的にはこれだけでロールバックできる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl rollout undo deployment/my-app-deploy
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ちょっと工夫したこととして、デプロイ時に &lt;a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#change-cause">kubernetes.io/change-cause&lt;/a> というアノテーションに git のリビジョンもセットしておくと確認するときにちょっと楽ができる。apply した後の deployment リソースに docker イメージのタグ情報 (= git のリビジョン) を書き込んでおく。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl apply -k &lt;span style="color:#e6db74">${&lt;/span>{ env.DEPLOYMENT_ENV &lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl annotate deployment my-app-deploy kubernetes.io/change-cause&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>{ env.IMAGE_TAG &lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#f92672">}&lt;/span> --overwrite&lt;span style="color:#f92672">=&lt;/span>true
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>kubectl から履歴をみたときに k8s のリビジョンがどの git のリビジョンを使っているかがわかりやすい。デフォルトでは何も設定されていないかもしれない。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl rollout history deployment/my-app-deploy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>deployment.apps/my-app-deploy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>REVISION CHANGE-CAUSE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">15&lt;/span> &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">16&lt;/span> &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">17&lt;/span> &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">18&lt;/span> &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">19&lt;/span> &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">20&lt;/span> &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">21&lt;/span> &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">22&lt;/span> &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">24&lt;/span> 1f17a22a6659ea0714a21fca034645cd191e189b
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">27&lt;/span> a84e113d8b7c124178b58e2f40f57b00aae65485
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">28&lt;/span> dcf3552db0668d416ed880f6e896455d7bab194c
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>kustomize の動的な設定</title><link>/diary/posts/2022/0228/</link><pubDate>Mon, 28 Feb 2022 08:11:25 +0900</pubDate><guid>/diary/posts/2022/0228/</guid><description>23時に寝て2時に起きてそこから断続的に寝たり起きたりを繰り返して6時に半に起きた。ウクライナ情勢のニュースや記事ばかり読んでてあてられた。
kustomize の動的な設定 kustomize で管理している image のタグをデプロイ処理の中で動的に変更したい。パラメーター渡しなり環境変数なり、なにかしらあるだろうとは予測している。Demo: change image names and tags のサンプルによると、次のように実行すればよいみたい。
$ kustomize edit set image busybox=alpine:3.6 次のような kustomization.yaml をセットしてくれるみたい。
images: - name: busybox newName: alpine newTag: 3.6 タグのところをリビジョン指定できればいいだけなのでとくに SECRET を使う必要もない。このやり方で書き換えた newTag が POD のデプロイ対象になってくれればよいはず。</description><content>&lt;p>23時に寝て2時に起きてそこから断続的に寝たり起きたりを繰り返して6時に半に起きた。ウクライナ情勢のニュースや記事ばかり読んでてあてられた。&lt;/p>
&lt;h2 id="kustomize-の動的な設定">kustomize の動的な設定&lt;/h2>
&lt;p>kustomize で管理している image のタグをデプロイ処理の中で動的に変更したい。パラメーター渡しなり環境変数なり、なにかしらあるだろうとは予測している。&lt;a href="https://github.com/kubernetes-sigs/kustomize/blob/master/examples/image.md">Demo: change image names and tags&lt;/a> のサンプルによると、次のように実行すればよいみたい。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kustomize edit set image busybox&lt;span style="color:#f92672">=&lt;/span>alpine:3.6
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>次のような &lt;code>kustomization.yaml&lt;/code> をセットしてくれるみたい。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yml" data-lang="yml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">images&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">busybox&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">newName&lt;/span>: &lt;span style="color:#ae81ff">alpine&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">newTag&lt;/span>: &lt;span style="color:#ae81ff">3.6&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>タグのところをリビジョン指定できればいいだけなのでとくに SECRET を使う必要もない。このやり方で書き換えた &lt;code>newTag&lt;/code> が POD のデプロイ対象になってくれればよいはず。&lt;/p></content></item><item><title>wiki のドキュメント整理</title><link>/diary/posts/2022/0201/</link><pubDate>Tue, 01 Feb 2022 07:28:46 +0900</pubDate><guid>/diary/posts/2022/0201/</guid><description>23時に寝て4時半に起きた。昨日の帰りに自転車でこけて胸を強打してひたすら痛い。起き上がるのも痛い。安静にしてた。
kubernetes のログ管理と datadog-agent のログ連携不具合 先日、datadog にログ連携されていない不具合 が発生していて、その1次調査を終えたことについて書いた。緊急対応としては datadog-agent を再起動することで改善することはわかっていたので、その後、kubernetes のログ管理と datadog-agent がどうやって kubernetes クラスター上で実行されているアプリケーションのログを収集しているかを調査していた。今日は wiki に調査してわかったことなどをまとめていた。
kubernetes クラスターはコンテナランタイムに docker を使っていて、アプリケーションの stdout/stderr を docker の logging driver にリダイレクトし、JSON Lines に設定された logging driver が kubernetes ノード上にログファイルとして出力する。datadog-agent は autodiscovery 機能で pod の情報を常にポーリングしていて、pod が新たにデプロイされたらログファイルを pod 内にマウントして、そのマウントしたログファイルを読み込んでログ収集していると思われる。datadog-agent から pod の情報を取得するには kubernetes のサービスアカウントを使っていて、その credential が projected volume としてマウントされて pod 内から利用できる。その credential を使って kubelet api にリクエストすることで pod の情報を取得している。
文章で書けばたったこれだけのことなんだけど、たったこれだけのことを理解するのに次のドキュメントを読んだ。実際の調査のときはわからなかったのでもっと多くのドキュメントを読んでいる。いま書いたことを理解するならこのドキュメントを読めば理解できるはず。
https://kubernetes.io/docs/concepts/overview/components/ https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/ https://kubernetes.io/docs/concepts/cluster-administration/logging/ https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/ https://kubernetes.io/docs/concepts/storage/projected-volumes/ https://docs.datadoghq.com/agent/kubernetes/log/?tab=helm https://docs.datadoghq.com/getting_started/agent/autodiscovery/?tab=kubernetes ドキュメントに書いてあることを深く理解するために、kubernetes と datadog-agent のソースコードも読んだ。どちらも go 言語で実装されている。</description><content>&lt;p>23時に寝て4時半に起きた。昨日の帰りに自転車でこけて胸を強打してひたすら痛い。起き上がるのも痛い。安静にしてた。&lt;/p>
&lt;h2 id="kubernetes-のログ管理と-datadog-agent-のログ連携不具合">kubernetes のログ管理と datadog-agent のログ連携不具合&lt;/h2>
&lt;p>先日、&lt;a href="/diary/diary/posts/2022/0127/#ログ連携の不具合調査">datadog にログ連携されていない不具合&lt;/a> が発生していて、その1次調査を終えたことについて書いた。緊急対応としては datadog-agent を再起動することで改善することはわかっていたので、その後、kubernetes のログ管理と datadog-agent がどうやって kubernetes クラスター上で実行されているアプリケーションのログを収集しているかを調査していた。今日は wiki に調査してわかったことなどをまとめていた。&lt;/p>
&lt;p>kubernetes クラスターはコンテナランタイムに docker を使っていて、アプリケーションの stdout/stderr を docker の logging driver にリダイレクトし、JSON Lines に設定された logging driver が kubernetes ノード上にログファイルとして出力する。datadog-agent は autodiscovery 機能で pod の情報を常にポーリングしていて、pod が新たにデプロイされたらログファイルを pod 内にマウントして、そのマウントしたログファイルを読み込んでログ収集していると思われる。datadog-agent から pod の情報を取得するには kubernetes のサービスアカウントを使っていて、その credential が projected volume としてマウントされて pod 内から利用できる。その credential を使って kubelet api にリクエストすることで pod の情報を取得している。&lt;/p>
&lt;p>文章で書けばたったこれだけのことなんだけど、たったこれだけのことを理解するのに次のドキュメントを読んだ。実際の調査のときはわからなかったのでもっと多くのドキュメントを読んでいる。いま書いたことを理解するならこのドキュメントを読めば理解できるはず。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/overview/components/">https://kubernetes.io/docs/concepts/overview/components/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/">https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/">https://kubernetes.io/docs/concepts/cluster-administration/logging/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/projected-volumes/">https://kubernetes.io/docs/concepts/storage/projected-volumes/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.datadoghq.com/agent/kubernetes/log/?tab=helm">https://docs.datadoghq.com/agent/kubernetes/log/?tab=helm&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.datadoghq.com/getting_started/agent/autodiscovery/?tab=kubernetes">https://docs.datadoghq.com/getting_started/agent/autodiscovery/?tab=kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>ドキュメントに書いてあることを深く理解するために、kubernetes と datadog-agent のソースコードも読んだ。どちらも go 言語で実装されている。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes">https://github.com/kubernetes/kubernetes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/DataDog/datadog-agent">https://github.com/DataDog/datadog-agent&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;code>kubectl logs&lt;/code> の振る舞いを確認するだけでも、ソースコードからは実際のログファイルを open してストリームを返しているところはわからなかった。api 呼び出しが連携されて抽象化されていて、コンポーネントの役割分担があって、何も知らずにコードを読んでいてもわからなかった。Kubernetes の低レイヤーのところは Container Runtime Interface (CRI) という標準化を行い、1.20 から docker は非推奨となり、将来的に CRI を提供する実装に置き換わるらしい。ログファイルを open する役割は CRI の実装が担うんじゃないかと思うけど、そこまでは調べきれなかった。また機会があれば CRI の実装も読んでみる。&lt;/p>
&lt;figure>&lt;img src="/diary/diary/img/2022/0201_kubectl-logs.png"/>
&lt;/figure></content></item><item><title>ヘルスチェックのレスポンスのステータスコード</title><link>/diary/posts/2022/0131/</link><pubDate>Mon, 31 Jan 2022 07:29:38 +0900</pubDate><guid>/diary/posts/2022/0131/</guid><description>1時に寝て6時に起きた。
404 のレスポンスをヘルスチェック ここ数日はお仕事でインフラ周りの調査をしていた。たまたまログをみていて、ELB のヘルスチェックが 404 になっているのを大量にみつけた。てっきりヘルスチェックを使ってないのだろうと思ってインフラ担当者に問い合わせたら、404 が返ってくることをヘルスチェックしているという。404 をチェックすることになんの意味もなく、ただ急ぎで設定する必要があったからとりあえず動かせるためにそう設定したという。一方でアプリケーション側は spring boot で開発していて、Spring Boot Actuator も導入されているので /actuator/health にアクセスすれば 200 が返ってくるようになっている。どういう経緯だったかはわからないけど、開発者に一言聞けば 404 のレスポンスをヘルスチェックすることは何もない状態でずっと運用されていたことがわかった。
アプリケーション側の Kubernetes: Ingress のマニフェストにもそういった設定が入っていて、インフラ側の CDK のコードにもそういったマニフェストがあって、両方の設定変更が必要なのか、アプリケーション側のものだけでいいのか、少し調査が必要ということになった。私も Ingress というのがなにものなのか、よくわかってないので調べて理解する機会としよう。</description><content>&lt;p>1時に寝て6時に起きた。&lt;/p>
&lt;h2 id="404-のレスポンスをヘルスチェック">404 のレスポンスをヘルスチェック&lt;/h2>
&lt;p>ここ数日はお仕事でインフラ周りの調査をしていた。たまたまログをみていて、ELB のヘルスチェックが 404 になっているのを大量にみつけた。てっきりヘルスチェックを使ってないのだろうと思ってインフラ担当者に問い合わせたら、404 が返ってくることをヘルスチェックしているという。404 をチェックすることになんの意味もなく、ただ急ぎで設定する必要があったからとりあえず動かせるためにそう設定したという。一方でアプリケーション側は spring boot で開発していて、&lt;a href="https://www.baeldung.com/spring-boot-actuators">Spring Boot Actuator&lt;/a> も導入されているので &lt;code>/actuator/health&lt;/code> にアクセスすれば 200 が返ってくるようになっている。どういう経緯だったかはわからないけど、開発者に一言聞けば 404 のレスポンスをヘルスチェックすることは何もない状態でずっと運用されていたことがわかった。&lt;/p>
&lt;p>アプリケーション側の &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Kubernetes: Ingress&lt;/a> のマニフェストにもそういった設定が入っていて、インフラ側の CDK のコードにもそういったマニフェストがあって、両方の設定変更が必要なのか、アプリケーション側のものだけでいいのか、少し調査が必要ということになった。私も Ingress というのがなにものなのか、よくわかってないので調べて理解する機会としよう。&lt;/p></content></item><item><title>datadog-agent のログ連携の不具合調査</title><link>/diary/posts/2022/0127/</link><pubDate>Thu, 27 Jan 2022 07:47:57 +0900</pubDate><guid>/diary/posts/2022/0127/</guid><description>0時に寝て4時に起きた。朝から1時間ほどドラクエタクトやってた。
ログ連携の不具合調査 少し前に本番環境で datadog-agent からログが (クラウドの) datadog に連携されていないことがわかった。kubectl logs のコマンドで確認すると、アプリケーションのログは出力されているので datadog-agent から datadog にログを送信するところの問題であるように推測された。たまたま今日、同じような現象をテスト環境で確認できた。ちょうどスクラムのプランニングでログ調査のための作業をするチケットの承認を得たところだった。満を持して発生したような障害だったので私が調査すると明言して調査してた。半日ぐらい調査して、pod 内の credential 情報が置き換わってしまうことが原因っぽいと特定できたが、なぜ置き換わってしまうのかはまだわからない。もう少し調査して解決したら会社のテックブログにいいなと思ったので、日記に書いてた内容を移行することにした。</description><content>&lt;p>0時に寝て4時に起きた。朝から1時間ほどドラクエタクトやってた。&lt;/p>
&lt;h2 id="ログ連携の不具合調査">ログ連携の不具合調査&lt;/h2>
&lt;p>少し前に本番環境で &lt;a href="https://github.com/DataDog/datadog-agent">datadog-agent&lt;/a> からログが (クラウドの) datadog に連携されていないことがわかった。kubectl logs のコマンドで確認すると、アプリケーションのログは出力されているので datadog-agent から datadog にログを送信するところの問題であるように推測された。たまたま今日、同じような現象をテスト環境で確認できた。ちょうどスクラムのプランニングでログ調査のための作業をするチケットの承認を得たところだった。満を持して発生したような障害だったので私が調査すると明言して調査してた。半日ぐらい調査して、pod 内の credential 情報が置き換わってしまうことが原因っぽいと特定できたが、なぜ置き換わってしまうのかはまだわからない。もう少し調査して解決したら会社のテックブログにいいなと思ったので、日記に書いてた内容を移行することにした。&lt;/p></content></item><item><title>頭文字Dを読了</title><link>/diary/posts/2021/1205/</link><pubDate>Sun, 05 Dec 2021 11:47:08 +0900</pubDate><guid>/diary/posts/2021/1205/</guid><description>0時に寝て7時に起きてだらだらやってて午前中は 頭文字D のアニメをみてた。漫画 (アニメも？) はすでに完結しているのでいつか読もうと思いつつ最後まで読んでいない。ゴッドフットやゴッドアームが出てくるぐらいまでは読んだ気がする。その後どうなったのかを知らない。イニシャルDをみていると、ストーリーも絵も演出もまったく派手さはなくて普通なんだけど、なぜかおもしろくて続きをみてしまうという人間の娯楽の本質をついている気がしてくる。なんでなんだろうなぁ。
頭文字D たまたま思い出したので夜に漫画喫茶行って頭文字Dを最後まで読んできた。全48巻で、31巻ぐらいから読み始めて3-4時間ぐらいで読み終えた。漫画なので仕方ないけど、対戦相手がどんどん強くなっていって勝ち方が玄人好みというのか、単純に抜いた・抜かれたの話しではなく、タイヤマネージメントがどうこうとか、恐怖に対する心理がどうこうとか、ドライバーと車のセッティングも含めた駆け引きが強くなっていって、どちらが速いかというよりは戦略通りの展開にもっていって最後はそれがうまくはまるみたいな、これまでもずっとそうだったんだけど、ここからはよりトップレベルのほんの僅かな差が勝敗を分けるといった描き方になっていったように思う。それはそれで現実に近い気はするけど、漫画的には派手な演出にならないので玄人好みなストーリーになっていった気がする。但し、そこまでやってきて最後の対戦相手だけは、個人的には納得感がなくて、ここまで緻密に作り上げてきた理論や個々のドライバーの修練の積み重ねが圧倒的天才の前にひれ伏すみたいな切り口が急展開していて、頭の切り替えができなかった感じがした。とはいえ、最後まで読み終えられてよかったし、作品としてはすごくおもしろかった。作者はモータースポーツが本当に好きなんだろうなというのが伝わってくる漫画だと思う。
ふるさと納税 あまり欲しいものもないし、ふるさと納税の行政手続きも一通り理解したから今年はやらなくてもいいかとも思っていた。しかし、paypayボーナスキャンペーン をみてやってみるかという気になった。paypay はいろんなものと連携していて見かけるたびにすごいなと思う。お得だからと必要もないものを買うことはないけど、ふるさと納税はやらなかったとしても、どのみち納税は必要なものなので還元があるということは節税につながるのかな？理屈はよくわからないけど、言いたいことは paypay はすごいという話でした。
dapr の api トークンを使った認証 Enable API token authentication in Dapr を一通り読んだ。内容はとくに難しくなく、こんな風に dapr の manifest を書けば JWT トークンを設定できますということを書いてある。私はずっとサーバーサイドばっかりやってきたからフロントエンドで使われる技術や仕組みに弱い。JWT トークンもその1つで、自分でちゃんと実装したことがないからちゃんとよく分かってない。これが OAuth2 なら provider を実装したこともあるからその仕組みも意図も理解できる。一度どこかで自分で JWT も実装してみないといけないのだろうな。
少し前にお仕事で kubernetes の secret の移行作業をやった。既存の secret にキーバリューを追加するときは patch を使う。
$ kubectl patch secret mydata -p=&amp;#39;{&amp;#34;stringData&amp;#34;:{&amp;#34;mykey&amp;#34;: &amp;#34;myvalue&amp;#34;}}&amp;#39; secret の内容を確認するときも2つのやり方がある。キーだけを確認するならこれでよい。
$ kubectl describe secrets mydata キーに対応する値もデコードして確認するならこうする。但し、閲覧注意。
$ kubectl get secret mydata -o json | jq &amp;#39;.</description><content>&lt;p>0時に寝て7時に起きてだらだらやってて午前中は &lt;a href="https://ja.wikipedia.org/wiki/%E9%A0%AD%E6%96%87%E5%AD%97D">頭文字D&lt;/a> のアニメをみてた。漫画 (アニメも？) はすでに完結しているのでいつか読もうと思いつつ最後まで読んでいない。ゴッドフットやゴッドアームが出てくるぐらいまでは読んだ気がする。その後どうなったのかを知らない。イニシャルDをみていると、ストーリーも絵も演出もまったく派手さはなくて普通なんだけど、なぜかおもしろくて続きをみてしまうという人間の娯楽の本質をついている気がしてくる。なんでなんだろうなぁ。&lt;/p>
&lt;h2 id="頭文字d">頭文字D&lt;/h2>
&lt;p>たまたま思い出したので夜に漫画喫茶行って頭文字Dを最後まで読んできた。全48巻で、31巻ぐらいから読み始めて3-4時間ぐらいで読み終えた。漫画なので仕方ないけど、対戦相手がどんどん強くなっていって勝ち方が玄人好みというのか、単純に抜いた・抜かれたの話しではなく、タイヤマネージメントがどうこうとか、恐怖に対する心理がどうこうとか、ドライバーと車のセッティングも含めた駆け引きが強くなっていって、どちらが速いかというよりは戦略通りの展開にもっていって最後はそれがうまくはまるみたいな、これまでもずっとそうだったんだけど、ここからはよりトップレベルのほんの僅かな差が勝敗を分けるといった描き方になっていったように思う。それはそれで現実に近い気はするけど、漫画的には派手な演出にならないので玄人好みなストーリーになっていった気がする。但し、そこまでやってきて最後の対戦相手だけは、個人的には納得感がなくて、ここまで緻密に作り上げてきた理論や個々のドライバーの修練の積み重ねが圧倒的天才の前にひれ伏すみたいな切り口が急展開していて、頭の切り替えができなかった感じがした。とはいえ、最後まで読み終えられてよかったし、作品としてはすごくおもしろかった。作者はモータースポーツが本当に好きなんだろうなというのが伝わってくる漫画だと思う。&lt;/p>
&lt;h2 id="ふるさと納税">ふるさと納税&lt;/h2>
&lt;p>あまり欲しいものもないし、ふるさと納税の行政手続きも一通り理解したから今年はやらなくてもいいかとも思っていた。しかし、&lt;a href="https://www.satofull.jp/static/campaign/202112_pcp.php">paypayボーナスキャンペーン&lt;/a> をみてやってみるかという気になった。paypay はいろんなものと連携していて見かけるたびにすごいなと思う。お得だからと必要もないものを買うことはないけど、ふるさと納税はやらなかったとしても、どのみち納税は必要なものなので還元があるということは節税につながるのかな？理屈はよくわからないけど、言いたいことは paypay はすごいという話でした。&lt;/p>
&lt;h2 id="dapr-の-api-トークンを使った認証">dapr の api トークンを使った認証&lt;/h2>
&lt;p>&lt;a href="https://docs.dapr.io/operations/security/api-token/">Enable API token authentication in Dapr&lt;/a> を一通り読んだ。内容はとくに難しくなく、こんな風に dapr の manifest を書けば &lt;a href="https://jwt.io/">JWT&lt;/a> トークンを設定できますということを書いてある。私はずっとサーバーサイドばっかりやってきたからフロントエンドで使われる技術や仕組みに弱い。JWT トークンもその1つで、自分でちゃんと実装したことがないからちゃんとよく分かってない。これが OAuth2 なら provider を実装したこともあるからその仕組みも意図も理解できる。一度どこかで自分で JWT も実装してみないといけないのだろうな。&lt;/p>
&lt;p>少し前にお仕事で kubernetes の secret の移行作業をやった。既存の secret にキーバリューを追加するときは patch を使う。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl patch secret mydata -p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;{&amp;#34;stringData&amp;#34;:{&amp;#34;mykey&amp;#34;: &amp;#34;myvalue&amp;#34;}}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>secret の内容を確認するときも2つのやり方がある。キーだけを確認するならこれでよい。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl describe secrets mydata
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>キーに対応する値もデコードして確認するならこうする。但し、閲覧注意。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get secret mydata -o json | jq &lt;span style="color:#e6db74">&amp;#39;.data | map_values(@base64d)&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>kustomize のパッチ適用の違い</title><link>/diary/posts/2021/1122/</link><pubDate>Mon, 22 Nov 2021 13:40:40 +0900</pubDate><guid>/diary/posts/2021/1122/</guid><description>22時ぐらいには寝て6時半に起きた。昨日はお出かけしてきてバテたんで19時頃からうたた寝を繰り返してずっと寝てた。実家に帰っていた期間を除いて、土日のどちらかを休むのはここ3ヶ月はなかったと思うし、土日の2日間ほとんど仕事をしなかったのは半年ぐらいはなかったと思う。久しぶりに土日に仕事しなかったなという印象で、その理由は業務委託のお仕事の契約が決まって余裕があるからだと思う。
kustomize の Inline Patch Inline Patch に次の3つのやり方が説明されている。
patchesStrategicMerge: Strategic Merge Patch として解析されるパッチファイルのリスト patchesJSON6902: 1つのターゲットリソースのみに適用可能な JSON Patch として解析されるパッチと関連付けされるターゲットのリスト patches: 関連付けされるターゲットとパッチのリスト。このパッチは複数のオブジェクトに適用でき、パッチが Strategic Merge Patch なのか JSON Patch かは自動的に検出 patches は patchesStrategicMerge と patchesJSON6902 の両方を記述できる。運用上は patchesStrategicMerge か patchesJSON6902 を適用したいパッチの内容によって使い分けることになる。おそらく前者は base にない要素を追加したり、base の要素をすべて置き換えたりするときに使う。後者は base にある map や list の一部の要素のみを限定して置き換えたり、削除したりするときに使う。ちなみに patchesJSON6902 の 6902 というのは RFC 6902 JavaScript Object Notation (JSON) Patch に由来する。
patchesJson6902 の例として次のような設定にパッチを適用する。base から読まれた metadata の要素から namespace のみを削除したり、spec.metadata の1番目のリストの secretKeyRef が参照する Secret を my-secret で置き換えたりできる。こういったパッチを patchesStrategicMerge で実現することはできないのではないかと思う (詳しくないので私が間違っているかもしれない) 。</description><content>&lt;p>22時ぐらいには寝て6時半に起きた。昨日はお出かけしてきてバテたんで19時頃からうたた寝を繰り返してずっと寝てた。実家に帰っていた期間を除いて、土日のどちらかを休むのはここ3ヶ月はなかったと思うし、土日の2日間ほとんど仕事をしなかったのは半年ぐらいはなかったと思う。久しぶりに土日に仕事しなかったなという印象で、その理由は業務委託のお仕事の契約が決まって余裕があるからだと思う。&lt;/p>
&lt;h2 id="kustomize-の-inline-patch">kustomize の Inline Patch&lt;/h2>
&lt;p>&lt;a href="https://kubectl.docs.kubernetes.io/guides/example/inline_patch/">Inline Patch&lt;/a> に次の3つのやり方が説明されている。&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>patchesStrategicMerge: Strategic Merge Patch として解析されるパッチファイルのリスト&lt;/li>
&lt;li>patchesJSON6902: 1つのターゲットリソースのみに適用可能な JSON Patch として解析されるパッチと関連付けされるターゲットのリスト&lt;/li>
&lt;li>patches: 関連付けされるターゲットとパッチのリスト。このパッチは複数のオブジェクトに適用でき、パッチが Strategic Merge Patch なのか JSON Patch かは自動的に検出&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>patches は patchesStrategicMerge と patchesJSON6902 の両方を記述できる。運用上は patchesStrategicMerge か patchesJSON6902 を適用したいパッチの内容によって使い分けることになる。おそらく前者は base にない要素を追加したり、base の要素をすべて置き換えたりするときに使う。後者は base にある map や list の一部の要素のみを限定して置き換えたり、削除したりするときに使う。ちなみに patchesJSON6902 の 6902 というのは &lt;a href="https://datatracker.ietf.org/doc/html/rfc6902">RFC 6902 JavaScript Object Notation (JSON) Patch&lt;/a> に由来する。&lt;/p>
&lt;p>patchesJson6902 の例として次のような設定にパッチを適用する。base から読まれた metadata の要素から namespace のみを削除したり、spec.metadata の1番目のリストの secretKeyRef が参照する Secret を my-secret で置き換えたりできる。こういったパッチを patchesStrategicMerge で実現することはできないのではないかと思う (詳しくないので私が間違っているかもしれない) 。&lt;/p>
&lt;h5 id="baseyml">base.yml&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Component&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">my-component&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">default&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">username&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">value&lt;/span>: &lt;span style="color:#ae81ff">user&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">password&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">secretKeyRef&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">base-secret&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">password&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="kustomizationyml">kustomization.yml&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">patchesJson6902&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#ae81ff">my-patch.yaml&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">target&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">group&lt;/span>: &lt;span style="color:#ae81ff">apps&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Component&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">my-component&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="my-patchyaml">my-patch.yaml&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">op&lt;/span>: &lt;span style="color:#ae81ff">remove&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#ae81ff">/metadata/namespace&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">op&lt;/span>: &lt;span style="color:#ae81ff">replace&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#ae81ff">/spec/metadata/1/secretKeyRef/name&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">value&lt;/span>: &lt;span style="color:#ae81ff">my-secret&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="リーンキャンバスレビュー-前半">リーンキャンバスレビュー (前半)&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2021/1114/#リーンキャンバス">前に作ったリーンキャンバス&lt;/a> を使って友だちにプロダクトの設計をレビューしてもらった。私がリーンキャンバスを作ったことがなかったので、この項目にはどういった内容を書くか、それぞれの項目がどういった関連付けや粒度で整理するかといった、リーンキャンバスの書き方そのものも含めて教えてもらった。&lt;/p>
&lt;p>私が設計のために作った40枚のスライドを話すと2時間必要とするが、リーンキャンバスを使えば要点のみ15分で話せるようになるのが狙いになるみたい。とはいえ、リーンキャンバスの書いてある内容の半分を確認するだけで今日は2時間弱かかってしまった。議事録を取りながらだったので話すだけならもっと短くなったかもしれないし、その背景や根拠を細かくツッコミしていくとそれなりの時間はかかるのかもしれない。リーンキャンバス上は数枚の付箋で簡潔に書いてあるが、これどういうこと？みたいな問いになると詳細を説明しないといけないので時間がかかったように思う。リーンキャンバスの精度や品質が上がれば、読み手が詳細を確認しなくても意図を理解しやすくて詳細のツッコミが不要になるのかもしれない。これまで使ったことがないツールでおもしろいので週末に後半を行う。課題管理の背景には実践知、認知心理学、情報共有、組織論といった様々な分野にまたがるのでそのコンテキストを共有するのはなかなか難しいのではないかという思いもある。&lt;/p></content></item><item><title>お仕事しかしなかった一日</title><link>/diary/posts/2021/1115/</link><pubDate>Mon, 15 Nov 2021 13:13:44 +0900</pubDate><guid>/diary/posts/2021/1115/</guid><description>2時に寝て6時半に起きた。寝る前にウォーキングしてくるとよく眠れる気がする。お仕事で簡単に終わると思ってた作業にちょっとはまってトラブルシューティングしてたら疲れた。原因はわかって自己解決できたのはよかったけど、消耗して早くお仕事を終えて帰ってくつろいでた。
ローカル開発環境の整備 お仕事でローカルの k8s 環境の保守の作業をしている。minikube でローカルの k8s クラスターを作成して kubectl コマンドで制御する。k8s の yaml の設定ファイルのことをマニフェストと呼ぶのかな？そのテンプレート？ジェネレーター的なツールに kustomize を使っている。先週から1週間触っていたので cli の操作にはだいぶ慣れてきた。
まだ基本的な delete &amp;amp; apply みたいなことしかやってないけど、また余裕のあるときに細かいコマンドやロールバックのやり方なども学習しようと思う。デプロイで一番重要なのはロールバック、次にローリングアップデート、ローリングアップデートができればカナリアリリースもできるかな？その2つがあれば運用は大幅にコスト削減できるし、開発のアジリティも上げられる。たぶん k8s を使えば簡単にできるんだろうなというのは delete &amp;amp; apply だけみてもそう受け取れる。k8s クラスターさえマネージドならよく出来た仕組みだなと感心した。</description><content>&lt;p>2時に寝て6時半に起きた。寝る前にウォーキングしてくるとよく眠れる気がする。お仕事で簡単に終わると思ってた作業にちょっとはまってトラブルシューティングしてたら疲れた。原因はわかって自己解決できたのはよかったけど、消耗して早くお仕事を終えて帰ってくつろいでた。&lt;/p>
&lt;h2 id="ローカル開発環境の整備">ローカル開発環境の整備&lt;/h2>
&lt;p>お仕事でローカルの k8s 環境の保守の作業をしている。&lt;a href="https://minikube.sigs.k8s.io/docs/">minikube&lt;/a> でローカルの k8s クラスターを作成して &lt;a href="https://kubernetes.io/docs/reference/kubectl/overview/">kubectl&lt;/a> コマンドで制御する。k8s の yaml の設定ファイルのことをマニフェストと呼ぶのかな？そのテンプレート？ジェネレーター的なツールに &lt;a href="https://kustomize.io/">kustomize&lt;/a> を使っている。先週から1週間触っていたので cli の操作にはだいぶ慣れてきた。&lt;/p>
&lt;p>まだ基本的な delete &amp;amp; apply みたいなことしかやってないけど、また余裕のあるときに細かいコマンドやロールバックのやり方なども学習しようと思う。デプロイで一番重要なのはロールバック、次にローリングアップデート、ローリングアップデートができればカナリアリリースもできるかな？その2つがあれば運用は大幅にコスト削減できるし、開発のアジリティも上げられる。たぶん k8s を使えば簡単にできるんだろうなというのは delete &amp;amp; apply だけみてもそう受け取れる。k8s クラスターさえマネージドならよく出来た仕組みだなと感心した。&lt;/p></content></item><item><title>Kubernetes 使い始めの雑感</title><link>/diary/posts/2021/1108/</link><pubDate>Mon, 08 Nov 2021 08:40:51 +0900</pubDate><guid>/diary/posts/2021/1108/</guid><description>1時に寝て7時に起きた。夜にウォーキングし始めてからよく眠れるようになった気がする。
udemy: Kubernetes入門 昨日 の続き。今日はセクション6から最後まで。CI/CD のセクションだけスキップして、他は一通り目を通した。
セクション6 Kubernetes実践 1つずつ書くのは大変だけど、数をこなして徐々に覚えていけばよい。手で書くのもよいが、別のやり方としてクライアント側で dry run すると、設定のひな型を作ってくれるのでそれに必要な設定を足すのもよい。
$ kubectl create deploy mysql --image=mysql:5.7 --dry-run=client -o yaml $ kubectl exec -n database -it mysql-787f86d65c-nflxx -- mysql -uroot -ppassword データベースとアプリケーションを異なる namespace にデプロイして、それらが通信できるような設定を行う。基本的には --dry-run=client でひな型を作りつつ、必要な設定を追加していくやり方が簡単そうにみえた。とはいえ、実際に設定していくときはどういう設定を追加するとどういう振る舞いになるかを調べながら作業すると思うのでこんな簡単にはできないとは思う。次のようなアプリケーションをデプロイする一覧の流れを理解できた。
namespace 作成 Deployment 作成 ConfigMap 作成 Secret 作成 Deployment, ConfigMap, Secret 適用 Service 適用 port-forward でローカルからアクセス (作成したリソースをすべて削除) セクション7 KubernetesのDebug 基本は pod のステータスを確認しながら問題があれば、その箇所を追いかけていって原因を調査する。
$ kubectl get pod -A $ kubectl get pod -A --selector run=nginx k8s 上で実行しているアプリケーションの依存先へ接続できない場合は Service の確認が必要となる。kubectl の get, describe, logs などのサブコマンドをあれこれみながらエラーの原因を把握して、yaml の設定を変更していく。k8s のアーキテクチャとコマンドを覚えていないとなかなか難しそう。</description><content>&lt;p>1時に寝て7時に起きた。夜にウォーキングし始めてからよく眠れるようになった気がする。&lt;/p>
&lt;h2 id="udemy-kubernetes入門">udemy: Kubernetes入門&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2021/1107/#udemy-kubernetes入門">昨日&lt;/a> の続き。今日はセクション6から最後まで。CI/CD のセクションだけスキップして、他は一通り目を通した。&lt;/p>
&lt;h3 id="セクション6-kubernetes実践">セクション6 Kubernetes実践&lt;/h3>
&lt;p>1つずつ書くのは大変だけど、数をこなして徐々に覚えていけばよい。手で書くのもよいが、別のやり方としてクライアント側で dry run すると、設定のひな型を作ってくれるのでそれに必要な設定を足すのもよい。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl create deploy mysql --image&lt;span style="color:#f92672">=&lt;/span>mysql:5.7 --dry-run&lt;span style="color:#f92672">=&lt;/span>client -o yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl exec -n database -it mysql-787f86d65c-nflxx -- mysql -uroot -ppassword
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>データベースとアプリケーションを異なる namespace にデプロイして、それらが通信できるような設定を行う。基本的には &lt;code>--dry-run=client&lt;/code> でひな型を作りつつ、必要な設定を追加していくやり方が簡単そうにみえた。とはいえ、実際に設定していくときはどういう設定を追加するとどういう振る舞いになるかを調べながら作業すると思うのでこんな簡単にはできないとは思う。次のようなアプリケーションをデプロイする一覧の流れを理解できた。&lt;/p>
&lt;ol>
&lt;li>namespace 作成&lt;/li>
&lt;li>Deployment 作成&lt;/li>
&lt;li>ConfigMap 作成&lt;/li>
&lt;li>Secret 作成&lt;/li>
&lt;li>Deployment, ConfigMap, Secret 適用&lt;/li>
&lt;li>Service 適用&lt;/li>
&lt;li>port-forward でローカルからアクセス&lt;/li>
&lt;li>(作成したリソースをすべて削除)&lt;/li>
&lt;/ol>
&lt;h3 id="セクション7-kubernetesのdebug">セクション7 KubernetesのDebug&lt;/h3>
&lt;p>基本は pod のステータスを確認しながら問題があれば、その箇所を追いかけていって原因を調査する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get pod -A
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get pod -A --selector run&lt;span style="color:#f92672">=&lt;/span>nginx
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>k8s 上で実行しているアプリケーションの依存先へ接続できない場合は Service の確認が必要となる。kubectl の get, describe, logs などのサブコマンドをあれこれみながらエラーの原因を把握して、yaml の設定を変更していく。k8s のアーキテクチャとコマンドを覚えていないとなかなか難しそう。&lt;/p>
&lt;p>とりあえず動かした後にまとめて全部削除できるのがテストやデバッグに便利そう。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl delete -f .
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>もしくは開発用に独自の namespace を作成して、あとで丸ごと namespace を削除するのでもよさそう。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl delete ns mynamespace
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="k8s-の調査">k8s の調査&lt;/h2>
&lt;p>業務のアプリケーションを minikube で作ったローカル k8s クラスターで動かしてみた。ローカルの開発環境の構築方法をメンテナンスして、自分でも一通り k8s の yaml を書いて、デプロイして、振る舞いを確認したりしていた。最初なのでおもしろい。自分で一通りやってみて、k8s が難しいとみんなが言っているのは k8s クラスターを自前で構築するのが難しいのだとようやく理解できた。k8s クラスターがすでにある状態なら kubectl の使い方を覚えるだけで全く難しくない。GKE や EKS を使って運用するなら k8s の運用コストは大したことがないと理解できた。k8s クラスター向けの yaml はたくさん書かないといけないけど、どうせ ECS や EC2 でやっていても CDK や Terraform などのインフラ設定を書くのは同じなのでそこはあまり差がない。k8s はコンテナオーケストレーションをやってくれるメリットが大きいので minikube と EKS の環境の差異があまり問題にならないようなアプリケーション開発であれば、普通に使っていって問題ないように思えた。ローカルで環境作るのが大変なんじゃないかという先入観があったけど、全然そんなことはなかった。コンテナのイメージをビルドしないといけないのが追加のコストかな。&lt;/p></content></item><item><title>普通の休日の翌日</title><link>/diary/posts/2021/1107/</link><pubDate>Sun, 07 Nov 2021 11:21:21 +0900</pubDate><guid>/diary/posts/2021/1107/</guid><description>5時に寝て10時に起きた。昨日は夕方に2-3時間寝てたのでその分、夜に調べものをしていた。休みたい気持ちもあるけど、調べるものが多過ぎて全然時間が足りない。
bizpy 勉強会の資料作り 昨日 の続き。昨日サンプルコードを実装したので、その設定や要点を 資料 に作成した。現時点で Python で Slack のインテグレーションをやってみる勉強会 #2 の参加者は10人。連続シリーズは回を重ねるごとに減っていくものなのでこんなもんかな。あともう1回やったら終わりにする。
udemy: Kubernetes入門 友だちから udemy の k8s のコースがよいと聞いたんだけど、そのコースはいまは提供されていなくて、せっかくなので適当に検索してヒットした Kubernetes入門 を受講することに決めた。本当は英語の本格的なコースを受講した方がよいのだろうけど、余裕のあるときはそれでいいけど、いま数日で概要を把握して使えるようにしたいので日本語のコースにしてみた。
Udemy の Learning Docker and Kubernetes by Lab がとてもよい Docker, Kubernetes 学習とツールとコンピュータサイエンス 昨日インストールした minikube のクラスターを使って「Kubernetes入門」のセクション1からセクション5までやった。だいたい半分ぐらい。所感としては、全く何も知らない人には要点をかいつまんで教えてくれるのと、最初に覚えるとよい基本的な CLI のコマンドとその振る舞いや設定を紹介してくれるのでよかった。初めて k8s に挑戦する自分にとってはちょうどよいレベル感だった。全体像の概念を捉えてコンテキストに沿って順番にハンズオン形式で学習していくスタイル。nakamasato/kubernetes-basics を使って自分でも CLI でコマンドを打ちながら進めてみた。yaml ファイルを定義するのもこれはこれで面倒だけど、この辺は慣れの問題かな？とも思う。いくつか学んだことを整理しておく。
セクション1 Introduction k8s には2つのコンポーネントがあり、これを k8s クラスターと呼んでいる。
Control Plane (API サーバー) 複数の Worker (Kubelet) yaml で設定する Desired State (理想状態) と呼ばれる設定が登録されると、Control Plane の API サーバーと Worker の kubelet が通信してそれを実現しようとする。pod とは k8s のデプロイの最小単位となる。コンテナ、ポート、レプリカ数などを設定する。pod をそれぞれの Worker にデプロイしたり、Worker がダウンしたときに別の Worker で起動させたりする。</description><content>&lt;p>5時に寝て10時に起きた。昨日は夕方に2-3時間寝てたのでその分、夜に調べものをしていた。休みたい気持ちもあるけど、調べるものが多過ぎて全然時間が足りない。&lt;/p>
&lt;h2 id="bizpy-勉強会の資料作り">bizpy 勉強会の資料作り&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2021/1106/#slack-apps-の調査">昨日&lt;/a> の続き。昨日サンプルコードを実装したので、その設定や要点を &lt;a href="https://github.com/t2y/python-study/tree/master/BizPy/slack/20211027">資料&lt;/a> に作成した。現時点で &lt;a href="https://bizpy.connpass.com/event/229091/">Python で Slack のインテグレーションをやってみる勉強会 #2&lt;/a> の参加者は10人。連続シリーズは回を重ねるごとに減っていくものなのでこんなもんかな。あともう1回やったら終わりにする。&lt;/p>
&lt;h2 id="udemy-kubernetes入門">udemy: Kubernetes入門&lt;/h2>
&lt;p>友だちから udemy の k8s のコースがよいと聞いたんだけど、そのコースはいまは提供されていなくて、せっかくなので適当に検索してヒットした &lt;a href="https://www.udemy.com/course/kubernetes-basics-2021/">Kubernetes入門&lt;/a> を受講することに決めた。本当は英語の本格的なコースを受講した方がよいのだろうけど、余裕のあるときはそれでいいけど、いま数日で概要を把握して使えるようにしたいので日本語のコースにしてみた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://blog.ayakumo.net/entry/2018/01/27/010000">Udemy の Learning Docker and Kubernetes by Lab がとてもよい&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.ayakumo.net/entry/2018/02/15/232918">Docker, Kubernetes 学習とツールとコンピュータサイエンス&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>昨日インストールした minikube のクラスターを使って「Kubernetes入門」のセクション1からセクション5までやった。だいたい半分ぐらい。所感としては、全く何も知らない人には要点をかいつまんで教えてくれるのと、最初に覚えるとよい基本的な CLI のコマンドとその振る舞いや設定を紹介してくれるのでよかった。初めて k8s に挑戦する自分にとってはちょうどよいレベル感だった。全体像の概念を捉えてコンテキストに沿って順番にハンズオン形式で学習していくスタイル。&lt;a href="https://github.com/nakamasato/kubernetes-basics">nakamasato/kubernetes-basics&lt;/a> を使って自分でも CLI でコマンドを打ちながら進めてみた。yaml ファイルを定義するのもこれはこれで面倒だけど、この辺は慣れの問題かな？とも思う。いくつか学んだことを整理しておく。&lt;/p>
&lt;h3 id="セクション1-introduction">セクション1 Introduction&lt;/h3>
&lt;p>k8s には2つのコンポーネントがあり、これを k8s クラスターと呼んでいる。&lt;/p>
&lt;ul>
&lt;li>Control Plane (API サーバー)&lt;/li>
&lt;li>複数の Worker (Kubelet)&lt;/li>
&lt;/ul>
&lt;p>yaml で設定する Desired State (理想状態) と呼ばれる設定が登録されると、Control Plane の API サーバーと Worker の kubelet が通信してそれを実現しようとする。pod とは k8s のデプロイの最小単位となる。コンテナ、ポート、レプリカ数などを設定する。pod をそれぞれの Worker にデプロイしたり、Worker がダウンしたときに別の Worker で起動させたりする。&lt;/p>
&lt;h3 id="セクション2-kubernets-概要">セクション2 Kubernets 概要&lt;/h3>
&lt;p>k8s はコンテナ化したアプリケーションのデプロイ、スケーリング、管理を行うためのオープンソースのコンテナオーケストレーションシステムである。&lt;/p>
&lt;ul>
&lt;li>コンテナ
&lt;ul>
&lt;li>独立した環境でアプリケーションを実行する仕組み&lt;/li>
&lt;li>コンテナの実態はプロセス&lt;/li>
&lt;li>Kernel Namespaces を利用し、プロセスID、ネットワークインターフェース、リソースなどを分離してコンテナ間で干渉しない&lt;/li>
&lt;li>ホストマシンへの依存度を最小化してアプリケーションをどこでも実行可能にする
&lt;ul>
&lt;li>従来のやり方の最大の違いはライブラリがホストマシンにインストールされるのではなく、コンテナの内部にインストールされる&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>オーケストレーション
&lt;ul>
&lt;li>デプロイ、スケーリング、管理などの仕組み&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>1つのアプリケーションは複数のマシン上で動かすことで可用性を高めたいが、コンテナを動かすために考えることが増えていくと管理コストも増えていく。コンテナオーケストレーション機能により次のようなシステム管理者が行っていたことが自動化される。&lt;/p>
&lt;ul>
&lt;li>デプロイメント&lt;/li>
&lt;li>スケジューリング&lt;/li>
&lt;li>オートスケーリング
&lt;ul>
&lt;li>負荷に応じてコンテナ数やマシン数を増減させる&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ネットワーク&lt;/li>
&lt;li>リソースマネジメント&lt;/li>
&lt;li>セキュリティ
&lt;ul>
&lt;li>ネットワークポリシーやリソースの権限定義&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>k8s クラスターの構造は次になる。&lt;/p>
&lt;ul>
&lt;li>Control Plane
&lt;ul>
&lt;li>api: kubelet と通信するサーバー&lt;/li>
&lt;li>etcd: 設定などを格納するキーバリューストア&lt;/li>
&lt;li>shed: kube スケジューラー&lt;/li>
&lt;li>c-m: コントロールマネージャー&lt;/li>
&lt;li>c-c-m: クラウドプロバイダと api 連携する
&lt;ul>
&lt;li>ローカルで使うときは必要ない&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Worker ノードはコンテナランタイムをいインストールしておく必要がある
&lt;ul>
&lt;li>kubelet は Control Plane と通信するためのエージェントとして動作する&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>一番需要なこととして、k8s は理想状態と現実状態を比較して、理想状態に近づけようとする。app.yaml の理想状態を kubectl を用いて api サーバーを介して etcd に格納する。現実状態は kubelet から api サーバーを介して etcd に格納される。c-m は理想状態と現実状態のチェックを行い、異なっていれば理想状態に近づけることをしていく。&lt;/p>
&lt;h3 id="セクション4-kubectl">セクション4 kubectl&lt;/h3>
&lt;p>minikube で最初に起動しているのは Control Plane を起動していることが理解できた。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ minikube start
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl config current-context
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>minikube
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>同時に ~/.kube/config に kubectl の設定も追加される。&lt;code>minikube&lt;/code> という名前でクラスター、ユーザー、コンテキストが設定される。&lt;/p>
&lt;p>リソース一覧の確認。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl api-resources
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>出力フォーマットも様々。例えば、デフォルトの表示は次になる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get node
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STATUS ROLES AGE VERSION
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>minikube Ready control-plane,master 7m21s v1.22.2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get node -o wide
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>minikube Ready control-plane,master 8m38s v1.22.2 192.168.49.2 &amp;lt;none&amp;gt; Ubuntu 20.04.2 LTS 5.11.0-38-generic docker://20.10.8
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>より詳細な情報をそれぞれのフォーマットで表示する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get node -o json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get node -o yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>namespace を確認する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STATUS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default Active 10m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-node-lease Active 10m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-public Active 10m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-system Active 10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>namespace を指定して pod 一覧を取得する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get pod --namespace kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>coredns-78fcd69978-qxqbn 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 11m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>etcd-minikube 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 11m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-apiserver-minikube 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 11m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-controller-manager-minikube 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 11m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-proxy-g55hg 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 11m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-scheduler-minikube 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 11m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>storage-provisioner 1/1 Running &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">(&lt;/span>10m ago&lt;span style="color:#f92672">)&lt;/span> 11m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>グローバルな CLI のオプションを確認する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl options
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ノードの詳細を表示する。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl describe node
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>describe は名前の接頭辞を指定できるので namespace ならこんな感じに実行できる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl describe namespace kube-
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="セクション5-kubernetes-リソース">セクション5 Kubernetes リソース&lt;/h3>
&lt;p>pod とは k8s 上のデプロイの最小単位である。&lt;/p>
&lt;ul>
&lt;li>1つまたは複数のコンテナをもつ&lt;/li>
&lt;li>ネットワークやストレージを共有リソースとしてもつ&lt;/li>
&lt;li>コンテナの実行方法に関する仕様をもつ&lt;/li>
&lt;/ul>
&lt;p>pod が使えなくなった場合に他のノードにデプロイされることもある。1つのアプリケーションを複数の pod でデプロイすることが多い。なるべく複数のアプリケーションを1つの pod に入れない。個別の pod を直接操作しない。&lt;/p>
&lt;p>共有コンテキスト&lt;/p>
&lt;ul>
&lt;li>同一 pod 内のコンテナは同じストレージにアクセスできる&lt;/li>
&lt;li>同一 pod 内のコンテナは ip アドレスとポートを含むネットワーク名前空間を共有する&lt;/li>
&lt;/ul>
&lt;p>k8s オブジェクト&lt;/p>
&lt;ul>
&lt;li>クラスタの状態を表現する&lt;/li>
&lt;li>2つのフィールドをもつ
&lt;ul>
&lt;li>spec: 理想状態 (desired status)&lt;/li>
&lt;li>status: 現実状態 (current status)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>pod の作成は k8s オブジェクトを作成している。オブジェクト作成時の必須フィールドが4つある。&lt;/p>
&lt;ul>
&lt;li>apiVersion&lt;/li>
&lt;li>kind&lt;/li>
&lt;li>metadata&lt;/li>
&lt;li>spec&lt;/li>
&lt;/ul>
&lt;p>namespace は同一クラスター上で複数の仮想クラスターの動作をサポートする。&lt;/p>
&lt;ul>
&lt;li>仮想クラスターとは、物理的には同じマシンで動いているかもしれないが、仮想的に環境を分離している
&lt;ul>
&lt;li>1つのクラスターを論理的にわける&lt;/li>
&lt;li>チームや部署ごとにわけて使い分けたりすることも多い&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>namespace を使うメリットは次になる。&lt;/p>
&lt;ul>
&lt;li>pod やコンテナのリソースの範囲設定&lt;/li>
&lt;li>namespace 全体の総リソース制限&lt;/li>
&lt;li>権限管理&lt;/li>
&lt;/ul>
&lt;p>初期の namespace として4つあるが、初心者は最初の2つだけをまず覚えておく。&lt;/p>
&lt;ul>
&lt;li>default:&lt;/li>
&lt;li>kube-system:&lt;/li>
&lt;li>kube-public:&lt;/li>
&lt;li>kube-node-lease&lt;/li>
&lt;/ul>
&lt;p>namespace と cluster の違い。&lt;/p>
&lt;ul>
&lt;li>Namespace-scoped リソース
&lt;ul>
&lt;li>namespace に属しているリソース&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Cluster-scoped リソース
&lt;ul>
&lt;li>クラスター全体で使われるもの&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>次のコマンドで確認できる。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl api-resources --namespaced&lt;span style="color:#f92672">=&lt;/span>true
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>namespace の作成&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl create namespace my-namespace
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ワークロードリソースとは複数の pod を作成・管理するためのリソース。ワークロードリソースは pod テンプレートを使って pod を作成する。&lt;/p>
&lt;ul>
&lt;li>ReplicaSet
&lt;ul>
&lt;li>常に指定したレプリカ数の pod を保つ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Deployment
&lt;ul>
&lt;li>ローリングアップデートやロールバックなどのアップデート機能を提供&lt;/li>
&lt;li>ReplicaSet のロールアウト&lt;/li>
&lt;li>不安定な場合の前のバージョンへロールバック&lt;/li>
&lt;li>使用頻度が高い
&lt;ul>
&lt;li>ほとんどのアプリケーションは Deployment で管理&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Secret
&lt;ul>
&lt;li>機密情報を保存・管理し、Pod から参照可能&lt;/li>
&lt;li>主な使用方法としてコンテナの環境変数の設定
&lt;ul>
&lt;li>アプリケーションの DB のパスワードなどに使う&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Service
&lt;ul>
&lt;li>pod の集合を抽象化して公開する
&lt;ul>
&lt;li>pod の集合に対する DNS 名&lt;/li>
&lt;li>pod の集合に対する負荷分散&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></content></item></channel></rss>