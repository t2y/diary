<!doctype html><html lang=en><head><title>kubernetes :: forest nook</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=/diary/tags/kubernetes/><link rel=stylesheet href=/diary/assets/style.css><link rel=stylesheet href=/diary/assets/green.css><link rel=stylesheet href=/diary/style.css><link rel=apple-touch-icon href=/diary/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=/diary/favicon.ico><meta name=twitter:card content="summary"><meta name=twitter:site content="t2y"><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta property="og:title" content="kubernetes"><meta property="og:description" content><meta property="og:url" content="/diary/tags/kubernetes/"><meta property="og:site_name" content="forest nook"><meta property="og:image" content="/diary/favicon.ico"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><link href=/diary/tags/kubernetes/index.xml rel=alternate type=application/rss+xml title="forest nook"></head><body class=green><div class="container full headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/diary><div class=logo>forest nook</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/diary/about>自己紹介</a></li><li><a href=/diary/dates>月別一覧</a></li><li><a href=/diary/tags>タグ一覧</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/diary/about>自己紹介</a></li><li><a href=/diary/dates>月別一覧</a></li><li><a href=/diary/tags>タグ一覧</a></li></ul></nav></header><div class=content><div class=posts><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0826/>簡単な現象の組み合わせ障害</a></h1><div class=post-meta><span class=post-date>2022-08-26</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/datadog/>datadog</a>&nbsp;
#<a href=/diary/tags/operation/>operation</a>&nbsp;
#<a href=/diary/tags/drinking/>drinking</a>&nbsp;
#<a href=/diary/tags/tax/>tax</a>&nbsp;</span><div class=post-content><p>0時に寝て6時に起きた。</p><h2 id=eks-クラスター障害の原因判明>eks クラスター障害の原因判明</h2><p><a href=/diary/posts/2022/0820/#aws-インフラの調子が悪い>過去に2回発生していた eks クラスター障害</a> の原因がようやくわかった。テスト環境も本番環境は5日ごとに再現していて、datadog で k8s のダッシュボードでそれぞれの pod 単位のメモリ使用量をみると datadog-agent の pod がメモリリークしていることに気付いた。そこから当たりをつけて datadog-agent の issue を調べると次のバグに遭遇していた。</p><ul><li><a href=https://github.com/DataDog/datadog-agent/issues/12997>[BUG] agent leaves defunct processes with version 7.38.0 #12997</a></li></ul><p>ゾンビプロセスが生成されて、それが os のプロセス数上限に達してしまい、それによってプロセス (スレッド) が生成できなくなって、その結果として <a href=https://github.com/aws/amazon-vpc-cni-k8s>aws/amazon-vpc-cni-k8s</a> の <code>aws-node</code> という eks クラスターの管理アプリケーションが動かなくなって、それが動かないと k8s ノードのステータスが NotReady になってしまって、通常の pod のアプリケーションも動かなくなってしまうという現象が発生していた。datadog-agent のアップグレードは私が行ったものだし、その後の k8s ノードの監視や調査で気付きが足りなかったと反省した。</p><ul><li>datadog-agent の新しいバージョンをテスト環境でもうしばらく検証してもよかった</li><li>datadog-agent をリソースリークの可能性を私の中の調査対象から外していた<ul><li>世の中で使われているものに致命的なバグが起きないだろうという先入観があった</li></ul></li><li>プロセスを生成できない原因として考えられる背景を調査すべきだった<ul><li>ulimit を確認してリソース制限はないようにみえた</li><li>プロセス数やゾンビプロセスを調べていなかった</li><li>kernel に <code>/proc/sys/kernel/pid_max</code> という上限設定があることを知らなかった</li></ul></li><li>テスト環境と本番環境で5日程度で落ちるという周期性から気付くべきだった<ul><li>たしかにテスト環境から1日遅れて本番環境で障害が発生していた</li><li>周期性があることでリソースリークの可能性は高いとすぐに調査すべきだった</li></ul></li><li>datadog で k8s のダッシュボードを調べるべきだった<ul><li>すでに用意されているものがあったのでみようと思えばみえた</li></ul></li><li>aws のインフラ要因ではないかと疑っていた<ul><li>ごめんなさい</li></ul></li></ul><p>これは悔しい。自分の無能さや気付きの低さを実感した事件だった。私が注意深く観察していればもう1週間早く気付けた。そのせいで余分な障害と調査に時間を費やした。1つ1つは全く難しくない現象が巧妙に絡みあって隠蔽された結果としての状況に気付けなかった。注意して1つずつ観察して追跡していけばすぐに気付けた。本当に悔しい。</p><p>1つだけ言い訳をさせてもらうと、私は本番環境にアクセスできない。だからテスト環境と本番環境で発生している現象が同じかどうかを判断できず、調査を進める確証をもてなかった。</p><h2 id=呑み>呑み</h2><p>あまりに悔しかったのと調査してたら遅くなって晩ご飯食べる気力もなかったので気分転換に仲のよい焼き鳥屋さんに寄ってみた。あとから常連客のセブンイレブンの店長さんも来られて、私は初対面かなと思ってたんだけど先方は知っていると言ってたから以前にもカウンターでご一緒していたみたい。何気はなしに3人で2時前ぐらいまで雑談していた。</p><p>その店長さんがロレックスを購入しようと考えているという話しになって、資産または投資商品としてのロレックスの話しになった。たまたまヒカキンが1億円で買ったロレックスがいま2億円になっているといった話しがあったそうで、いまがバブルな状態らしいが、ロレックスをはじめとした高級時計の資産価値が上がっているらしい。私は腕時計を身につけないし高級時計もまったく興味はないが、投資商品の1つなんだというところに関心がもてた。</p><div class=video-container><iframe src=https://www.youtube.com/embed/1knsQZLeh7U allowfullscreen title=1億円で買った時計が大変なことになってしまいました…></iframe></div><p>中小企業の社長の一般的な節税方法の1つに外車を買ったり売ったりするという話しがある。儲かったときに経費で外車を買って、赤字のときに外車を売って雑所得に変える。車は社用車として経費で落とせるから可能なことだが、高級時計はどうなのだろうか？ 結論から言うと、普通の会社では高級時計は経費にできない。経費の原則は売上を上げるために必要な支出を経費とできる。普通の会社は高級時計で売上を上げることはできない。一方で経費として認められる職業もある。芸能人がそうだという。それは番組のために必要だという理屈で経費で落とせる。おそらくヒカキンも経費で高級時計を購入して、そのことを動画にしているのも仕事で必要だという言い訳作りの目的もあるのだと推測する。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0820/>休日の本番障害</a></h1><div class=post-meta><span class=post-date>2022-08-20</span></div><span class=post-tags>#<a href=/diary/tags/life/>life</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/operation/>operation</a>&nbsp;</span><div class=post-content><p>夕方から寝ていて何度か起きたものの、そのままずっと寝ていた。あまりないことなんだけど、珍しくたくさん眠れた。</p><h2 id=ストレッチ>ストレッチ</h2><p>今日の開脚幅は開始前160cmで、ストレッチ後163cmだった。計測の仕方がややいい加減だった気もしたが、先週より少しよくなったということにしておく。右腰の張りが強いのと肩が前に入りがちなので肩を開いて姿勢を保つように心がけるとよいとアドバイスをいただいた。もう通い始めて1年半ぐらい経つ。トレーナーさんも大半が入れ替わっていて通い始めたときに話しかけてくれた私が知っているトレーナーさんはほとんどいない。1年半も経つと人は変わっていくなというのを実感している。私の最初のトレーナーさんは社内制度で別の店舗の助っ人に行っているのでいなくなった人たちが辞めているわけでもないとは思うけど、1-2年で人が入れ替わってもサービスは継続していかないといけないし、会社ってそういうものだなと実感する機会でもある。</p><h2 id=aws-インフラの調子が悪い>aws インフラの調子が悪い？</h2><p>1-2週間ぐらい前からテスト環境を含めると複数回発生している <a href=/diary/posts/2022/0815/>eks クラスターの障害</a> がたまたま土曜日の夜という休日に発生した。いま eks クラスターのインフラの振る舞いを把握しているのは私だけなので、気付いてから指示を出して問題が発生している k8s ノードの削除 (ec2 インスタンスの削除) で復旧させるワークアラウンドで復旧させた。私は本番環境にアクセスできないので詳しい調査はできない。状況を正しく把握できてはいないけれど、k8s ノードが死んだり生き返ったりする不安定な状況に発生しているらしく、k8s ノードを削除して新規に作り直すと復旧することがわかっている。NotReady と Ready を繰り返したりしてアプリケーションの振る舞いが不安定になる。NotReady,SchedulingDisabled になれば、おそらく drain して k8s ノードが入れ替わってくれるのだけど、そうならない不安定な状況があるみたい。これ以上の調査は aws のサポートに問い合わせないとわからない。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0815/>k8s ノードの削除方法がわからない</a></h1><div class=post-meta><span class=post-date>2022-08-15</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/operation/>operation</a>&nbsp;</span><div class=post-content><p>1時に寝て7時に起きた。寝冷えしてお腹痛い。</p><h2 id=eks-クラスターの障害>eks クラスターの障害</h2><p>日曜日にテスト環境の eks クラスターで障害が発生していた。k8s ノードが NotReady になっていて、しばらくすると NotReady,SchedulingDisabled に変わって、それから新しい k8s ノードが起動して古いものが削除されて置き換わった。おそらくエラーが発生し始めてから1時間ほどはかかっていたと思う。わりと時間がかかるので明らかに k8s ノードが不調だと人間が判断しているなら ec2 インスタンスの切り替えを早くやりたい。k8s の公式ドキュメントの <a href=https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#use-kubectl-drain-to-remove-a-node-from-service>Use kubectl drain to remove a node from service</a> では次の手順で行うように書いてある。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl drain &lt;node name&gt;
</span></span></code></pre></div><p>drain が正常終了すれば安全に k8s ノードを削除してよいのかな？</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl delete node &lt;node name&gt;
</span></span></code></pre></div><p>eks クラスターで障害が発生していたときに drain を実行するとエラーになったのでそのまま delete node したら k8s ノードは削除されたものの、自動的に新しい k8s ノードが起動しなかった。aws のマネジメントコンソールから ec2 インスタンスを調べたら起動したままだったので強制的に ec2 インスタンスを終了させたところ、オートスケールの設定から ec2 インスタンスが起動してきて復旧した。但し、このやり方は k8s が意図した手順ではないようにも思える。軽く調べた範囲では k8s ノードの正しい削除方法 (置き換え方法？) がみつからなかった。そんなことを日曜日に確認していたら月曜日にほぼ同じ現象が本番環境の eks クラスターでも発生した。私は一度経験していたので同僚に指示して経過を観察していた。ここで書いたのと同じような手順で復旧した。おそらく aws 側のなにかのメンテナンス作業でうちの eks クラスターだと k8s ノードが死んでしまうような作業があったのではないか？と疑いをもっている。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0810/>アプリケーションとジョブの違いと一時停止</a></h1><div class=post-meta><span class=post-date>2022-08-10</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>2時に寝て5時に起きた。たった3時間しか寝てないのに夜中に1回は起きている。スポットで9時から会議が入ったのでそれまでに朝のお仕事を終わらせようと思ったら早起きできた。6時前にはオフィスに行ってお仕事を始められた。</p><h2 id=アプリケーションからジョブへの移行>アプリケーションからジョブへの移行</h2><p>k8s の cronjob を活用する前は spring の <a href=https://www.baeldung.com/spring-scheduled-tasks>Scheduled</a> アノテーションで定期実行処理を書いていた。アプリケーションサーバー内の1スレッドが定期実行していた。これはスケールアウト前提のアプリケーションサーバーには向いてなくて、複数のアプリケーションサーバーをスケールアウトさせてデプロイすると、定期処理も複数実行されてしまい、同時実行できない類の処理だと問題になる。k8s の cronjob は同時実行しない設定などもあるため、k8s の cronjob へ移行している。</p><p>移行作業の準備をしていてアプリケーションサーバーの pod は <code>replicas</code> を調整することで一時停止の代替となるオペレーションができる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl scale --replicas<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> deployment/my-app
</span></span><span style=display:flex><span>deployment.apps/my-app scaled
</span></span></code></pre></div><p>移行時のアプリケーションサーバーはこれで無効にしておき、cronjob に入れ替えるといった切り戻し可能な状態で移行できる。cronjob も <code>suspend</code> のフラグを使うことで一時停止できるので検証しながら双方を切り替えることができる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl patch cronjob my-batch-job1 -p <span style=color:#e6db74>&#39;{&#34;spec&#34;: {&#34;suspend&#34;: true}}}&#39;</span>
</span></span><span style=display:flex><span>cronjob.batch/my-batch-job1 patched
</span></span></code></pre></div></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0808/>k8s クラスターの service account とロール</a></h1><div class=post-meta><span class=post-date>2022-08-08</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>23時に寝て6時に起きた。前日は夕方からだらだらしてた。</p><h2 id=k8s-クラスターの権限管理>k8s クラスターの権限管理</h2><p>初期のクラスターを私が構築したわけではないため、権限周りはあまりよくわかっていない。k8s には2つのユーザーアカウントがある。</p><ul><li>user account: 人向け</li><li>service account: アプリケーション向け</li></ul><blockquote><p>In this regard, Kubernetes does not have objects which represent normal user accounts. Normal users cannot be added to a cluster through an API call.</p><p><a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#users-in-kubernetes>https://kubernetes.io/docs/reference/access-authn-authz/authentication/#users-in-kubernetes</a></p></blockquote><p>但し、ユーザーアカウントを表すオブジェクトはなく、api 経由でクラスターにユーザーを追加したりはできない。ユーザーは存在しないけど、認証はできる仕組みを私は知らないので関心がある。それはまた今度調べるとして、今回は service account の権限を変更した。service account は pod 内から k8s の api サーバーにアクセスするときの認証などに使われる。デフォルトの権限だと、api サーバーのエンドポイントへの書き込み権限がない (?) ようなので追加する。</p><blockquote><p>In order from most secure to least secure, the approaches are:</p><ol><li>Grant a role to an application-specific service account (best practice)</li><li>Grant a role to the &ldquo;default&rdquo; service account in a namespace</li><li>Grant a role to all service accounts in a namespace</li><li>Grant a limited role to all service accounts cluster-wide (discouraged)</li><li>Grant super-user access to all service accounts cluster-wide (strongly discouraged)</li></ol><p><a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions>ServiceAccount permissions</a></p></blockquote><p>セキュアな順番として上から自分たちの環境にあうかどうかを判断すればよい。私の場合、わざわざ新規に service account を作るほどの要件ではないため、2番目の <code>default</code> の service account にロールを与えることにした。<code>RoleBinding</code> というキーワードがあるので既存のクラスターロールから <code>edit</code> という権限を与える。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl create rolebinding default-edit-role --clusterrole<span style=color:#f92672>=</span>edit --serviceaccount<span style=color:#f92672>=</span>default:default --namespace default
</span></span><span style=display:flex><span>rolebinding.rbac.authorization.k8s.io/default-edit-role created
</span></span></code></pre></div><p>ここでは <code>default-edit-role</code> という RoleBinding を作成した。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get rolebinding default-edit-role -o yaml
</span></span><span style=display:flex><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style=display:flex><span>kind: RoleBinding
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  creationTimestamp: <span style=color:#e6db74>&#34;2022-08-08T08:27:12Z&#34;</span>
</span></span><span style=display:flex><span>  name: default-edit-role
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  resourceVersion: <span style=color:#e6db74>&#34;152660975&#34;</span>
</span></span><span style=display:flex><span>  uid: 6de13b71-3103-448c-805a-66e9400f61c3
</span></span><span style=display:flex><span>roleRef:
</span></span><span style=display:flex><span>  apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>  kind: ClusterRole
</span></span><span style=display:flex><span>  name: edit
</span></span><span style=display:flex><span>subjects:
</span></span><span style=display:flex><span>- kind: ServiceAccount
</span></span><span style=display:flex><span>  name: default
</span></span><span style=display:flex><span>  namespace: default
</span></span></code></pre></div><p>これで cronjob から job を作成する権限が付与された。<a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/#write-access-for-endpoints>Write access for Endpoints</a> によると、新規に作成した v1.22 以降のクラスターではエンドポイントに対する write アクセス権限が異なるように記載されている。一方で v1.22 にアップグレードしたクラスターは影響を受けないとも記述されている。私がいま運用しているクラスターは v1.21 から v1.22 にアップグレードしたクラスターなので <code>edit</code> ロールでうまくいったのかもしれない。クラスターのバージョンによって設定が異なるかもしれない。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0807/>wiremock を使った kubernetes モックサーバーのテスト拡張</a></h1><div class=post-meta><span class=post-date>2022-08-07</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/java/>java</a>&nbsp;
#<a href=/diary/tags/testing/>testing</a>&nbsp;</span><div class=post-content><p>2時に寝て7時に起きて9時までだらだらしてた。</p><h2 id=kubernetes-clients-のライブラリ実装>Kubernetes Clients のライブラリ実装</h2><p><a href=/diary/posts/2022/0806/#kubernetes-clients-のサンプル実装>昨日の続き</a> 。ある程度、クライアントの振る舞いを確認できたので自前のライブラリを作ることにした。ライブラリなのでテストをちゃんと書きたいと思って単体テストのやり方を調べてたら Kubernetes Clients のリポジトリにも単体テストはほとんどなくて、どうも e2e テストの方を重視しているようにもみえた。github issues を検索してみたら次の issue をみつけた。</p><ul><li><a href=https://github.com/apache/submarine/issues/956>[DESIGN] Add k8s mock client and server test case #956</a></li></ul><p>なにかしら単体テストの仕組みを作った方がいいんじゃないかという提案と一緒に issue の作者？かどうかはわからんけど、<a href=https://wiremock.org/>wiremock</a> を使ったテストのサンプルコードをあげていた。名前だけは聞いたことがあったけど、過去に使ったこともなく、どういうものか全くわかってない。ドキュメントを軽く読んでみたら http モックサーバーらしい。issue の内容を参考にしながら wiremock のドキュメントをみて junit5 のテスト拡張を書いてみた。これが適切な実装かはあまり自信がないけど、こんな感じでモックサーバーとモッククライアントの junit5 のテスト拡張を実装した。これは static なモックサーバーの設定になるので wiremock 自体の起動コストは速く感じた。ライブラリのテストとしては申し分ない。いまのところは自画自賛。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span><span style=color:#a6e22e>@Documented</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@Target</span><span style=color:#f92672>(</span>ElementType<span style=color:#f92672>.</span><span style=color:#a6e22e>FIELD</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@Retention</span><span style=color:#f92672>(</span>RetentionPolicy<span style=color:#f92672>.</span><span style=color:#a6e22e>RUNTIME</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>public</span> <span style=color:#a6e22e>@interface</span> KubernetesApiClient <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span><span style=color:#66d9ef>public</span> <span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SetupKubernetesWireMock</span> <span style=color:#66d9ef>implements</span> BeforeAllCallback<span style=color:#f92672>,</span> BeforeEachCallback<span style=color:#f92672>,</span> ExtensionContext<span style=color:#f92672>.</span><span style=color:#a6e22e>Store</span><span style=color:#f92672>.</span><span style=color:#a6e22e>CloseableResource</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>private</span> <span style=color:#66d9ef>static</span> <span style=color:#66d9ef>final</span> Logger logger <span style=color:#f92672>=</span> LogManager<span style=color:#f92672>.</span><span style=color:#a6e22e>getLogger</span><span style=color:#f92672>(</span>SetupKubernetesWireMock<span style=color:#f92672>.</span><span style=color:#a6e22e>class</span><span style=color:#f92672>.</span><span style=color:#a6e22e>getName</span><span style=color:#f92672>());</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>private</span> <span style=color:#66d9ef>static</span> <span style=color:#66d9ef>int</span> PORT <span style=color:#f92672>=</span> 8384<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>private</span> <span style=color:#66d9ef>static</span> WireMockServer wireMockServer <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> WireMockServer<span style=color:#f92672>(</span>options<span style=color:#f92672>().</span><span style=color:#a6e22e>port</span><span style=color:#f92672>(</span>PORT<span style=color:#f92672>));</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>private</span> <span style=color:#66d9ef>static</span> <span style=color:#66d9ef>boolean</span> started <span style=color:#f92672>=</span> <span style=color:#66d9ef>false</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>private</span> ApiClient apiClient<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@Override</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>beforeAll</span><span style=color:#f92672>(</span>ExtensionContext context<span style=color:#f92672>)</span> <span style=color:#66d9ef>throws</span> Exception <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>(!</span>started<span style=color:#f92672>)</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>            wireMockServer<span style=color:#f92672>.</span><span style=color:#a6e22e>start</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>            started <span style=color:#f92672>=</span> <span style=color:#66d9ef>true</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>configureMockClient</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>            var basePath <span style=color:#f92672>=</span> String<span style=color:#f92672>.</span><span style=color:#a6e22e>format</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;http://localhost:%d&#34;</span><span style=color:#f92672>,</span> wireMockServer<span style=color:#f92672>.</span><span style=color:#a6e22e>port</span><span style=color:#f92672>());</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>apiClient</span> <span style=color:#f92672>=</span> ClientBuilder<span style=color:#f92672>.</span><span style=color:#a6e22e>standard</span><span style=color:#f92672>().</span><span style=color:#a6e22e>setBasePath</span><span style=color:#f92672>(</span>basePath<span style=color:#f92672>).</span><span style=color:#a6e22e>build</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>            logger<span style=color:#f92672>.</span><span style=color:#a6e22e>info</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;started kubernetes wiremock: {}&#34;</span><span style=color:#f92672>,</span> basePath<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>            context<span style=color:#f92672>.</span><span style=color:#a6e22e>getRoot</span><span style=color:#f92672>().</span><span style=color:#a6e22e>getStore</span><span style=color:#f92672>(</span>GLOBAL<span style=color:#f92672>).</span><span style=color:#a6e22e>put</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;SetupKubernetesWireMock&#34;</span><span style=color:#f92672>,</span> <span style=color:#66d9ef>this</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@Override</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>beforeEach</span><span style=color:#f92672>(</span>ExtensionContext context<span style=color:#f92672>)</span> <span style=color:#66d9ef>throws</span> Exception <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> <span style=color:#f92672>(</span>var instance <span style=color:#f92672>:</span> context<span style=color:#f92672>.</span><span style=color:#a6e22e>getRequiredTestInstances</span><span style=color:#f92672>().</span><span style=color:#a6e22e>getAllInstances</span><span style=color:#f92672>())</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> <span style=color:#f92672>(</span>var field <span style=color:#f92672>:</span> instance<span style=color:#f92672>.</span><span style=color:#a6e22e>getClass</span><span style=color:#f92672>().</span><span style=color:#a6e22e>getDeclaredFields</span><span style=color:#f92672>())</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> <span style=color:#f92672>(</span>field<span style=color:#f92672>.</span><span style=color:#a6e22e>isAnnotationPresent</span><span style=color:#f92672>(</span>KubernetesApiClient<span style=color:#f92672>.</span><span style=color:#a6e22e>class</span><span style=color:#f92672>))</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>                    field<span style=color:#f92672>.</span><span style=color:#a6e22e>setAccessible</span><span style=color:#f92672>(</span><span style=color:#66d9ef>true</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>                    field<span style=color:#f92672>.</span><span style=color:#a6e22e>set</span><span style=color:#f92672>(</span>instance<span style=color:#f92672>,</span> <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>apiClient</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>                <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@Override</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>close</span><span style=color:#f92672>()</span> <span style=color:#66d9ef>throws</span> Throwable <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>        wireMockServer<span style=color:#f92672>.</span><span style=color:#a6e22e>stop</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>private</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>configureMockClient</span><span style=color:#f92672>()</span> <span style=color:#66d9ef>throws</span> IOException <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>// add static stubs for mock client
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        configureFor<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;localhost&#34;</span><span style=color:#f92672>,</span> PORT<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>stubForBatchGetCronjobs</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>stubForBatchGetSingleCronJob</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;my-job1&#34;</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>stubForBatchGetSingleCronJob</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;my-job2&#34;</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>stubForBatchGetSingleCronJob</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;my-job3&#34;</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>stubForBatchPostJob</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>stubForBatchGetJob</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>private</span> <span style=color:#66d9ef>byte</span><span style=color:#f92672>[]</span> <span style=color:#a6e22e>getContents</span><span style=color:#f92672>(</span>String name<span style=color:#f92672>)</span> <span style=color:#66d9ef>throws</span> IOException <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>        var json <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> File<span style=color:#f92672>(</span><span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>getClass</span><span style=color:#f92672>().</span><span style=color:#a6e22e>getResource</span><span style=color:#f92672>(</span>name<span style=color:#f92672>).</span><span style=color:#a6e22e>getPath</span><span style=color:#f92672>());</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> Files<span style=color:#f92672>.</span><span style=color:#a6e22e>readAllBytes</span><span style=color:#f92672>(</span>Paths<span style=color:#f92672>.</span><span style=color:#a6e22e>get</span><span style=color:#f92672>(</span>json<span style=color:#f92672>.</span><span style=color:#a6e22e>getPath</span><span style=color:#f92672>()));</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>private</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>stubForBatchGetCronjobs</span><span style=color:#f92672>()</span> <span style=color:#66d9ef>throws</span> IOException <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>        var contents <span style=color:#f92672>=</span> <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>getContents</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;/fixtures/kubernetes/batch/cronjobs.json&#34;</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        var path <span style=color:#f92672>=</span> urlPathEqualTo<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;/apis/batch/v1/namespaces/default/cronjobs&#34;</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        stubFor<span style=color:#f92672>(</span>get<span style=color:#f92672>(</span>path<span style=color:#f92672>).</span><span style=color:#a6e22e>willReturn</span><span style=color:#f92672>(</span>aResponse<span style=color:#f92672>().</span><span style=color:#a6e22e>withStatus</span><span style=color:#f92672>(</span>200<span style=color:#f92672>).</span><span style=color:#a6e22e>withBody</span><span style=color:#f92672>(</span>contents<span style=color:#f92672>)));</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>private</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>stubForBatchGetSingleCronJob</span><span style=color:#f92672>(</span>String jobName<span style=color:#f92672>)</span> <span style=color:#66d9ef>throws</span> IOException <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>        var contents <span style=color:#f92672>=</span> <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>getContents</span><span style=color:#f92672>(</span>String<span style=color:#f92672>.</span><span style=color:#a6e22e>format</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;/fixtures/kubernetes/batch/%s.json&#34;</span><span style=color:#f92672>,</span> jobName<span style=color:#f92672>));</span>
</span></span><span style=display:flex><span>        var url <span style=color:#f92672>=</span> String<span style=color:#f92672>.</span><span style=color:#a6e22e>format</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;/apis/batch/v1/namespaces/default/cronjobs/%s&#34;</span><span style=color:#f92672>,</span> jobName<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        var path <span style=color:#f92672>=</span> urlPathEqualTo<span style=color:#f92672>(</span>url<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        stubFor<span style=color:#f92672>(</span>get<span style=color:#f92672>(</span>path<span style=color:#f92672>).</span><span style=color:#a6e22e>willReturn</span><span style=color:#f92672>(</span>aResponse<span style=color:#f92672>().</span><span style=color:#a6e22e>withStatus</span><span style=color:#f92672>(</span>200<span style=color:#f92672>).</span><span style=color:#a6e22e>withBody</span><span style=color:#f92672>(</span>contents<span style=color:#f92672>)));</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>private</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>stubForBatchPostJob</span><span style=color:#f92672>()</span> <span style=color:#66d9ef>throws</span> IOException <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>        var name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;/fixtures/kubernetes/batch/post-my-job.json&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        var contents <span style=color:#f92672>=</span> <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>getContents</span><span style=color:#f92672>(</span>name<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        var url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;/apis/batch/v1/namespaces/default/jobs&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        var path <span style=color:#f92672>=</span> urlPathEqualTo<span style=color:#f92672>(</span>url<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        stubFor<span style=color:#f92672>(</span>post<span style=color:#f92672>(</span>path<span style=color:#f92672>).</span><span style=color:#a6e22e>willReturn</span><span style=color:#f92672>(</span>aResponse<span style=color:#f92672>().</span><span style=color:#a6e22e>withStatus</span><span style=color:#f92672>(</span>200<span style=color:#f92672>).</span><span style=color:#a6e22e>withBody</span><span style=color:#f92672>(</span>contents<span style=color:#f92672>)));</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>private</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>stubForBatchGetJob</span><span style=color:#f92672>()</span> <span style=color:#66d9ef>throws</span> IOException <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>        var name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;/fixtures/kubernetes/batch/get-my-job.json&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        var contents <span style=color:#f92672>=</span> <span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>getContents</span><span style=color:#f92672>(</span>name<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        var url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;/apis/batch/v1/namespaces/default/jobs/my-job&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        var path <span style=color:#f92672>=</span> urlPathEqualTo<span style=color:#f92672>(</span>url<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        stubFor<span style=color:#f92672>(</span>get<span style=color:#f92672>(</span>urlPathEqualTo<span style=color:#f92672>(</span>url<span style=color:#f92672>)).</span><span style=color:#a6e22e>willReturn</span><span style=color:#f92672>(</span>aResponse<span style=color:#f92672>().</span><span style=color:#a6e22e>withStatus</span><span style=color:#f92672>(</span>200<span style=color:#f92672>).</span><span style=color:#a6e22e>withBody</span><span style=color:#f92672>(</span>contents<span style=color:#f92672>)));</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0806/>Kubernetes Clients のサンプル実装</a></h1><div class=post-meta><span class=post-date>2022-08-06</span></div><span class=post-tags>#<a href=/diary/tags/life/>life</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>23時に寝て7時半に起きた。夜中も2回ぐらい起きる。暑さでまいってきた。</p><h2 id=ストレッチ>ストレッチ</h2><p>今日の開脚幅は開始前159cmで、ストレッチ後162cmだった。数字は悪くない。いつもはストレッチを受けていると疲労しているところが伸びることで体が軽くなっていく感覚があるのだけど、今日は体全体がだるくてストレッチを受けていてもなんかしんどいなぁとだるさを感じていた。コロナに感染してないと思うけど、夏バテの状態をそのままストレッチにも持ち込んだような感覚があった。腰の張りや肩甲骨の硬さなどが少し気になったかな。トレーナーさんには立ったときの姿勢が少し前よりで重心のバランスがよくないといったアドバイスをされた。とくにどこが悪いというわけでもないのになんかしんどい。</p><h2 id=kubernetes-clients-のサンプル実装>Kubernetes Clients のサンプル実装</h2><p><a href=/diary/posts/2022/0804/>Kubernetes Clients の調査</a> の続き。java クライアントを使って minikube でいくつか動かしてみた。openapi で生成した rest api クライントが提供されている。デフォルト設定でも minikube で普通に動いたのでおそらく裏で <code>$HOME/.kube/config</code> をみたり <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code> を読み込んで認証ヘッダーに設定してくれたりするのだと推測する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span><span style=color:#66d9ef>public</span> ApiClient <span style=color:#a6e22e>getKubernetesClient</span><span style=color:#f92672>()</span> <span style=color:#66d9ef>throws</span> IOException <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    var client <span style=color:#f92672>=</span> Config<span style=color:#f92672>.</span><span style=color:#a6e22e>defaultClient</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>    io<span style=color:#f92672>.</span><span style=color:#a6e22e>kubernetes</span><span style=color:#f92672>.</span><span style=color:#a6e22e>client</span><span style=color:#f92672>.</span><span style=color:#a6e22e>openapi</span><span style=color:#f92672>.</span><span style=color:#a6e22e>Configuration</span><span style=color:#f92672>.</span><span style=color:#a6e22e>setDefaultApiClient</span><span style=color:#f92672>(</span>client<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> client<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><p>このクライアントを使って <a href=https://github.com/kubernetes-client/java/tree/master/kubernetes/docs>java/kubernetes/docs/</a> 配下にある api インスタンスを生成する。例えば、cronjob や job を扱うならば <code>BatchV1Api</code> というドキュメントがある。BatchApi だけでも3つのドキュメントがあるのでちょっとやり過ぎな気もする。</p><ul><li>BatchApi.md</li><li>BatchV1Api.md</li><li>BatchV1beta1Api.md</li></ul><p>kubectl コマンドで使う cronjob から手動で job を設定するのを実装してみる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl create job --from<span style=color:#f92672>=</span>cronjob/my-schedule-job my-manual-job
</span></span></code></pre></div><p>細かい設定はちゃんと調べないといけないけど、一応はこれで動いた。cronjob のオブジェクトを取得して job のオブジェクトを生成して create するだけ。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span>var api <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> BatchV1Api<span style=color:#f92672>(</span><span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>getKubernetesClient</span><span style=color:#f92672>());</span>
</span></span><span style=display:flex><span>var cronJob <span style=color:#f92672>=</span> api<span style=color:#f92672>.</span><span style=color:#a6e22e>readNamespacedCronJob</span><span style=color:#f92672>(</span>cronJobName<span style=color:#f92672>,</span> NAMESPACE_DEFAULT<span style=color:#f92672>,</span> <span style=color:#66d9ef>null</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>var ann <span style=color:#f92672>=</span> Map<span style=color:#f92672>.</span><span style=color:#a6e22e>of</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;cronjob.kubernetes.io/instantiate&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;manual&#34;</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>var metadata <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> V1ObjectMeta<span style=color:#f92672>().</span><span style=color:#a6e22e>name</span><span style=color:#f92672>(</span>newJobName<span style=color:#f92672>).</span><span style=color:#a6e22e>annotations</span><span style=color:#f92672>(</span>ann<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>var spec <span style=color:#f92672>=</span> cronJob<span style=color:#f92672>.</span><span style=color:#a6e22e>getSpec</span><span style=color:#f92672>().</span><span style=color:#a6e22e>getJobTemplate</span><span style=color:#f92672>().</span><span style=color:#a6e22e>getSpec</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>spec<span style=color:#f92672>.</span><span style=color:#a6e22e>setTtlSecondsAfterFinished</span><span style=color:#f92672>(</span>10<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>var job <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> V1Job<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>apiVersion</span><span style=color:#f92672>(</span>cronJob<span style=color:#f92672>.</span><span style=color:#a6e22e>getApiVersion</span><span style=color:#f92672>())</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>kind</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Job&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>spec</span><span style=color:#f92672>(</span>cronJob<span style=color:#f92672>.</span><span style=color:#a6e22e>getSpec</span><span style=color:#f92672>().</span><span style=color:#a6e22e>getJobTemplate</span><span style=color:#f92672>().</span><span style=color:#a6e22e>getSpec</span><span style=color:#f92672>())</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>metadata</span><span style=color:#f92672>(</span>metadata<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>var result <span style=color:#f92672>=</span> api<span style=color:#f92672>.</span><span style=color:#a6e22e>createNamespacedJob</span><span style=color:#f92672>(</span>NAMESPACE_DEFAULT<span style=color:#f92672>,</span> job<span style=color:#f92672>,</span> <span style=color:#66d9ef>null</span><span style=color:#f92672>,</span> <span style=color:#66d9ef>null</span><span style=color:#f92672>,</span> <span style=color:#66d9ef>null</span><span style=color:#f92672>,</span> <span style=color:#66d9ef>null</span><span style=color:#f92672>);</span>
</span></span></code></pre></div><p>手動で作成した job の pod は終了後にゴミとして残ってしまうので ttl を設定すれば自動的に削除できることに気付いた。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span>spec<span style=color:#f92672>.</span><span style=color:#a6e22e>setTtlSecondsAfterFinished</span><span style=color:#f92672>(</span>10<span style=color:#f92672>);</span>
</span></span></code></pre></div><p>k8s クラスターの内部、つまり pod 内からリクエストするには <a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions>ServiceAccount permissions</a> を適切に設定しないといけない。ひとまずローカルの minikube で super user 権限にしたらリクエストはできた。実運用では適切なロールを定義して適切に権限設定しないといけない。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl create clusterrolebinding serviceaccounts-cluster-admin <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --clusterrole<span style=color:#f92672>=</span>cluster-admin <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --group<span style=color:#f92672>=</span>system:serviceaccounts
</span></span></code></pre></div></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0804/>Kubernetes Clients の調査</a></h1><div class=post-meta><span class=post-date>2022-08-04</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>23時に寝て6時に起きた。</p><h2 id=k8s-cronjob-の手動実行>k8s cronjob の手動実行</h2><p>いろんな定期／バッチ処理を k8s の cronjob に置き換えつつある。これまでアプリケーションサーバーでスケジュール実行していたものも本来サーバーである必要はないのでサーバーアプリケーションから cli アプリケーションに移行したりしている。そうやって定期実行ジョブが増えてくると、今度は調査やデバッグ目的で任意のタイミングで実行したくなる。kubectl を使って次のように実行できる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl create job --from<span style=color:#f92672>=</span>cronjob/my-schedule-job my-manual-job
</span></span></code></pre></div><p>この cli を実行すると、cronjob のマニフェストから <code>my-manual-job</code> というジョブの pod が生成されて実行される。開発者ならこれでよいのだけど、非開発者も調査や検証目的で実行したい。そのためには非開発者向けのインターフェースを作らないといけない。本当は chatops のように slack apps によるコマンド実行ができるとカッコよいのだけど、k8s クラスターと slack 間の認証やセキュリティの仕組みを作る必要があって、既存の仕組みがないならそこはセキュリティリスクにも成り得るのでちょっと控えたい。そうすると、既存のサーバーアプリケーションの web api のインターフェースで提供できるようにしたい。複数の言語向けに <a href=https://github.com/kubernetes-client>Kubernetes Clients</a> が提供されている。これを使って cronjob の手動実行を実装できそうな気がする。時間があれば週末に軽く調べてみようと思う。</p><ul><li>python</li><li>c#</li><li>javascript</li><li>java</li><li>c</li><li>haskel</li><li>go</li><li>ruby</li></ul></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0720/>ノードグループと nodeSelector</a></h1><div class=post-meta><span class=post-date>2022-07-20</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>1時に寝て6時に起きた。</p><h2 id=eks-クラスターのノードグループと-k8s-の-nodeselector>EKS クラスターのノードグループと k8s の nodeSelector</h2><p>先日 <a href=/diary/posts/2022/0711/>k8s の nodeSelector</a> の調査について書いた。いまテスト環境でその調査結果の運用検証中。当初は k8s の機能だけを使いたいと考えていたが、いま eksctl コマンドで EKS クラスターを管理していて、k8s ノードの実体である ec2 のプロビジョニングはノードグループがもつ起動テンプレートとオートスケールポリシーにより制御される。そのため、ノードグループを分割してそれぞれにノードラベルが必ず付与されるように管理する方が簡単だとわかってきた。要はアプリケーションサーバー向けのノードグループとバッチ処理向けのノードグループの2つを作った。あと覚えておくとよいのが、<a href=https://eksctl.io/usage/iam-policies/#adding-a-custom-instance-role>Adding a custom instance role</a> で任意のポリシーもノードグループの iam ロールに追加できる。設定例としてはこんな感じ。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span>  <span style=color:#f92672>iam</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>attachPolicyARNs</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>arn:aws:iam::${accountId}:policy/my-custom-policy</span>
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly</span>
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy</span>
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy</span>
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore</span>
</span></span></code></pre></div><p>ノードグループの準備が整ったら nodeSelector を指定した pod をデプロイするだけ。k8s ノードがどんなノードラベルをもっているかは次のようにして確認できる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get node --show-labels
</span></span></code></pre></div><p>意図したノードラベルが付与された k8s ノードに pod がデプロイされたかどうかは次のようにして確認できる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get pod --output wide --field-selector spec.nodeName<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>ノードラベルをもつノード名<span style=color:#e6db74>}</span>
</span></span></code></pre></div><p>これらの環境構築、検証、wiki にドキュメントを書いて本番作業手順もまとめた。一通りきれいにまとまったインフラ作業を完遂できて気分がよい。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0711/>nodeSelector を試す</a></h1><div class=post-meta><span class=post-date>2022-07-11</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>4時に寝て8時に起きた。久しぶりに寝坊した。</p><h2 id=k8s-の-nodeselector>k8s の nodeSelector</h2><p>先日 <a href=/diary/posts/2022/0706/>定期/バッチ処理を k8s の cronjob にすべて移行</a> した。すでに本番運用もしていて調子もよさそうにみえる。あと残課題としてバッチ処理とアプリケーションサーバーの pod がデプロイされる k8s ノードを分割したい。現時点では、バッチ処理の負荷は小さいから同じスペックのインスタンスの k8s ノード上で混在させて運用している。しかし、いずれ運用上の問題になる懸念がある。そのため、バッチ処理のみを実行する k8s ノードを管理したい。次のドキュメントに書いてある。</p><ul><li><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/>Assigning Pods to Nodes</a></li></ul><p>k8s のドキュメントによると、大きく分けて nodeSelector と Affinity という2つのやり方がある。前者はラベルでフィルターするシンプルな仕組み、後者はさらに複雑な要件に対応するもの。いまのところ、ただ分割できればよいのでシンプルな nodeSelector で実装してみることにした。</p><ul><li>nodeSelector</li><li>Affinity and anti-affinity</li></ul><p>余談だが、nodeSelector はいずれ Affinity に置き換わるので deprecated だと一時期ドキュメントに書かれていたらしい。具体的に決まっていることでもないため、<a href=https://github.com/kubernetes/kubernetes/issues/82184>nodeSelector: when will it be deprecated? #82184</a> によると deprecated という文言を含む文章がその後に削除された。Affinity は高機能且つ高コストであることから、(現時点では) nodeSelector はシンプルで推奨すべき方法とまで書いてあるのですぐになくなるわけではなさそう。</p><p>minikube で nodeSelector の検証を始めたんだけど、いくつかうまくいかないことがあって断念した。multi-node 機能を使って controll plane と worker ノードの2つを起動できたけど、worker ノードから docker host にアクセスできなかった。何かしら設定が必要なのか、別途レジストリが必要になるのかよくわからなかった。あと node にラベル設定したときに worker ノードにラベル設定しても minikube を再起動するとそのラベルが消えてしまっていて保持されないようだった。ちょっと調べてローカルの環境を作るのが面倒になったので早々に断念した。</p><ul><li><a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>Using Multi-Node Clusters</a></li></ul></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0706/>lambda から cronjob へ</a></h1><div class=post-meta><span class=post-date>2022-07-06</span></div><span class=post-tags>#<a href=/diary/tags/cdk/>cdk</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>0時に6時に起きた。今日は晴れたので自転車通勤。</p><h2 id=優雅にドキュメントを書きながら障害対応>優雅にドキュメントを書きながら障害対応</h2><p>昨日は凸凹しながら乗り切ってサービスイン2日目。今日から外部システムとの連携なども絡んでくる。昨日の今日なんで何か起こるだろうと思いつつ、暇だったらドキュメント書くタスクがいくつも溜まっているのでそれを片付けるかと業務を始めた。私ぐらいの人間になると、いつ凸凹が発生してもよいように、この日のために取っておいたようなドキュメントタスクがいくつも溜まっている。午前中に私の出番はなく、優雅にドキュメントの1つを完成させた。</p><p>以前から定期実行やバッチ処理は lambda 関数をトリガーに作られていた。それらを <a href=/diary/posts/2022/0609/#serverless-framework-から-cdk-移行の背景>serverless framework から cdk へ移行した</a> んだけど、その後にバッチ処理を <a href=/diary/posts/2022/0622/>k8s の cronjob で実装した</a> 。この cronjob が思いの外、うまくいって、私がフルスクラッチで cli を作っているのだから、私からみてさいきょうのばっちしょりの土台を実装している。定期実行もすべて cronjob でやればいいやんと気付いて、過去に lambda 関数 (cdk/python) で実装したものをすべて移行することに決めた。昨日の流れからわかるように過去に作成済みの一連の lambda 関数はまだ本番環境にデプロイされていない。<a href=/diary/posts/2022/0704/>m1 chip macbook 問題</a> で同僚のマシンからデプロイできないという不運もあったんだけど、もうデプロイしなくていいよ、すべて cronjob で置き換えるからと伝えて2つ移行した。あと半日もあれば完了できそうな見通し。</p><p>lambda 関数から cronjob への移行作業をしていると、本番環境でのバッチ処理の一部のロジックが誤っているとわかってそれを修正したり、当然のようにテスト環境のデータも誤っているので正しいかどうかは本番環境にデプロイするしかないみたいな凸凹した状況を横切りながら本日の作業を終えた。いくつか残課題は残っているものの、明日中には平常業務に戻れるぐらいの状況にはなりつつあるのかもしれない。サービスイン2日目を終えて致命的な問題は起こっていないようにみえる。ひとまずはよかったという感じ。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0629/>cdk で既存の eks クラスターを管理すべきか</a></h1><div class=post-meta><span class=post-date>2022-06-29</span></div><span class=post-tags>#<a href=/diary/tags/cdk/>cdk</a>&nbsp;
#<a href=/diary/tags/infrastructure/>infrastructure</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>0時に寝て6時に起きた。</p><h2 id=cdk-から既存の-eks-クラスターを制御する>cdk から既存の eks クラスターを制御する</h2><p>1ヶ月ほど前に検証していた <a href=/diary/posts/2022/0518/#cdk-のパッチ検証>cdk による eks クラスターの helm 管理</a> を再検証した。kubectlRoleArn にどういった権限をもつ iam role を設定したらよいかがよくわからなくて苦労していた。最終的にそれが理解できて helm 管理もできるようになったのでまとめておく。</p><blockquote><p>kubectlRoleArn - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually. The trust policy of this role should include the the arn:aws::iam::${accountId}:root principal in order to allow the execution role of the kubectl resource to assume it.</p><p><a href=https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters>https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters</a></p></blockquote><p>aws-auth の configmap に設定されている system:masters に所属している iam role を調べる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl describe configmap -n kube-system aws-auth
</span></span></code></pre></div><p>この iam role には <code>sts:AssumeRole</code> 権限を与え、trust relationships に <code>arn:aws:iam::${accountId}:root</code> といった root ユーザーを含める必要がある。この root ユーザーの設定がないと次のような権限エラーが発生する。この権限エラーの修正方法がわからなくて苦労していた。結果的には関係なかった kubectlLambdaRole の設定も必要なんじゃないかと検証していたのが前回の作業の中心だった。</p><pre tabindex=0><code>An error occurred (AccessDenied) when calling the AssumeRole operation:
  User: arn:aws:sts::${accountId}:assumed-role/xxx is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::${accountId}:role/myrole
Error: Kubernetes cluster unreachable: Get &#34;https://xxx.gr7.ap-northeast-1.eks.amazonaws.com/version
</code></pre><p>ようやく cdk で既存の eks クラスターをインポートして helm パッケージを管理できるようになった。とはいえ、cdk/cf の実行時間を測ってみると次のようになった。</p><ul><li>helm パッケージの新規インストール: 約5分</li><li>helm パッケージのアンインストール: 約25分</li></ul><p>これは cdk が helm パッケージを管理するための lambda 環境を構築/削除するときの時間になる。cdk はアプリケーションの stack から nested stack を作成して、そこに lambda や iam role などをまとめて作成する。一度作成してしまえば、バージョンのアップグレードは30秒ほどで完了した。</p><p>この振る舞いを検証した上で、cdk で eks クラスターをインポートする管理はやめようとチームに提案した。正しい設定を作ってしまえば運用は楽になると言える一面もあるが、新規に helm パッケージを追加するときのちょっとした typo や設定ミスなどがあると、1回の試行に30分かかる。私がこの検証に1週間以上のデバッグ時間を割いている理由がそれに相当する。お手伝い先の運用ではテスト/本番環境ともにローカルから接続できる状態なので helm コマンドを直接実行した方が遥かに管理コストや保守コストを下げると言える。cdk を使って嬉しいことは helm コマンドでわかるバージョン情報と設定内容が cdk のコードとして管理されているぐらいでしかない。ドキュメントと helm コマンドで管理する方が現状ではよいだろうと私は結論付けた。同じような理由で eks クラスターも cdk ではなく eksctl コマンドで管理されている。</p><p>1週間以上の労力と時間を費やしてやらない方がよいとわかったという、一般的には失敗と呼ばれる作業に終わったわけだけど、eks/cdk の勉強にはなった。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0625/>k8s のアップグレードをやってみた</a></h1><div class=post-meta><span class=post-date>2022-06-25</span></div><span class=post-tags>#<a href=/diary/tags/life/>life</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/event/>event</a>&nbsp;</span><div class=post-content><p>0時に寝て6時半に起きた。起きてから1時間ほどだらだらしてた。</p><h2 id=ストレッチ>ストレッチ</h2><p>今日の開脚幅は開始前161cmで、ストレッチ後163cmだった。先週と同じなので現状維持とも言えるし、よい状態を維持しているとも言えるかもしれない。もう1年以上通っているせいか、なにかポイントが溜まっていて使わないといけないという話しで今日は20分延長でやってくれた。とは言っても、基本的なストレッチ項目が変わるわけではなく、いつもより伸ばす時間や手順が少し増えているぐらいだった気がする。今週はとくに腰の負荷もあまり感じなかったせいか、いつもの右腰の張りもなかったように思う。トレーナーさんに聞くと、暑くなると筋肉は伸びやすくなるので季節要因でストレッチをしたときの伸び具合が変わるのは普通とのこと。調子がよくなってきたのでこのまま好調を維持したい。</p><h2 id=eks-k8s-のアップグレード>eks (k8s) のアップグレード</h2><p>お手伝い先のお仕事がもうすぐサービスインなのでそれまでにリスクのある作業をやっとこうみたいな状況にある。たまたま eks (k8s) のバージョンを 1.21 から 1.22 にあげようと思い立って、木曜日に提案したら、どんな障害が起きるかわからないので他メンバーがテスト環境を使っていない時間帯で作業した方がよいだろうという話になって土日にやることにした。</p><ul><li><a href=https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html>Updating an Amazon EKS cluster Kubernetes version</a></li></ul><p>何が起きるか分からなくても、土曜日から始めて致命的なトラブルに見舞われても1日もあれば解決できるだろうという見通しで作業を始めた。その見通しも「私がやれば」という前提に成り立っている。良くも悪くも私がやろうと言ったことに反対されることはほとんどないが、それは私が言ったことは一定時間に私がすべてやり切るという信頼に基づいている。本当の意味でできるかどうか分からないことを必要以上に抱え込んでしまうときもあるのでバランス感覚は必要かもしれない。言わばサービス休日出勤だし、なぜ私がやっているかと言うと、システムの運用や保守の展望を考えたら、サービスインの前にインフラのバージョンを上げておく方が将来の保守コストを下げることに繋がるという1点のみに重要性を見い出していて、それをもっとも強く主張しているのが私だからという理由。</p><p>結論から言って2時間でアップグレード作業を完了した。1つ手順漏れがあって、アプリケーションの pod がすべてエラーになるというトラブルに見舞われたものの、すぐ手順漏れに気付いて難なく復旧できた。今日はテスト環境のアップグレードをしたわけだけど、また後日、本番向けの作業手順書を作れば、ほぼタウンタイムなしで1時間もあればアップグレード作業を完了できそうな見通しではある。</p><p>実際はミスもあったので次の順番でやったわけではないが、おそらくこの手順でやれば正しいはず。</p><ol><li>aws cli と eksctl コマンドのインストール</li><li>aws のアップグレードドキュメンを読む</li><li>cert-manager のアップグレード (1.1.1 から 1.5.4)</li><li>aws-load-balancer-controller のアップグレード (2.2.0 から 2.4.2)</li><li>k8s control plane のアップグレード (1.21 から 1.22)</li><li>(オプション: 不要) autoscaler のアップグレード</li><li>(オプション: 不要) gpu サポートノードのアップグレード</li><li>vpc cni プラグインのアップグレード (1.7.5 から 1.11.2)</li><li>coredns プラグインのアップグレード (1.8.4 から 1.8.7)</li><li>kube-proxy のアップグレード (1.21.2 から 1.22.6)</li><li>k8s nodegroup のアップグレード (1.21 から 1.22)<ul><li>k8s ノードが存在する nodegroup をアップグレードするとそのインスタンスが再作成されて pod が再デプロイされる</li></ul></li></ol><p>細かい手順は aws のドキュメントの指示に従いながらやったらできた。add-on と self-managed add-on の種別の違いがあったり、helm と k8s manifest の手順が別々だったり、どのバージョンからのアップグレードかで作業手順が異なったりと、ドキュメントをちゃんと読まないと正しい作業手順がわからない。基本的にはドキュメント通りの作業で完了できた。</p><h2 id=もくもく会>もくもく会</h2><p>アップグレード作業を終えてから1時間ほど残っていたので16時から <a href=https://kobe-sannomiya-dev.connpass.com/event/251117/>【三宮.dev ＆ KELab 共催】もくもく会</a> に参加した。今回は <a href=https://kobe-engr-lab.studio.site/>Kobe Engineers Lab</a> さんと共催ということで <a href=https://120workplace.jp/>120 WORKPLACE KOBE</a> で開催された。Kobe Engineers Lab の主催者の会社が 120 workplace でオフィスを借りているため、会議室を5時間/月まで無料で借りられるという。私も過去に何度か 120 workplace のコワーキングスペースで作業したこともあった。久しぶりに行ってよい場所だとは思う。会議室は初めて入ったけど、10人ぐらいは余裕で作業できる大きなテーブルがあって広くてよかった。終わってからわたなべさんと3時間ほど立ち呑みしてた。</p><h2 id=はんなりビジネス>はんなりビジネス</h2><p>21時から <a href=https://hannari-python.connpass.com/event/250916/>はんなりビジネス #0</a> に参加した。おがわさんがまた新しいことやるんだなと思って興味本位で参加してみた。現実の課題に対してコミュニティの有志を募ってチームで取り組んでみたら、問題解決能力も身についてプログラミングの知識を活かしてより実践的なスキルが身に着いてよいのではないかといったところから始まった企画らしい。今日は初回だったので参加者でどういう取り組みがよいのかを雑談してた。まだまだこの先どうなるかわからないけど、私はあまりこの手の取り組みには懐疑的かなぁ。自分たちにとってちょうどよい課題レベルの対象をみつけるのは難しいし、誰でも参加できるオープンなビジネスコンテストやアイディアソンが本当に大事な問題を扱っているかも怪しい。現実の課題はお仕事でいくらでもあるので、それをコミュニティでやろうと思うとニッチな何かになるか、価値があるかどうかよりも本人がやりたいかどうかの目的になってしまうような気もする。とはいえ、私自身、ビジネス力はまったくないのでなにかしらやっているうちに価値に気付くこともあるかもしれない。もうしばらく様子をみてみる。</p><ul><li><a href=https://chomoku.notion.site/6872dcf2ba8d4f6887cb16f71879f4ca>https://chomoku.notion.site/6872dcf2ba8d4f6887cb16f71879f4ca</a></li></ul></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0622/>k8s の cronjob を検証中</a></h1><div class=post-meta><span class=post-date>2022-06-22</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/cli/>cli</a>&nbsp;</span><div class=post-content><p>0時に寝て6時に起きた。寝不足を解消して体調が戻ってきた。</p><h2 id=k8s-の-cronjob>k8s の cronjob</h2><p>バッチ処理を <a href=https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/>Kubernetes: CronJob</a> で作る。一通り設定して minikube で検証して eks 上でも動くようになった。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>batch/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>CronJob</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app-hourly-job</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>schedule</span>: <span style=color:#e6db74>&#34;5 */1 * * *&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>concurrencyPolicy</span>: <span style=color:#ae81ff>Forbid</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>startingDeadlineSeconds</span>: <span style=color:#ae81ff>600</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>jobTemplate</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>backoffLimit</span>: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>app</span>: <span style=color:#ae81ff>my-app-hourly</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>annotations</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>dapr.io/enabled</span>: <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>dapr.io/app-id</span>: <span style=color:#e6db74>&#34;my-app-hourly&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>          - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app-hourly-job</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>image</span>: <span style=color:#ae81ff>my-app-image</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>imagePullPolicy</span>: <span style=color:#ae81ff>Always</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>env</span>:
</span></span><span style=display:flex><span>            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>BATCH_ENV</span>
</span></span><span style=display:flex><span>              <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;dev&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>command</span>:
</span></span><span style=display:flex><span>            - <span style=color:#e6db74>&#34;/bin/sh&#34;</span>
</span></span><span style=display:flex><span>            - <span style=color:#e6db74>&#34;/app/scripts/my-app.sh&#34;</span>
</span></span><span style=display:flex><span>            - <span style=color:#e6db74>&#34;param1&#34;</span>
</span></span><span style=display:flex><span>            - <span style=color:#e6db74>&#34;param2&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>restartPolicy</span>: <span style=color:#ae81ff>Never</span>
</span></span></code></pre></div><p>command の設定がわかりにくい。さらに k8s のドキュメントのサンプル設定も誤解を招くような例になっている。どうも実行できるのは1つの cli だけで、複数コマンドを指定できるわけではない。シェルスクリプトを docker イメージに含めて、そこで任意のスクリプトを実装した方がよいだろう。</p><ul><li>&ldquo;/bin/sh&rdquo;</li><li>&ldquo;/app/scripts/my-app.sh&rdquo;</li><li>&ldquo;param1&rdquo;</li><li>&ldquo;param2&rdquo;</li></ul><p>この設定は次の cli として実行される。</p><blockquote><p>/bin/sh /app/scripts/my-app.sh param1 param2</p></blockquote><p><a href=https://stackoverflow.com/questions/51657105/how-to-ensure-kubernetes-cronjob-does-not-restart-on-failure>How to ensure kubernetes cronjob does not restart on failure</a> によると、バッチ処理が失敗したときに再実行したくないときは次の3つの設定をする。</p><ul><li>concurrencyPolicy: Forbid</li><li>backoffLimit: 0</li><li>restartPolicy: Never</li></ul><p>restartPolicy が Never 以外だと、エラーが発生すると永遠に再実行されてしまうので障害時に2次被害を増やしてしまう懸念があったような気がする。</p><p>あと、うちの環境は dapr 経由で他の pod サービスと通信しているので dapr を有効にしないと pod 間通信ができない。dapr はデーモンでずっと起動しているからバッチ処理の終了時に daprd も shutdown してやらないといけない。<a href=https://docs.dapr.io/operations/hosting/kubernetes/kubernetes-job/>Running Dapr with a Kubernetes Job</a> にその方法が書いてある。daprd を shutdown しないと、pod のステータスが NotReady のままで Completed にならない。</p><p>まだまだよくわかってないので <a href=https://kubernetes.io/docs/concepts/workloads/controllers/job/>Jobs</a> のドキュメントに一通り目を通そうと思っている。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0524/>法人として消費税を納めた</a></h1><div class=post-meta><span class=post-date>2022-05-24</span></div><span class=post-tags>#<a href=/diary/tags/founding/>founding</a>&nbsp;
#<a href=/diary/tags/tax/>tax</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>5時に寝て7時過ぎに起きた。前日の夜から法人決算の電子申告に取り組み始めた。本当は紙でやるつもりだったんだけど、eltax が快適だったので e-tax も衝動的にやってみたくなった。</p><h2 id=消費税と地方消費税の申告>消費税と地方消費税の申告</h2><p><a href=/diary/posts/2022/0505/#法人決算>法人決算</a> の一部。今回が初めての消費税と地方消費税の申告になる。簡易課税で支払う。</p><blockquote><p>消費税は、国税（国に納付する税金）であり消費税の納税義務がある事業者が納付します。地方消費税とは、 <strong>消費税と同様で商品の販売やサービスの提供などの取引にかかる税金</strong> です。消費税との違いは、 <strong>地方消費税は国税ではなく地方税（都道府県や市町村に納付する税金）という点です。</strong> しかし実際に納付するときは消費税と分けて納付はせずに、 <strong>消費税と一緒に地方消費税を所管税務署へ納付します。</strong></p><p><a href=https://biz.moneyforward.com/tax_return/basic/70/#i>消費税と地方消費税の違いは？納付対象者や納付方法、計算の仕方まで徹底解説！</a></p></blockquote><p>freee で出力した書類をみながら e-tax の画面で同じ書類の項目を埋めていくだけの作業。1つだけバリデーションエラーが発生して、何度やり直しても数値は正しいようにみえるので無視して処理を継続することにした。メッセージにも値が正しければ継続してくださいと書いてあるのでバリデーションがバグっているのだろうと推測する。書類を作成して、署名して、送信して、納付情報が返ってきて、pay-easy で納付額を振り込む。1時間ほどで完了できた。</p><h2 id=eks-k8s-から-alb-の管理>eks (k8s) から alb の管理</h2><p>eks (k8s) に aws-load-balancer-controller をインストールすると k8s 上のリソースとして alb を管理できるようになる。</p><ul><li><a href=https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/aws-load-balancer-controller.html>AWS Load Balancer Controller アドオンのインストール</a></li></ul><p>具体的には k8s の Ingress と Nodepoint リソースから次の3つのリソースを生成してくれる。</p><ul><li>application load balancer</li><li>http listener</li><li>target groups</li></ul><p>alb からのヘルスチェックは次のようにエンドポイントを記述する。spring boot だと <a href=https://docs.spring.io/spring-boot/docs/current/actuator-api/htmlsingle/>Actuator</a> という web api がヘルスチェックの機能を提供している。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>alb.ingress.kubernetes.io/healthcheck-path</span>: <span style=color:#ae81ff>/actuator/health</span>
</span></span></code></pre></div><p><code>alb.ingress.kubernetes.io/scheme</code> の設定で alb を配置するサブネットを指定できる。デフォルトは <code>internal</code> になる。</p><p>private subnet に配置するとき</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>alb.ingress.kubernetes.io/scheme</span>: <span style=color:#ae81ff>internal</span>
</span></span></code></pre></div><p>public subnet に配置するとき</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>alb.ingress.kubernetes.io/scheme</span>: <span style=color:#ae81ff>internet-facing</span>
</span></span></code></pre></div></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0511/>helm を調べた</a></h1><div class=post-meta><span class=post-date>2022-05-11</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/helm/>helm</a>&nbsp;</span><div class=post-content><p>1時に寝て6時半に起きて8時に起きた。前日は資料づくりで遅くまでオフィスに残っていたせいか、なんか寝坊した。</p><h2 id=helm-調査>helm 調査</h2><p><a href=/diary/posts/2022/0201/#kubernetes-のログ管理と-datadog-agent-のログ連携不具合>k8s 上の datadog-agent</a> が <a href=https://helm.sh/>helm</a> で管理されていて、あるバージョンから <a href=https://docs.dapr.io/operations/hosting/kubernetes/kubernetes-deploy/#install-with-helm-advanced>dapr も helm 管理できる</a> ようになった。dapr は cli からもインストールできるけど、helm のことをよくわかってなかったので調べることにした。そんなたくさん記事をみたわけではないけど、いくつか記事を読んで quora のやり取りが一番よかった。</p><ul><li><a href=https://www.quora.com/When-should-you-use-Kubernetes-Helm-and-not-use-it>When should you use Kubernetes Helm and not use it?</a></li></ul><p>ざっくりまとめるとこうかな。</p><ul><li>helm は oss 且つ cncf の公式プロジェクトだからまぁ安心</li><li>helm はサードパーティのパッケージのインストールや設定の利便性を高める<ul><li>k8s はテンプレート機能が弱いので共通設定と特定環境向けの設定を管理するのがあまり得意ではない</li><li>セキュリティを考慮した k8s 設定は自分でやるよりコミュニティに任せた方がよい場合もある</li><li>パッケージなのでバージョン管理は得意</li></ul></li><li>helm は k8s 向けのパッケージマネージャとレポジトリマネージャーとマーケットプレイスを組み合わせたみたいなもの</li></ul><p>k8s 上でサードパーティのパッケージを自分で設定したい特別な理由がない限りは helm を使うのがよさそうという結論になった。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0324/>spring boot の環境とログ設定</a></h1><div class=post-meta><span class=post-date>2022-03-24</span></div><span class=post-tags>#<a href=/diary/tags/spring-boot/>spring boot</a>&nbsp;
#<a href=/diary/tags/logging/>logging</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>0時に寝て4時に起きて6時に起きた。</p><h2 id=spring-のプロファイル設定>spring のプロファイル設定</h2><p>spring の <a href=https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.profiles>Profiles</a> の仕組みを使って環境ごとの設定を作る。デプロイは k8s で管理しているため、spring boot の <a href=https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.external-config>Externalized Configuration</a> の仕組みを使って、環境変数から application.yml に定義された設定を書き換える。k8s は kustomize で管理していて prod, test, dev の3つの環境で任意の設定を記述できる。</p><p>問題はログ出力の設定を環境ごとに変えたい。具体的には datadog に連携されるログは構造化ログ (json lines) を、ローカルの開発ではコンソールログをみたい。<a href=https://logging.apache.org/log4j/2.x/log4j-spring-boot/index.html>Log4j Spring Boot Support</a> によると、1つの設定ファイルに複数のプロファイル設定を記述できるようにもみえるけど、実際にやってみたらうまく動かなかった。xml ではなく yml を使っているせいかもしれないし、私の記述方法が誤っているのかもしれない。いずれにしても yml で複数のプロファイルを設定しているサンプルをみつけられなかった。</p><p>そこで <a href=https://www.baeldung.com/spring-log4j2-config-per-profile>Different Log4j2 Configurations per Spring Profile</a> をみて、環境ごとにログ設定ファイルも分割することにした。application.yml には次のように記述する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span><span style=color:#f92672>spring</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>profiles</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>active</span>: <span style=color:#ae81ff>dev</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>logging</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>config</span>: <span style=color:#ae81ff>classpath:log4j2-${spring.profiles.active}.yml</span>
</span></span></code></pre></div><p>ローカル開発向けの lgo4j2-dev.yml は次のようになる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span><span style=color:#f92672>Configuration</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>status</span>: <span style=color:#ae81ff>warn</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>YAMLConfig</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>appenders</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>Console</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>STDOUT</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>target</span>: <span style=color:#ae81ff>SYSTEM_OUT</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>PatternLayout</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>Pattern</span>: <span style=color:#e6db74>&#34;%d{yyyy-MM-dd HH:mm:ss.SSS}[%t]%-5level %logger{36} - %msg%n&#34;</span>
</span></span></code></pre></div><p>k8s のマニフェストで環境変数を次のように定義すれば prod というプロファイルが設定される。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>env</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>spring.profiles.active</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;prod&#34;</span>
</span></span></code></pre></div><p>クラウド環境向けの log4j2-prod.yml は次のようになる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span><span style=color:#f92672>Configuration</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>status</span>: <span style=color:#ae81ff>warn</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>YAMLConfig</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>appenders</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>Console</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>STDOUT</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>target</span>: <span style=color:#ae81ff>SYSTEM_OUT</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>EcsLayout</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>serviceNodeName</span>: <span style=color:#66d9ef>null</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>includeMarkers</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>KeyValuePair</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>key</span>: <span style=color:#ae81ff>type</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#ae81ff>app</span>
</span></span></code></pre></div></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0313/>k8s のロールバック</a></h1><div class=post-meta><span class=post-date>2022-03-13</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>0時に寝て7時に起きた。</p><h2 id=k8s-のロールバック>k8s のロールバック</h2><p><a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-to-a-previous-revision>Rolling Back to a Previous Revision</a> をみながらすぐできた。ロールバックもこれまでと同様、github actions の workflow dispatch で管理できるようにした。基本的にはこれだけでロールバックできる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl rollout undo deployment/my-app-deploy
</span></span></code></pre></div><p>ちょっと工夫したこととして、デプロイ時に <a href=https://kubernetes.io/docs/reference/labels-annotations-taints/#change-cause>kubernetes.io/change-cause</a> というアノテーションに git のリビジョンもセットしておくと確認するときにちょっと楽ができる。apply した後の deployment リソースに docker イメージのタグ情報 (= git のリビジョン) を書き込んでおく。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -k <span style=color:#e6db74>${</span>{ env.DEPLOYMENT_ENV <span style=color:#e6db74>}</span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>$ kubectl annotate deployment my-app-deploy kubernetes.io/change-cause<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>{ env.IMAGE_TAG <span style=color:#e6db74>}</span><span style=color:#f92672>}</span> --overwrite<span style=color:#f92672>=</span>true
</span></span></code></pre></div><p>kubectl から履歴をみたときに k8s のリビジョンがどの git のリビジョンを使っているかがわかりやすい。デフォルトでは何も設定されていないかもしれない。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl rollout history deployment/my-app-deploy
</span></span><span style=display:flex><span>deployment.apps/my-app-deploy
</span></span><span style=display:flex><span>REVISION  CHANGE-CAUSE
</span></span><span style=display:flex><span><span style=color:#ae81ff>15</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>16</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>17</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>18</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>19</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>20</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>21</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>22</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>24</span>       1f17a22a6659ea0714a21fca034645cd191e189b
</span></span><span style=display:flex><span><span style=color:#ae81ff>27</span>       a84e113d8b7c124178b58e2f40f57b00aae65485 
</span></span><span style=display:flex><span><span style=color:#ae81ff>28</span>       dcf3552db0668d416ed880f6e896455d7bab194c
</span></span></code></pre></div></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0228/>kustomize の動的な設定</a></h1><div class=post-meta><span class=post-date>2022-02-28</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>23時に寝て2時に起きてそこから断続的に寝たり起きたりを繰り返して6時に半に起きた。ウクライナ情勢のニュースや記事ばかり読んでてあてられた。</p><h2 id=kustomize-の動的な設定>kustomize の動的な設定</h2><p>kustomize で管理している image のタグをデプロイ処理の中で動的に変更したい。パラメーター渡しなり環境変数なり、なにかしらあるだろうとは予測している。<a href=https://github.com/kubernetes-sigs/kustomize/blob/master/examples/image.md>Demo: change image names and tags</a> のサンプルによると、次のように実行すればよいみたい。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kustomize edit set image busybox<span style=color:#f92672>=</span>alpine:3.6
</span></span></code></pre></div><p>次のような <code>kustomization.yaml</code> をセットしてくれるみたい。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span><span style=color:#f92672>images</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>busybox</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>newName</span>: <span style=color:#ae81ff>alpine</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>newTag</span>: <span style=color:#ae81ff>3.6</span>
</span></span></code></pre></div><p>タグのところをリビジョン指定できればいいだけなのでとくに SECRET を使う必要もない。このやり方で書き換えた <code>newTag</code> が POD のデプロイ対象になってくれればよいはず。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0201/>wiki のドキュメント整理</a></h1><div class=post-meta><span class=post-date>2022-02-01</span></div><span class=post-tags>#<a href=/diary/tags/datadog/>datadog</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>23時に寝て4時半に起きた。昨日の帰りに自転車でこけて胸を強打してひたすら痛い。起き上がるのも痛い。安静にしてた。</p><h2 id=kubernetes-のログ管理と-datadog-agent-のログ連携不具合>kubernetes のログ管理と datadog-agent のログ連携不具合</h2><p>先日、<a href=/diary/posts/2022/0127/#ログ連携の不具合調査>datadog にログ連携されていない不具合</a> が発生していて、その1次調査を終えたことについて書いた。緊急対応としては datadog-agent を再起動することで改善することはわかっていたので、その後、kubernetes のログ管理と datadog-agent がどうやって kubernetes クラスター上で実行されているアプリケーションのログを収集しているかを調査していた。今日は wiki に調査してわかったことなどをまとめていた。</p><p>kubernetes クラスターはコンテナランタイムに docker を使っていて、アプリケーションの stdout/stderr を docker の logging driver にリダイレクトし、JSON Lines に設定された logging driver が kubernetes ノード上にログファイルとして出力する。datadog-agent は autodiscovery 機能で pod の情報を常にポーリングしていて、pod が新たにデプロイされたらログファイルを pod 内にマウントして、そのマウントしたログファイルを読み込んでログ収集していると思われる。datadog-agent から pod の情報を取得するには kubernetes のサービスアカウントを使っていて、その credential が projected volume としてマウントされて pod 内から利用できる。その credential を使って kubelet api にリクエストすることで pod の情報を取得している。</p><p>文章で書けばたったこれだけのことなんだけど、たったこれだけのことを理解するのに次のドキュメントを読んだ。実際の調査のときはわからなかったのでもっと多くのドキュメントを読んでいる。いま書いたことを理解するならこのドキュメントを読めば理解できるはず。</p><ul><li><a href=https://kubernetes.io/docs/concepts/overview/components/>https://kubernetes.io/docs/concepts/overview/components/</a></li><li><a href=https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/>https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/</a></li><li><a href=https://kubernetes.io/docs/concepts/cluster-administration/logging/>https://kubernetes.io/docs/concepts/cluster-administration/logging/</a></li><li><a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/>https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/</a></li><li><a href=https://kubernetes.io/docs/concepts/storage/projected-volumes/>https://kubernetes.io/docs/concepts/storage/projected-volumes/</a></li><li><a href="https://docs.datadoghq.com/agent/kubernetes/log/?tab=helm">https://docs.datadoghq.com/agent/kubernetes/log/?tab=helm</a></li><li><a href="https://docs.datadoghq.com/getting_started/agent/autodiscovery/?tab=kubernetes">https://docs.datadoghq.com/getting_started/agent/autodiscovery/?tab=kubernetes</a></li></ul><p>ドキュメントに書いてあることを深く理解するために、kubernetes と datadog-agent のソースコードも読んだ。どちらも go 言語で実装されている。</p><ul><li><a href=https://github.com/kubernetes/kubernetes>https://github.com/kubernetes/kubernetes</a></li><li><a href=https://github.com/DataDog/datadog-agent>https://github.com/DataDog/datadog-agent</a></li></ul><p><code>kubectl logs</code> の振る舞いを確認するだけでも、ソースコードからは実際のログファイルを open してストリームを返しているところはわからなかった。api 呼び出しが連携されて抽象化されていて、コンポーネントの役割分担があって、何も知らずにコードを読んでいてもわからなかった。Kubernetes の低レイヤーのところは Container Runtime Interface (CRI) という標準化を行い、1.20 から docker は非推奨となり、将来的に CRI を提供する実装に置き換わるらしい。ログファイルを open する役割は CRI の実装が担うんじゃないかと思うけど、そこまでは調べきれなかった。また機会があれば CRI の実装も読んでみる。</p><figure><img src=/diary/img/2022/0201_kubectl-logs.png></figure></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0131/>ヘルスチェックのレスポンスのステータスコード</a></h1><div class=post-meta><span class=post-date>2022-01-31</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>1時に寝て6時に起きた。</p><h2 id=404-のレスポンスをヘルスチェック>404 のレスポンスをヘルスチェック</h2><p>ここ数日はお仕事でインフラ周りの調査をしていた。たまたまログをみていて、ELB のヘルスチェックが 404 になっているのを大量にみつけた。てっきりヘルスチェックを使ってないのだろうと思ってインフラ担当者に問い合わせたら、404 が返ってくることをヘルスチェックしているという。404 をチェックすることになんの意味もなく、ただ急ぎで設定する必要があったからとりあえず動かせるためにそう設定したという。一方でアプリケーション側は spring boot で開発していて、<a href=https://www.baeldung.com/spring-boot-actuators>Spring Boot Actuator</a> も導入されているので <code>/actuator/health</code> にアクセスすれば 200 が返ってくるようになっている。どういう経緯だったかはわからないけど、開発者に一言聞けば 404 のレスポンスをヘルスチェックすることは何もない状態でずっと運用されていたことがわかった。</p><p>アプリケーション側の <a href=https://kubernetes.io/docs/concepts/services-networking/ingress/>Kubernetes: Ingress</a> のマニフェストにもそういった設定が入っていて、インフラ側の CDK のコードにもそういったマニフェストがあって、両方の設定変更が必要なのか、アプリケーション側のものだけでいいのか、少し調査が必要ということになった。私も Ingress というのがなにものなのか、よくわかってないので調べて理解する機会としよう。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0127/>datadog-agent のログ連携の不具合調査</a></h1><div class=post-meta><span class=post-date>2022-01-27</span></div><span class=post-tags>#<a href=/diary/tags/datadog/>datadog</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>0時に寝て4時に起きた。朝から1時間ほどドラクエタクトやってた。</p><h2 id=ログ連携の不具合調査>ログ連携の不具合調査</h2><p>少し前に本番環境で <a href=https://github.com/DataDog/datadog-agent>datadog-agent</a> からログが (クラウドの) datadog に連携されていないことがわかった。kubectl logs のコマンドで確認すると、アプリケーションのログは出力されているので datadog-agent から datadog にログを送信するところの問題であるように推測された。たまたま今日、同じような現象をテスト環境で確認できた。ちょうどスクラムのプランニングでログ調査のための作業をするチケットの承認を得たところだった。満を持して発生したような障害だったので私が調査すると明言して調査してた。半日ぐらい調査して、pod 内の credential 情報が置き換わってしまうことが原因っぽいと特定できたが、なぜ置き換わってしまうのかはまだわからない。もう少し調査して解決したら会社のテックブログにいいなと思ったので、日記に書いてた内容を移行することにした。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2021/1205/>頭文字Dを読了</a></h1><div class=post-meta><span class=post-date>2021-12-05</span></div><span class=post-tags>#<a href=/diary/tags/life/>life</a>&nbsp;
#<a href=/diary/tags/commic/>commic</a>&nbsp;
#<a href=/diary/tags/tax/>tax</a>&nbsp;
#<a href=/diary/tags/dapr/>dapr</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>0時に寝て7時に起きてだらだらやってて午前中は <a href=https://ja.wikipedia.org/wiki/%E9%A0%AD%E6%96%87%E5%AD%97D>頭文字D</a> のアニメをみてた。漫画 (アニメも？) はすでに完結しているのでいつか読もうと思いつつ最後まで読んでいない。ゴッドフットやゴッドアームが出てくるぐらいまでは読んだ気がする。その後どうなったのかを知らない。イニシャルDをみていると、ストーリーも絵も演出もまったく派手さはなくて普通なんだけど、なぜかおもしろくて続きをみてしまうという人間の娯楽の本質をついている気がしてくる。なんでなんだろうなぁ。</p><h2 id=頭文字d>頭文字D</h2><p>たまたま思い出したので夜に漫画喫茶行って頭文字Dを最後まで読んできた。全48巻で、31巻ぐらいから読み始めて3-4時間ぐらいで読み終えた。漫画なので仕方ないけど、対戦相手がどんどん強くなっていって勝ち方が玄人好みというのか、単純に抜いた・抜かれたの話しではなく、タイヤマネージメントがどうこうとか、恐怖に対する心理がどうこうとか、ドライバーと車のセッティングも含めた駆け引きが強くなっていって、どちらが速いかというよりは戦略通りの展開にもっていって最後はそれがうまくはまるみたいな、これまでもずっとそうだったんだけど、ここからはよりトップレベルのほんの僅かな差が勝敗を分けるといった描き方になっていったように思う。それはそれで現実に近い気はするけど、漫画的には派手な演出にならないので玄人好みなストーリーになっていった気がする。但し、そこまでやってきて最後の対戦相手だけは、個人的には納得感がなくて、ここまで緻密に作り上げてきた理論や個々のドライバーの修練の積み重ねが圧倒的天才の前にひれ伏すみたいな切り口が急展開していて、頭の切り替えができなかった感じがした。とはいえ、最後まで読み終えられてよかったし、作品としてはすごくおもしろかった。作者はモータースポーツが本当に好きなんだろうなというのが伝わってくる漫画だと思う。</p><h2 id=ふるさと納税>ふるさと納税</h2><p>あまり欲しいものもないし、ふるさと納税の行政手続きも一通り理解したから今年はやらなくてもいいかとも思っていた。しかし、<a href=https://www.satofull.jp/static/campaign/202112_pcp.php>paypayボーナスキャンペーン</a> をみてやってみるかという気になった。paypay はいろんなものと連携していて見かけるたびにすごいなと思う。お得だからと必要もないものを買うことはないけど、ふるさと納税はやらなかったとしても、どのみち納税は必要なものなので還元があるということは節税につながるのかな？理屈はよくわからないけど、言いたいことは paypay はすごいという話でした。</p><h2 id=dapr-の-api-トークンを使った認証>dapr の api トークンを使った認証</h2><p><a href=https://docs.dapr.io/operations/security/api-token/>Enable API token authentication in Dapr</a> を一通り読んだ。内容はとくに難しくなく、こんな風に dapr の manifest を書けば <a href=https://jwt.io/>JWT</a> トークンを設定できますということを書いてある。私はずっとサーバーサイドばっかりやってきたからフロントエンドで使われる技術や仕組みに弱い。JWT トークンもその1つで、自分でちゃんと実装したことがないからちゃんとよく分かってない。これが OAuth2 なら provider を実装したこともあるからその仕組みも意図も理解できる。一度どこかで自分で JWT も実装してみないといけないのだろうな。</p><p>少し前にお仕事で kubernetes の secret の移行作業をやった。既存の secret にキーバリューを追加するときは patch を使う。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl patch secret mydata -p<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;{&#34;stringData&#34;:{&#34;mykey&#34;: &#34;myvalue&#34;}}&#39;</span>
</span></span></code></pre></div><p>secret の内容を確認するときも2つのやり方がある。キーだけを確認するならこれでよい。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl describe secrets mydata
</span></span></code></pre></div><p>キーに対応する値もデコードして確認するならこうする。但し、閲覧注意。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get secret mydata -o json | jq <span style=color:#e6db74>&#39;.data | map_values(@base64d)&#39;</span>
</span></span></code></pre></div></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2021/1122/>kustomize のパッチ適用の違い</a></h1><div class=post-meta><span class=post-date>2021-11-22</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/writing/>writing</a>&nbsp;</span><div class=post-content><p>22時ぐらいには寝て6時半に起きた。昨日はお出かけしてきてバテたんで19時頃からうたた寝を繰り返してずっと寝てた。実家に帰っていた期間を除いて、土日のどちらかを休むのはここ3ヶ月はなかったと思うし、土日の2日間ほとんど仕事をしなかったのは半年ぐらいはなかったと思う。久しぶりに土日に仕事しなかったなという印象で、その理由は業務委託のお仕事の契約が決まって余裕があるからだと思う。</p><h2 id=kustomize-の-inline-patch>kustomize の Inline Patch</h2><p><a href=https://kubectl.docs.kubernetes.io/guides/example/inline_patch/>Inline Patch</a> に次の3つのやり方が説明されている。</p><blockquote><ul><li>patchesStrategicMerge: Strategic Merge Patch として解析されるパッチファイルのリスト</li><li>patchesJSON6902: 1つのターゲットリソースのみに適用可能な JSON Patch として解析されるパッチと関連付けされるターゲットのリスト</li><li>patches: 関連付けされるターゲットとパッチのリスト。このパッチは複数のオブジェクトに適用でき、パッチが Strategic Merge Patch なのか JSON Patch かは自動的に検出</li></ul></blockquote><p>patches は patchesStrategicMerge と patchesJSON6902 の両方を記述できる。運用上は patchesStrategicMerge か patchesJSON6902 を適用したいパッチの内容によって使い分けることになる。おそらく前者は base にない要素を追加したり、base の要素をすべて置き換えたりするときに使う。後者は base にある map や list の一部の要素のみを限定して置き換えたり、削除したりするときに使う。ちなみに patchesJSON6902 の 6902 というのは <a href=https://datatracker.ietf.org/doc/html/rfc6902>RFC 6902 JavaScript Object Notation (JSON) Patch</a> に由来する。</p><p>patchesJson6902 の例として次のような設定にパッチを適用する。base から読まれた metadata の要素から namespace のみを削除したり、spec.metadata の1番目のリストの secretKeyRef が参照する Secret を my-secret で置き換えたりできる。こういったパッチを patchesStrategicMerge で実現することはできないのではないかと思う (詳しくないので私が間違っているかもしれない) 。</p><h5 id=baseyml>base.yml</h5><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Component</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-component</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>default</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>username</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>value</span>: <span style=color:#ae81ff>user</span>
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>password</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>secretKeyRef</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>base-secret</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>key</span>: <span style=color:#ae81ff>password</span>
</span></span></code></pre></div><h5 id=kustomizationyml>kustomization.yml</h5><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span><span style=color:#f92672>patchesJson6902</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>path</span>: <span style=color:#ae81ff>my-patch.yaml</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>target</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>group</span>: <span style=color:#ae81ff>apps</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>version</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Component</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-component</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h5 id=my-patchyaml>my-patch.yaml</h5><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>- <span style=color:#f92672>op</span>: <span style=color:#ae81ff>remove</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/metadata/namespace</span>
</span></span><span style=display:flex><span>- <span style=color:#f92672>op</span>: <span style=color:#ae81ff>replace</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/spec/metadata/1/secretKeyRef/name</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>value</span>: <span style=color:#ae81ff>my-secret</span>
</span></span></code></pre></div><h2 id=リーンキャンバスレビュー-前半>リーンキャンバスレビュー (前半)</h2><p><a href=/diary/posts/2021/1114/#リーンキャンバス>前に作ったリーンキャンバス</a> を使って友だちにプロダクトの設計をレビューしてもらった。私がリーンキャンバスを作ったことがなかったので、この項目にはどういった内容を書くか、それぞれの項目がどういった関連付けや粒度で整理するかといった、リーンキャンバスの書き方そのものも含めて教えてもらった。</p><p>私が設計のために作った40枚のスライドを話すと2時間必要とするが、リーンキャンバスを使えば要点のみ15分で話せるようになるのが狙いになるみたい。とはいえ、リーンキャンバスの書いてある内容の半分を確認するだけで今日は2時間弱かかってしまった。議事録を取りながらだったので話すだけならもっと短くなったかもしれないし、その背景や根拠を細かくツッコミしていくとそれなりの時間はかかるのかもしれない。リーンキャンバス上は数枚の付箋で簡潔に書いてあるが、これどういうこと？みたいな問いになると詳細を説明しないといけないので時間がかかったように思う。リーンキャンバスの精度や品質が上がれば、読み手が詳細を確認しなくても意図を理解しやすくて詳細のツッコミが不要になるのかもしれない。これまで使ったことがないツールでおもしろいので週末に後半を行う。課題管理の背景には実践知、認知心理学、情報共有、組織論といった様々な分野にまたがるのでそのコンテキストを共有するのはなかなか難しいのではないかという思いもある。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2021/1115/>お仕事しかしなかった一日</a></h1><div class=post-meta><span class=post-date>2021-11-15</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>2時に寝て6時半に起きた。寝る前にウォーキングしてくるとよく眠れる気がする。お仕事で簡単に終わると思ってた作業にちょっとはまってトラブルシューティングしてたら疲れた。原因はわかって自己解決できたのはよかったけど、消耗して早くお仕事を終えて帰ってくつろいでた。</p><h2 id=ローカル開発環境の整備>ローカル開発環境の整備</h2><p>お仕事でローカルの k8s 環境の保守の作業をしている。<a href=https://minikube.sigs.k8s.io/docs/>minikube</a> でローカルの k8s クラスターを作成して <a href=https://kubernetes.io/docs/reference/kubectl/overview/>kubectl</a> コマンドで制御する。k8s の yaml の設定ファイルのことをマニフェストと呼ぶのかな？そのテンプレート？ジェネレーター的なツールに <a href=https://kustomize.io/>kustomize</a> を使っている。先週から1週間触っていたので cli の操作にはだいぶ慣れてきた。</p><p>まだ基本的な delete & apply みたいなことしかやってないけど、また余裕のあるときに細かいコマンドやロールバックのやり方なども学習しようと思う。デプロイで一番重要なのはロールバック、次にローリングアップデート、ローリングアップデートができればカナリアリリースもできるかな？その2つがあれば運用は大幅にコスト削減できるし、開発のアジリティも上げられる。たぶん k8s を使えば簡単にできるんだろうなというのは delete & apply だけみてもそう受け取れる。k8s クラスターさえマネージドならよく出来た仕組みだなと感心した。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2021/1108/>Kubernetes 使い始めの雑感</a></h1><div class=post-meta><span class=post-date>2021-11-08</span></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>1時に寝て7時に起きた。夜にウォーキングし始めてからよく眠れるようになった気がする。</p><h2 id=udemy-kubernetes入門>udemy: Kubernetes入門</h2><p><a href=/diary/posts/2021/1107/#udemy-kubernetes入門>昨日</a> の続き。今日はセクション6から最後まで。CI/CD のセクションだけスキップして、他は一通り目を通した。</p><h3 id=セクション6-kubernetes実践>セクション6 Kubernetes実践</h3><p>1つずつ書くのは大変だけど、数をこなして徐々に覚えていけばよい。手で書くのもよいが、別のやり方としてクライアント側で dry run すると、設定のひな型を作ってくれるのでそれに必要な設定を足すのもよい。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl create deploy mysql --image<span style=color:#f92672>=</span>mysql:5.7 --dry-run<span style=color:#f92672>=</span>client -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl exec -n database -it mysql-787f86d65c-nflxx -- mysql -uroot -ppassword
</span></span></code></pre></div><p>データベースとアプリケーションを異なる namespace にデプロイして、それらが通信できるような設定を行う。基本的には <code>--dry-run=client</code> でひな型を作りつつ、必要な設定を追加していくやり方が簡単そうにみえた。とはいえ、実際に設定していくときはどういう設定を追加するとどういう振る舞いになるかを調べながら作業すると思うのでこんな簡単にはできないとは思う。次のようなアプリケーションをデプロイする一覧の流れを理解できた。</p><ol><li>namespace 作成</li><li>Deployment 作成</li><li>ConfigMap 作成</li><li>Secret 作成</li><li>Deployment, ConfigMap, Secret 適用</li><li>Service 適用</li><li>port-forward でローカルからアクセス</li><li>(作成したリソースをすべて削除)</li></ol><h3 id=セクション7-kubernetesのdebug>セクション7 KubernetesのDebug</h3><p>基本は pod のステータスを確認しながら問題があれば、その箇所を追いかけていって原因を調査する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get pod -A
</span></span><span style=display:flex><span>$ kubectl get pod -A --selector run<span style=color:#f92672>=</span>nginx
</span></span></code></pre></div><p>k8s 上で実行しているアプリケーションの依存先へ接続できない場合は Service の確認が必要となる。kubectl の get, describe, logs などのサブコマンドをあれこれみながらエラーの原因を把握して、yaml の設定を変更していく。k8s のアーキテクチャとコマンドを覚えていないとなかなか難しそう。</p><p>とりあえず動かした後にまとめて全部削除できるのがテストやデバッグに便利そう。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl delete -f .
</span></span></code></pre></div><p>もしくは開発用に独自の namespace を作成して、あとで丸ごと namespace を削除するのでもよさそう。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl delete ns mynamespace
</span></span></code></pre></div><h2 id=k8s-の調査>k8s の調査</h2><p>業務のアプリケーションを minikube で作ったローカル k8s クラスターで動かしてみた。ローカルの開発環境の構築方法をメンテナンスして、自分でも一通り k8s の yaml を書いて、デプロイして、振る舞いを確認したりしていた。最初なのでおもしろい。自分で一通りやってみて、k8s が難しいとみんなが言っているのは k8s クラスターを自前で構築するのが難しいのだとようやく理解できた。k8s クラスターがすでにある状態なら kubectl の使い方を覚えるだけで全く難しくない。GKE や EKS を使って運用するなら k8s の運用コストは大したことがないと理解できた。k8s クラスター向けの yaml はたくさん書かないといけないけど、どうせ ECS や EC2 でやっていても CDK や Terraform などのインフラ設定を書くのは同じなのでそこはあまり差がない。k8s はコンテナオーケストレーションをやってくれるメリットが大きいので minikube と EKS の環境の差異があまり問題にならないようなアプリケーション開発であれば、普通に使っていって問題ないように思えた。ローカルで環境作るのが大変なんじゃないかという先入観があったけど、全然そんなことはなかった。コンテナのイメージをビルドしないといけないのが追加のコストかな。</p></div></div><div class="post on-list"><h1 class=post-title><a href=/diary/posts/2021/1107/>普通の休日の翌日</a></h1><div class=post-meta><span class=post-date>2021-11-07</span></div><span class=post-tags>#<a href=/diary/tags/bizpy/>bizpy</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>5時に寝て10時に起きた。昨日は夕方に2-3時間寝てたのでその分、夜に調べものをしていた。休みたい気持ちもあるけど、調べるものが多過ぎて全然時間が足りない。</p><h2 id=bizpy-勉強会の資料作り>bizpy 勉強会の資料作り</h2><p><a href=/diary/posts/2021/1106/#slack-apps-の調査>昨日</a> の続き。昨日サンプルコードを実装したので、その設定や要点を <a href=https://github.com/t2y/python-study/tree/master/BizPy/slack/20211027>資料</a> に作成した。現時点で <a href=https://bizpy.connpass.com/event/229091/>Python で Slack のインテグレーションをやってみる勉強会 #2</a> の参加者は10人。連続シリーズは回を重ねるごとに減っていくものなのでこんなもんかな。あともう1回やったら終わりにする。</p><h2 id=udemy-kubernetes入門>udemy: Kubernetes入門</h2><p>友だちから udemy の k8s のコースがよいと聞いたんだけど、そのコースはいまは提供されていなくて、せっかくなので適当に検索してヒットした <a href=https://www.udemy.com/course/kubernetes-basics-2021/>Kubernetes入門</a> を受講することに決めた。本当は英語の本格的なコースを受講した方がよいのだろうけど、余裕のあるときはそれでいいけど、いま数日で概要を把握して使えるようにしたいので日本語のコースにしてみた。</p><ul><li><a href=https://blog.ayakumo.net/entry/2018/01/27/010000>Udemy の Learning Docker and Kubernetes by Lab がとてもよい</a></li><li><a href=https://blog.ayakumo.net/entry/2018/02/15/232918>Docker, Kubernetes 学習とツールとコンピュータサイエンス</a></li></ul><p>昨日インストールした minikube のクラスターを使って「Kubernetes入門」のセクション1からセクション5までやった。だいたい半分ぐらい。所感としては、全く何も知らない人には要点をかいつまんで教えてくれるのと、最初に覚えるとよい基本的な CLI のコマンドとその振る舞いや設定を紹介してくれるのでよかった。初めて k8s に挑戦する自分にとってはちょうどよいレベル感だった。全体像の概念を捉えてコンテキストに沿って順番にハンズオン形式で学習していくスタイル。<a href=https://github.com/nakamasato/kubernetes-basics>nakamasato/kubernetes-basics</a> を使って自分でも CLI でコマンドを打ちながら進めてみた。yaml ファイルを定義するのもこれはこれで面倒だけど、この辺は慣れの問題かな？とも思う。いくつか学んだことを整理しておく。</p><h3 id=セクション1-introduction>セクション1 Introduction</h3><p>k8s には2つのコンポーネントがあり、これを k8s クラスターと呼んでいる。</p><ul><li>Control Plane (API サーバー)</li><li>複数の Worker (Kubelet)</li></ul><p>yaml で設定する Desired State (理想状態) と呼ばれる設定が登録されると、Control Plane の API サーバーと Worker の kubelet が通信してそれを実現しようとする。pod とは k8s のデプロイの最小単位となる。コンテナ、ポート、レプリカ数などを設定する。pod をそれぞれの Worker にデプロイしたり、Worker がダウンしたときに別の Worker で起動させたりする。</p><h3 id=セクション2-kubernets-概要>セクション2 Kubernets 概要</h3><p>k8s はコンテナ化したアプリケーションのデプロイ、スケーリング、管理を行うためのオープンソースのコンテナオーケストレーションシステムである。</p><ul><li>コンテナ<ul><li>独立した環境でアプリケーションを実行する仕組み</li><li>コンテナの実態はプロセス</li><li>Kernel Namespaces を利用し、プロセスID、ネットワークインターフェース、リソースなどを分離してコンテナ間で干渉しない</li><li>ホストマシンへの依存度を最小化してアプリケーションをどこでも実行可能にする<ul><li>従来のやり方の最大の違いはライブラリがホストマシンにインストールされるのではなく、コンテナの内部にインストールされる</li></ul></li></ul></li><li>オーケストレーション<ul><li>デプロイ、スケーリング、管理などの仕組み</li></ul></li></ul><p>1つのアプリケーションは複数のマシン上で動かすことで可用性を高めたいが、コンテナを動かすために考えることが増えていくと管理コストも増えていく。コンテナオーケストレーション機能により次のようなシステム管理者が行っていたことが自動化される。</p><ul><li>デプロイメント</li><li>スケジューリング</li><li>オートスケーリング<ul><li>負荷に応じてコンテナ数やマシン数を増減させる</li></ul></li><li>ネットワーク</li><li>リソースマネジメント</li><li>セキュリティ<ul><li>ネットワークポリシーやリソースの権限定義</li></ul></li></ul><p>k8s クラスターの構造は次になる。</p><ul><li>Control Plane<ul><li>api: kubelet と通信するサーバー</li><li>etcd: 設定などを格納するキーバリューストア</li><li>shed: kube スケジューラー</li><li>c-m: コントロールマネージャー</li><li>c-c-m: クラウドプロバイダと api 連携する<ul><li>ローカルで使うときは必要ない</li></ul></li></ul></li><li>Worker ノードはコンテナランタイムをいインストールしておく必要がある<ul><li>kubelet は Control Plane と通信するためのエージェントとして動作する</li></ul></li></ul><p>一番需要なこととして、k8s は理想状態と現実状態を比較して、理想状態に近づけようとする。app.yaml の理想状態を kubectl を用いて api サーバーを介して etcd に格納する。現実状態は kubelet から api サーバーを介して etcd に格納される。c-m は理想状態と現実状態のチェックを行い、異なっていれば理想状態に近づけることをしていく。</p><h3 id=セクション4-kubectl>セクション4 kubectl</h3><p>minikube で最初に起動しているのは Control Plane を起動していることが理解できた。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ minikube start
</span></span><span style=display:flex><span>$ kubectl config current-context
</span></span><span style=display:flex><span>minikube
</span></span></code></pre></div><p>同時に ~/.kube/config に kubectl の設定も追加される。<code>minikube</code> という名前でクラスター、ユーザー、コンテキストが設定される。</p><p>リソース一覧の確認。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl api-resources
</span></span></code></pre></div><p>出力フォーマットも様々。例えば、デフォルトの表示は次になる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get node
</span></span><span style=display:flex><span>NAME       STATUS   ROLES                  AGE     VERSION
</span></span><span style=display:flex><span>minikube   Ready    control-plane,master   7m21s   v1.22.2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl get node -o wide
</span></span><span style=display:flex><span>NAME       STATUS   ROLES                  AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
</span></span><span style=display:flex><span>minikube   Ready    control-plane,master   8m38s   v1.22.2   192.168.49.2   &lt;none&gt;        Ubuntu 20.04.2 LTS   5.11.0-38-generic   docker://20.10.8
</span></span></code></pre></div><p>より詳細な情報をそれぞれのフォーマットで表示する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get node -o json
</span></span><span style=display:flex><span>$ kubectl get node -o yaml
</span></span></code></pre></div><p>namespace を確認する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get namespace
</span></span><span style=display:flex><span>NAME              STATUS   AGE
</span></span><span style=display:flex><span>default           Active   10m
</span></span><span style=display:flex><span>kube-node-lease   Active   10m
</span></span><span style=display:flex><span>kube-public       Active   10m
</span></span><span style=display:flex><span>kube-system       Active   10m
</span></span></code></pre></div><p>namespace を指定して pod 一覧を取得する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get pod --namespace kube-system
</span></span><span style=display:flex><span>NAME                               READY   STATUS    RESTARTS      AGE
</span></span><span style=display:flex><span>coredns-78fcd69978-qxqbn           1/1     Running   <span style=color:#ae81ff>0</span>             11m
</span></span><span style=display:flex><span>etcd-minikube                      1/1     Running   <span style=color:#ae81ff>0</span>             11m
</span></span><span style=display:flex><span>kube-apiserver-minikube            1/1     Running   <span style=color:#ae81ff>0</span>             11m
</span></span><span style=display:flex><span>kube-controller-manager-minikube   1/1     Running   <span style=color:#ae81ff>0</span>             11m
</span></span><span style=display:flex><span>kube-proxy-g55hg                   1/1     Running   <span style=color:#ae81ff>0</span>             11m
</span></span><span style=display:flex><span>kube-scheduler-minikube            1/1     Running   <span style=color:#ae81ff>0</span>             11m
</span></span><span style=display:flex><span>storage-provisioner                1/1     Running   <span style=color:#ae81ff>1</span> <span style=color:#f92672>(</span>10m ago<span style=color:#f92672>)</span>   11m
</span></span></code></pre></div><p>グローバルな CLI のオプションを確認する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl options
</span></span></code></pre></div><p>ノードの詳細を表示する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl describe node
</span></span></code></pre></div><p>describe は名前の接頭辞を指定できるので namespace ならこんな感じに実行できる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl describe namespace kube-
</span></span></code></pre></div><h3 id=セクション5-kubernetes-リソース>セクション5 Kubernetes リソース</h3><p>pod とは k8s 上のデプロイの最小単位である。</p><ul><li>1つまたは複数のコンテナをもつ</li><li>ネットワークやストレージを共有リソースとしてもつ</li><li>コンテナの実行方法に関する仕様をもつ</li></ul><p>pod が使えなくなった場合に他のノードにデプロイされることもある。1つのアプリケーションを複数の pod でデプロイすることが多い。なるべく複数のアプリケーションを1つの pod に入れない。個別の pod を直接操作しない。</p><p>共有コンテキスト</p><ul><li>同一 pod 内のコンテナは同じストレージにアクセスできる</li><li>同一 pod 内のコンテナは ip アドレスとポートを含むネットワーク名前空間を共有する</li></ul><p>k8s オブジェクト</p><ul><li>クラスタの状態を表現する</li><li>2つのフィールドをもつ<ul><li>spec: 理想状態 (desired status)</li><li>status: 現実状態 (current status)</li></ul></li></ul><p>pod の作成は k8s オブジェクトを作成している。オブジェクト作成時の必須フィールドが4つある。</p><ul><li>apiVersion</li><li>kind</li><li>metadata</li><li>spec</li></ul><p>namespace は同一クラスター上で複数の仮想クラスターの動作をサポートする。</p><ul><li>仮想クラスターとは、物理的には同じマシンで動いているかもしれないが、仮想的に環境を分離している<ul><li>1つのクラスターを論理的にわける</li><li>チームや部署ごとにわけて使い分けたりすることも多い</li></ul></li></ul><p>namespace を使うメリットは次になる。</p><ul><li>pod やコンテナのリソースの範囲設定</li><li>namespace 全体の総リソース制限</li><li>権限管理</li></ul><p>初期の namespace として4つあるが、初心者は最初の2つだけをまず覚えておく。</p><ul><li>default:</li><li>kube-system:</li><li>kube-public:</li><li>kube-node-lease</li></ul><p>namespace と cluster の違い。</p><ul><li>Namespace-scoped リソース<ul><li>namespace に属しているリソース</li></ul></li><li>Cluster-scoped リソース<ul><li>クラスター全体で使われるもの</li></ul></li></ul><p>次のコマンドで確認できる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl api-resources --namespaced<span style=color:#f92672>=</span>true 
</span></span></code></pre></div><p>namespace の作成</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl create namespace my-namespace
</span></span></code></pre></div><p>ワークロードリソースとは複数の pod を作成・管理するためのリソース。ワークロードリソースは pod テンプレートを使って pod を作成する。</p><ul><li>ReplicaSet<ul><li>常に指定したレプリカ数の pod を保つ</li></ul></li><li>Deployment<ul><li>ローリングアップデートやロールバックなどのアップデート機能を提供</li><li>ReplicaSet のロールアウト</li><li>不安定な場合の前のバージョンへロールバック</li><li>使用頻度が高い<ul><li>ほとんどのアプリケーションは Deployment で管理</li></ul></li></ul></li><li>Secret<ul><li>機密情報を保存・管理し、Pod から参照可能</li><li>主な使用方法としてコンテナの環境変数の設定<ul><li>アプリケーションの DB のパスワードなどに使う</li></ul></li></ul></li><li>Service<ul><li>pod の集合を抽象化して公開する<ul><li>pod の集合に対する DNS 名</li><li>pod の集合に対する負荷分散</li></ul></li></ul></li></ul></div></div><div class=pagination><div class=pagination__buttons><span class="button previous"><a href=/diary/tags/kubernetes/><span class=button__icon>←</span>
<span class=button__text>最近の日記</span></a></span>
<span class="button next"><a href=/diary/tags/kubernetes/page/3/><span class=button__text>過去の日記</span>
<span class=button__icon>→</span></a></span></div></div></div></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>© 2021 Tetsuya Morimoto</span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=/diary/assets/main.js></script>
<script src=/diary/assets/prism.js></script></div></body></html>