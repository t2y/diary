<!doctype html><html lang=en><head><title>kubernetes :: forest nook</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=/diary/tags/kubernetes/><link rel=stylesheet href=/diary/styles.css><link rel=stylesheet href=/diary/style.css><link rel="shortcut icon" href=/diary/favicon.ico><meta name=twitter:card content="summary"><meta name=twitter:site content="t2y"><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta property="og:title" content="kubernetes"><meta property="og:description" content><meta property="og:url" content="/diary/tags/kubernetes/"><meta property="og:site_name" content="forest nook"><meta property="og:image" content="/diary/favicon.ico"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><link href=/diary/tags/kubernetes/index.xml rel=alternate type=application/rss+xml title="forest nook"></head><body class=green><div class="container full headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/diary><div class=logo>forest nook</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/diary/about>自己紹介</a></li><li><a href=/diary/dates>月別一覧</a></li><li><a href=/diary/tags>タグ一覧</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/diary/about>自己紹介</a></li><li><a href=/diary/dates>月別一覧</a></li><li><a href=/diary/tags>タグ一覧</a></li></ul></nav></header><div class=content><div class=posts><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0806/>Kubernetes Clients のサンプル実装</a></h1><div class=post-meta><time class=post-date>2022-08-06 (Sat.) ::</time></div><span class=post-tags>#<a href=/diary/tags/life/>life</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>23時に寝て7時半に起きた。夜中も2回ぐらい起きる。暑さでまいってきた。</p><h2 id=ストレッチ>ストレッチ</h2><p>今日の開脚幅は開始前159cmで、ストレッチ後162cmだった。数字は悪くない。いつもはストレッチを受けていると疲労しているところが伸びることで体が軽くなっていく感覚があるのだけど、今日は体全体がだるくてストレッチを受けていてもなんかしんどいなぁとだるさを感じていた。コロナに感染してないと思うけど、夏バテの状態をそのままストレッチにも持ち込んだような感覚があった。腰の張りや肩甲骨の硬さなどが少し気になったかな。トレーナーさんには立ったときの姿勢が少し前よりで重心のバランスがよくないといったアドバイスをされた。とくにどこが悪いというわけでもないのになんかしんどい。</p><h2 id=kubernetes-clients-のサンプル実装>Kubernetes Clients のサンプル実装</h2><p><a href=/diary/posts/2022/0804/>Kubernetes Clients の調査</a> の続き。java クライアントを使って minikube でいくつか動かしてみた。openapi で生成した rest api クライントが提供されている。デフォルト設定でも minikube で普通に動いたのでおそらく裏で <code>$HOME/.kube/config</code> をみたり <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code> を読み込んで認証ヘッダーに設定してくれたりするのだと推測する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span><span style=color:#66d9ef>public</span> ApiClient <span style=color:#a6e22e>getKubernetesClient</span><span style=color:#f92672>()</span> <span style=color:#66d9ef>throws</span> IOException <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    var client <span style=color:#f92672>=</span> Config<span style=color:#f92672>.</span><span style=color:#a6e22e>defaultClient</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>    io<span style=color:#f92672>.</span><span style=color:#a6e22e>kubernetes</span><span style=color:#f92672>.</span><span style=color:#a6e22e>client</span><span style=color:#f92672>.</span><span style=color:#a6e22e>openapi</span><span style=color:#f92672>.</span><span style=color:#a6e22e>Configuration</span><span style=color:#f92672>.</span><span style=color:#a6e22e>setDefaultApiClient</span><span style=color:#f92672>(</span>client<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> client<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><p>このクライアントを使って <a href=https://github.com/kubernetes-client/java/tree/master/kubernetes/docs>java/kubernetes/docs/</a> 配下にある api インスタンスを生成する。例えば、cronjob や job を扱うならば <code>BatchV1Api</code> というドキュメントがある。BatchApi だけでも3つのドキュメントがあるのでちょっとやり過ぎな気もする。</p><ul><li>BatchApi.md</li><li>BatchV1Api.md</li><li>BatchV1beta1Api.md</li></ul><p>kubectl コマンドで使う cronjob から手動で job を設定するのを実装してみる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl create job --from<span style=color:#f92672>=</span>cronjob/my-schedule-job my-manual-job
</span></span></code></pre></div><p>細かい設定はちゃんと調べないといけないけど、一応はこれで動いた。cronjob のオブジェクトを取得して job のオブジェクトを生成して create するだけ。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span>var api <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> BatchV1Api<span style=color:#f92672>(</span><span style=color:#66d9ef>this</span><span style=color:#f92672>.</span><span style=color:#a6e22e>getKubernetesClient</span><span style=color:#f92672>());</span>
</span></span><span style=display:flex><span>var cronJob <span style=color:#f92672>=</span> api<span style=color:#f92672>.</span><span style=color:#a6e22e>readNamespacedCronJob</span><span style=color:#f92672>(</span>cronJobName<span style=color:#f92672>,</span> NAMESPACE_DEFAULT<span style=color:#f92672>,</span> <span style=color:#66d9ef>null</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>var ann <span style=color:#f92672>=</span> Map<span style=color:#f92672>.</span><span style=color:#a6e22e>of</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;cronjob.kubernetes.io/instantiate&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;manual&#34;</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>var metadata <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> V1ObjectMeta<span style=color:#f92672>().</span><span style=color:#a6e22e>name</span><span style=color:#f92672>(</span>newJobName<span style=color:#f92672>).</span><span style=color:#a6e22e>annotations</span><span style=color:#f92672>(</span>ann<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>var spec <span style=color:#f92672>=</span> cronJob<span style=color:#f92672>.</span><span style=color:#a6e22e>getSpec</span><span style=color:#f92672>().</span><span style=color:#a6e22e>getJobTemplate</span><span style=color:#f92672>().</span><span style=color:#a6e22e>getSpec</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>spec<span style=color:#f92672>.</span><span style=color:#a6e22e>setTtlSecondsAfterFinished</span><span style=color:#f92672>(</span><span style=color:#ae81ff>10</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>var job <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> V1Job<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>apiVersion</span><span style=color:#f92672>(</span>cronJob<span style=color:#f92672>.</span><span style=color:#a6e22e>getApiVersion</span><span style=color:#f92672>())</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>kind</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Job&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>spec</span><span style=color:#f92672>(</span>cronJob<span style=color:#f92672>.</span><span style=color:#a6e22e>getSpec</span><span style=color:#f92672>().</span><span style=color:#a6e22e>getJobTemplate</span><span style=color:#f92672>().</span><span style=color:#a6e22e>getSpec</span><span style=color:#f92672>())</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>metadata</span><span style=color:#f92672>(</span>metadata<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>var result <span style=color:#f92672>=</span> api<span style=color:#f92672>.</span><span style=color:#a6e22e>createNamespacedJob</span><span style=color:#f92672>(</span>NAMESPACE_DEFAULT<span style=color:#f92672>,</span> job<span style=color:#f92672>,</span> <span style=color:#66d9ef>null</span><span style=color:#f92672>,</span> <span style=color:#66d9ef>null</span><span style=color:#f92672>,</span> <span style=color:#66d9ef>null</span><span style=color:#f92672>,</span> <span style=color:#66d9ef>null</span><span style=color:#f92672>);</span>
</span></span></code></pre></div><p>手動で作成した job の pod は終了後にゴミとして残ってしまうので ttl を設定すれば自動的に削除できることに気付いた。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span>spec<span style=color:#f92672>.</span><span style=color:#a6e22e>setTtlSecondsAfterFinished</span><span style=color:#f92672>(</span><span style=color:#ae81ff>10</span><span style=color:#f92672>);</span>
</span></span></code></pre></div><p>k8s クラスターの内部、つまり pod 内からリクエストするには <a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions>ServiceAccount permissions</a> を適切に設定しないといけない。ひとまずローカルの minikube で super user 権限にしたらリクエストはできた。実運用では適切なロールを定義して適切に権限設定しないといけない。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl create clusterrolebinding serviceaccounts-cluster-admin <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --clusterrole<span style=color:#f92672>=</span>cluster-admin <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --group<span style=color:#f92672>=</span>system:serviceaccounts
</span></span></code></pre></div></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0804/>Kubernetes Clients の調査</a></h1><div class=post-meta><time class=post-date>2022-08-04 (Thu.) ::</time></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>23時に寝て6時に起きた。</p><h2 id=k8s-cronjob-の手動実行>k8s cronjob の手動実行</h2><p>いろんな定期／バッチ処理を k8s の cronjob に置き換えつつある。これまでアプリケーションサーバーでスケジュール実行していたものも本来サーバーである必要はないのでサーバーアプリケーションから cli アプリケーションに移行したりしている。そうやって定期実行ジョブが増えてくると、今度は調査やデバッグ目的で任意のタイミングで実行したくなる。kubectl を使って次のように実行できる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl create job --from<span style=color:#f92672>=</span>cronjob/my-schedule-job my-manual-job
</span></span></code></pre></div><p>この cli を実行すると、cronjob のマニフェストから <code>my-manual-job</code> というジョブの pod が生成されて実行される。開発者ならこれでよいのだけど、非開発者も調査や検証目的で実行したい。そのためには非開発者向けのインターフェースを作らないといけない。本当は chatops のように slack apps によるコマンド実行ができるとカッコよいのだけど、k8s クラスターと slack 間の認証やセキュリティの仕組みを作る必要があって、既存の仕組みがないならそこはセキュリティリスクにも成り得るのでちょっと控えたい。そうすると、既存のサーバーアプリケーションの web api のインターフェースで提供できるようにしたい。複数の言語向けに <a href=https://github.com/kubernetes-client>Kubernetes Clients</a> が提供されている。これを使って cronjob の手動実行を実装できそうな気がする。時間があれば週末に軽く調べてみようと思う。</p><ul><li>python</li><li>c#</li><li>javascript</li><li>java</li><li>c</li><li>haskel</li><li>go</li><li>ruby</li></ul></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0720/>ノードグループと nodeSelector</a></h1><div class=post-meta><time class=post-date>2022-07-20 (Wed.) ::</time></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>1時に寝て6時に起きた。</p><h2 id=eks-クラスターのノードグループと-k8s-の-nodeselector>EKS クラスターのノードグループと k8s の nodeSelector</h2><p>先日 <a href=/diary/posts/2022/0711/>k8s の nodeSelector</a> の調査について書いた。いまテスト環境でその調査結果の運用検証中。当初は k8s の機能だけを使いたいと考えていたが、いま eksctl コマンドで EKS クラスターを管理していて、k8s ノードの実体である ec2 のプロビジョニングはノードグループがもつ起動テンプレートとオートスケールポリシーにより制御される。そのため、ノードグループを分割してそれぞれにノードラベルが必ず付与されるように管理する方が簡単だとわかってきた。要はアプリケーションサーバー向けのノードグループとバッチ処理向けのノードグループの2つを作った。あと覚えておくとよいのが、<a href=https://eksctl.io/usage/iam-policies/#adding-a-custom-instance-role>Adding a custom instance role</a> で任意のポリシーもノードグループの iam ロールに追加できる。設定例としてはこんな感じ。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span>  <span style=color:#f92672>iam</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>attachPolicyARNs</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>arn:aws:iam::${accountId}:policy/my-custom-policy</span>
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly</span>
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy</span>
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy</span>
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore</span>
</span></span></code></pre></div><p>ノードグループの準備が整ったら nodeSelector を指定した pod をデプロイするだけ。k8s ノードがどんなノードラベルをもっているかは次のようにして確認できる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get node --show-labels
</span></span></code></pre></div><p>意図したノードラベルが付与された k8s ノードに pod がデプロイされたかどうかは次のようにして確認できる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get pod --output wide --field-selector spec.nodeName<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>ノードラベルをもつノード名<span style=color:#e6db74>}</span>
</span></span></code></pre></div><p>これらの環境構築、検証、wiki にドキュメントを書いて本番作業手順もまとめた。一通りきれいにまとまったインフラ作業を完遂できて気分がよい。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0711/>nodeSelector を試す</a></h1><div class=post-meta><time class=post-date>2022-07-11 (Mon.) ::</time></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>4時に寝て8時に起きた。久しぶりに寝坊した。</p><h2 id=k8s-の-nodeselector>k8s の nodeSelector</h2><p>先日 <a href=/diary/posts/2022/0706/>定期/バッチ処理を k8s の cronjob にすべて移行</a> した。すでに本番運用もしていて調子もよさそうにみえる。あと残課題としてバッチ処理とアプリケーションサーバーの pod がデプロイされる k8s ノードを分割したい。現時点では、バッチ処理の負荷は小さいから同じスペックのインスタンスの k8s ノード上で混在させて運用している。しかし、いずれ運用上の問題になる懸念がある。そのため、バッチ処理のみを実行する k8s ノードを管理したい。次のドキュメントに書いてある。</p><ul><li><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/>Assigning Pods to Nodes</a></li></ul><p>k8s のドキュメントによると、大きく分けて nodeSelector と Affinity という2つのやり方がある。前者はラベルでフィルターするシンプルな仕組み、後者はさらに複雑な要件に対応するもの。いまのところ、ただ分割できればよいのでシンプルな nodeSelector で実装してみることにした。</p><ul><li>nodeSelector</li><li>Affinity and anti-affinity</li></ul><p>余談だが、nodeSelector はいずれ Affinity に置き換わるので deprecated だと一時期ドキュメントに書かれていたらしい。具体的に決まっていることでもないため、<a href=https://github.com/kubernetes/kubernetes/issues/82184>nodeSelector: when will it be deprecated? #82184</a> によると deprecated という文言を含む文章がその後に削除された。Affinity は高機能且つ高コストであることから、(現時点では) nodeSelector はシンプルで推奨すべき方法とまで書いてあるのですぐになくなるわけではなさそう。</p><p>minikube で nodeSelector の検証を始めたんだけど、いくつかうまくいかないことがあって断念した。multi-node 機能を使って controll plane と worker ノードの2つを起動できたけど、worker ノードから docker host にアクセスできなかった。何かしら設定が必要なのか、別途レジストリが必要になるのかよくわからなかった。あと node にラベル設定したときに worker ノードにラベル設定しても minikube を再起動するとそのラベルが消えてしまっていて保持されないようだった。ちょっと調べてローカルの環境を作るのが面倒になったので早々に断念した。</p><ul><li><a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>Using Multi-Node Clusters</a></li></ul></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0706/>lambda から cronjob へ</a></h1><div class=post-meta><time class=post-date>2022-07-06 (Wed.) ::</time></div><span class=post-tags>#<a href=/diary/tags/cdk/>cdk</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>0時に6時に起きた。今日は晴れたので自転車通勤。</p><h2 id=優雅にドキュメントを書きながら障害対応>優雅にドキュメントを書きながら障害対応</h2><p>昨日は凸凹しながら乗り切ってサービスイン2日目。今日から外部システムとの連携なども絡んでくる。昨日の今日なんで何か起こるだろうと思いつつ、暇だったらドキュメント書くタスクがいくつも溜まっているのでそれを片付けるかと業務を始めた。私ぐらいの人間になると、いつ凸凹が発生してもよいように、この日のために取っておいたようなドキュメントタスクがいくつも溜まっている。午前中に私の出番はなく、優雅にドキュメントの1つを完成させた。</p><p>以前から定期実行やバッチ処理は lambda 関数をトリガーに作られていた。それらを <a href=/diary/posts/2022/0609/#serverless-framework-から-cdk-移行の背景>serverless framework から cdk へ移行した</a> んだけど、その後にバッチ処理を <a href=/diary/posts/2022/0622/>k8s の cronjob で実装した</a> 。この cronjob が思いの外、うまくいって、私がフルスクラッチで cli を作っているのだから、私からみてさいきょうのばっちしょりの土台を実装している。定期実行もすべて cronjob でやればいいやんと気付いて、過去に lambda 関数 (cdk/python) で実装したものをすべて移行することに決めた。昨日の流れからわかるように過去に作成済みの一連の lambda 関数はまだ本番環境にデプロイされていない。<a href=/diary/posts/2022/0704/>m1 chip macbook 問題</a> で同僚のマシンからデプロイできないという不運もあったんだけど、もうデプロイしなくていいよ、すべて cronjob で置き換えるからと伝えて2つ移行した。あと半日もあれば完了できそうな見通し。</p><p>lambda 関数から cronjob への移行作業をしていると、本番環境でのバッチ処理の一部のロジックが誤っているとわかってそれを修正したり、当然のようにテスト環境のデータも誤っているので正しいかどうかは本番環境にデプロイするしかないみたいな凸凹した状況を横切りながら本日の作業を終えた。いくつか残課題は残っているものの、明日中には平常業務に戻れるぐらいの状況にはなりつつあるのかもしれない。サービスイン2日目を終えて致命的な問題は起こっていないようにみえる。ひとまずはよかったという感じ。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0629/>cdk で既存の eks クラスターを管理すべきか</a></h1><div class=post-meta><time class=post-date>2022-06-29 (Wed.) ::</time></div><span class=post-tags>#<a href=/diary/tags/cdk/>cdk</a>&nbsp;
#<a href=/diary/tags/infrastructure/>infrastructure</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>0時に寝て6時に起きた。</p><h2 id=cdk-から既存の-eks-クラスターを制御する>cdk から既存の eks クラスターを制御する</h2><p>1ヶ月ほど前に検証していた <a href=/diary/posts/2022/0518/#cdk-のパッチ検証>cdk による eks クラスターの helm 管理</a> を再検証した。kubectlRoleArn にどういった権限をもつ iam role を設定したらよいかがよくわからなくて苦労していた。最終的にそれが理解できて helm 管理もできるようになったのでまとめておく。</p><blockquote><p>kubectlRoleArn - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually. The trust policy of this role should include the the arn:aws::iam::${accountId}:root principal in order to allow the execution role of the kubectl resource to assume it.</p><p><a href=https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters>https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters</a></p></blockquote><p>aws-auth の configmap に設定されている system:masters に所属している iam role を調べる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl describe configmap -n kube-system aws-auth
</span></span></code></pre></div><p>この iam role には <code>sts:AssumeRole</code> 権限を与え、trust relationships に <code>arn:aws:iam::${accountId}:root</code> といった root ユーザーを含める必要がある。この root ユーザーの設定がないと次のような権限エラーが発生する。この権限エラーの修正方法がわからなくて苦労していた。結果的には関係なかった kubectlLambdaRole の設定も必要なんじゃないかと検証していたのが前回の作業の中心だった。</p><pre tabindex=0><code>An error occurred (AccessDenied) when calling the AssumeRole operation:
  User: arn:aws:sts::${accountId}:assumed-role/xxx is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::${accountId}:role/myrole
Error: Kubernetes cluster unreachable: Get &#34;https://xxx.gr7.ap-northeast-1.eks.amazonaws.com/version
</code></pre><p>ようやく cdk で既存の eks クラスターをインポートして helm パッケージを管理できるようになった。とはいえ、cdk/cf の実行時間を測ってみると次のようになった。</p><ul><li>helm パッケージの新規インストール: 約5分</li><li>helm パッケージのアンインストール: 約25分</li></ul><p>これは cdk が helm パッケージを管理するための lambda 環境を構築/削除するときの時間になる。cdk はアプリケーションの stack から nested stack を作成して、そこに lambda や iam role などをまとめて作成する。一度作成してしまえば、バージョンのアップグレードは30秒ほどで完了した。</p><p>この振る舞いを検証した上で、cdk で eks クラスターをインポートする管理はやめようとチームに提案した。正しい設定を作ってしまえば運用は楽になると言える一面もあるが、新規に helm パッケージを追加するときのちょっとした typo や設定ミスなどがあると、1回の試行に30分かかる。私がこの検証に1週間以上のデバッグ時間を割いている理由がそれに相当する。お手伝い先の運用ではテスト/本番環境ともにローカルから接続できる状態なので helm コマンドを直接実行した方が遥かに管理コストや保守コストを下げると言える。cdk を使って嬉しいことは helm コマンドでわかるバージョン情報と設定内容が cdk のコードとして管理されているぐらいでしかない。ドキュメントと helm コマンドで管理する方が現状ではよいだろうと私は結論付けた。同じような理由で eks クラスターも cdk ではなく eksctl コマンドで管理されている。</p><p>1週間以上の労力と時間を費やしてやらない方がよいとわかったという、一般的には失敗と呼ばれる作業に終わったわけだけど、eks/cdk の勉強にはなった。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0625/>k8s のアップグレードをやってみた</a></h1><div class=post-meta><time class=post-date>2022-06-25 (Sat.) ::</time></div><span class=post-tags>#<a href=/diary/tags/life/>life</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/event/>event</a>&nbsp;</span><div class=post-content><p>0時に寝て6時半に起きた。起きてから1時間ほどだらだらしてた。</p><h2 id=ストレッチ>ストレッチ</h2><p>今日の開脚幅は開始前161cmで、ストレッチ後163cmだった。先週と同じなので現状維持とも言えるし、よい状態を維持しているとも言えるかもしれない。もう1年以上通っているせいか、なにかポイントが溜まっていて使わないといけないという話しで今日は20分延長でやってくれた。とは言っても、基本的なストレッチ項目が変わるわけではなく、いつもより伸ばす時間や手順が少し増えているぐらいだった気がする。今週はとくに腰の負荷もあまり感じなかったせいか、いつもの右腰の張りもなかったように思う。トレーナーさんに聞くと、暑くなると筋肉は伸びやすくなるので季節要因でストレッチをしたときの伸び具合が変わるのは普通とのこと。調子がよくなってきたのでこのまま好調を維持したい。</p><h2 id=eks-k8s-のアップグレード>eks (k8s) のアップグレード</h2><p>お手伝い先のお仕事がもうすぐサービスインなのでそれまでにリスクのある作業をやっとこうみたいな状況にある。たまたま eks (k8s) のバージョンを 1.21 から 1.22 にあげようと思い立って、木曜日に提案したら、どんな障害が起きるかわからないので他メンバーがテスト環境を使っていない時間帯で作業した方がよいだろうという話になって土日にやることにした。</p><ul><li><a href=https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html>Updating an Amazon EKS cluster Kubernetes version</a></li></ul><p>何が起きるか分からなくても、土曜日から始めて致命的なトラブルに見舞われても1日もあれば解決できるだろうという見通しで作業を始めた。その見通しも「私がやれば」という前提に成り立っている。良くも悪くも私がやろうと言ったことに反対されることはほとんどないが、それは私が言ったことは一定時間に私がすべてやり切るという信頼に基づいている。本当の意味でできるかどうか分からないことを必要以上に抱え込んでしまうときもあるのでバランス感覚は必要かもしれない。言わばサービス休日出勤だし、なぜ私がやっているかと言うと、システムの運用や保守の展望を考えたら、サービスインの前にインフラのバージョンを上げておく方が将来の保守コストを下げることに繋がるという1点のみに重要性を見い出していて、それをもっとも強く主張しているのが私だからという理由。</p><p>結論から言って2時間でアップグレード作業を完了した。1つ手順漏れがあって、アプリケーションの pod がすべてエラーになるというトラブルに見舞われたものの、すぐ手順漏れに気付いて難なく復旧できた。今日はテスト環境のアップグレードをしたわけだけど、また後日、本番向けの作業手順書を作れば、ほぼタウンタイムなしで1時間もあればアップグレード作業を完了できそうな見通しではある。</p><p>実際はミスもあったので次の順番でやったわけではないが、おそらくこの手順でやれば正しいはず。</p><ol><li>aws cli と eksctl コマンドのインストール</li><li>aws のアップグレードドキュメンを読む</li><li>cert-manager のアップグレード (1.1.1 から 1.5.4)</li><li>aws-load-balancer-controller のアップグレード (2.2.0 から 2.4.2)</li><li>k8s control plane のアップグレード (1.21 から 1.22)</li><li>(オプション: 不要) autoscaler のアップグレード</li><li>(オプション: 不要) gpu サポートノードのアップグレード</li><li>vpc cni プラグインのアップグレード (1.7.5 から 1.11.2)</li><li>coredns プラグインのアップグレード (1.8.4 から 1.8.7)</li><li>kube-proxy のアップグレード (1.21.2 から 1.22.6)</li><li>k8s nodegroup のアップグレード (1.21 から 1.22)<ul><li>k8s ノードが存在する nodegroup をアップグレードするとそのインスタンスが再作成されて pod が再デプロイされる</li></ul></li></ol><p>細かい手順は aws のドキュメントの指示に従いながらやったらできた。add-on と self-managed add-on の種別の違いがあったり、helm と k8s manifest の手順が別々だったり、どのバージョンからのアップグレードかで作業手順が異なったりと、ドキュメントをちゃんと読まないと正しい作業手順がわからない。基本的にはドキュメント通りの作業で完了できた。</p><h2 id=もくもく会>もくもく会</h2><p>アップグレード作業を終えてから1時間ほど残っていたので16時から <a href=https://kobe-sannomiya-dev.connpass.com/event/251117/>【三宮.dev ＆ KELab 共催】もくもく会</a> に参加した。今回は <a href=https://kobe-engr-lab.studio.site/>Kobe Engineers Lab</a> さんと共催ということで <a href=https://120workplace.jp/>120 WORKPLACE KOBE</a> で開催された。Kobe Engineers Lab の主催者の会社が 120 workplace でオフィスを借りているため、会議室を5時間/月まで無料で借りられるという。私も過去に何度か 120 workplace のコワーキングスペースで作業したこともあった。久しぶりに行ってよい場所だとは思う。会議室は初めて入ったけど、10人ぐらいは余裕で作業できる大きなテーブルがあって広くてよかった。終わってからわたなべさんと3時間ほど立ち呑みしてた。</p><h2 id=はんなりビジネス>はんなりビジネス</h2><p>21時から <a href=https://hannari-python.connpass.com/event/250916/>はんなりビジネス #0</a> に参加した。おがわさんがまた新しいことやるんだなと思って興味本位で参加してみた。現実の課題に対してコミュニティの有志を募ってチームで取り組んでみたら、問題解決能力も身についてプログラミングの知識を活かしてより実践的なスキルが身に着いてよいのではないかといったところから始まった企画らしい。今日は初回だったので参加者でどういう取り組みがよいのかを雑談してた。まだまだこの先どうなるかわからないけど、私はあまりこの手の取り組みには懐疑的かなぁ。自分たちにとってちょうどよい課題レベルの対象をみつけるのは難しいし、誰でも参加できるオープンなビジネスコンテストやアイディアソンが本当に大事な問題を扱っているかも怪しい。現実の課題はお仕事でいくらでもあるので、それをコミュニティでやろうと思うとニッチな何かになるか、価値があるかどうかよりも本人がやりたいかどうかの目的になってしまうような気もする。とはいえ、私自身、ビジネス力はまったくないのでなにかしらやっているうちに価値に気付くこともあるかもしれない。もうしばらく様子をみてみる。</p><ul><li><a href=https://chomoku.notion.site/6872dcf2ba8d4f6887cb16f71879f4ca>https://chomoku.notion.site/6872dcf2ba8d4f6887cb16f71879f4ca</a></li></ul></div></article><div class=pagination><div class=pagination__buttons><a href=/diary/tags/kubernetes/ class="button previous"><span class=button__icon>←</span>
<span class=button__text>最近の日記</span></a>
<a href=/diary/tags/kubernetes/page/3/ class="button next"><span class=button__text>過去の日記</span>
<span class=button__icon>→</span></a></div></div></div></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>© 2021 Tetsuya Morimoto</span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script type=text/javascript src=/diary/bundle.min.js></script></div></body></html>