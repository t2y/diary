<!doctype html><html lang=en><head><title>kubernetes :: forest nook</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=/diary/tags/kubernetes/><link rel=stylesheet href=/diary/styles.css><link rel=stylesheet href=/diary/style.css><link rel="shortcut icon" href=/diary/favicon.ico><meta name=twitter:card content="summary"><meta name=twitter:site content="t2y"><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta property="og:title" content="kubernetes"><meta property="og:description" content><meta property="og:url" content="/diary/tags/kubernetes/"><meta property="og:site_name" content="forest nook"><meta property="og:image" content="/diary/favicon.ico"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><link href=/diary/tags/kubernetes/index.xml rel=alternate type=application/rss+xml title="forest nook"></head><body class=green><div class="container full headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/diary><div class=logo>forest nook</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/diary/about>自己紹介</a></li><li><a href=/diary/dates>月別一覧</a></li><li><a href=/diary/tags>タグ一覧</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/diary/about>自己紹介</a></li><li><a href=/diary/dates>月別一覧</a></li><li><a href=/diary/tags>タグ一覧</a></li></ul></nav></header><div class=content><div class=posts><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0711/>nodeSelector を試す</a></h1><div class=post-meta><time class=post-date>2022-07-11 (Mon.) ::</time></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>4時に寝て8時に起きた。久しぶりに寝坊した。</p><h2 id=k8s-の-nodeselector>k8s の nodeSelector</h2><p>先日 <a href=/diary/posts/2022/0706/>定期/バッチ処理を k8s の cronjob にすべて移行</a> した。すでに本番運用もしていて調子もよさそうにみえる。あと残課題としてバッチ処理とアプリケーションサーバーの pod がデプロイされる k8s ノードを分割したい。現時点では、バッチ処理の負荷は小さいから同じスペックのインスタンスの k8s ノード上で混在させて運用している。しかし、いずれ運用上の問題になる懸念がある。そのため、バッチ処理のみを実行する k8s ノードを管理したい。次のドキュメントに書いてある。</p><ul><li><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/>Assigning Pods to Nodes</a></li></ul><p>k8s のドキュメントによると、大きく分けて nodeSelector と Affinity という2つのやり方がある。前者はラベルでフィルターするシンプルな仕組み、後者はさらに複雑な要件に対応するもの。いまのところ、ただ分割できればよいのでシンプルな nodeSelector で実装してみることにした。</p><ul><li>nodeSelector</li><li>Affinity and anti-affinity</li></ul><p>余談だが、nodeSelector はいずれ Affinity に置き換わるので deprecated だと一時期ドキュメントに書かれていたらしい。具体的に決まっていることでもないため、<a href=https://github.com/kubernetes/kubernetes/issues/82184>nodeSelector: when will it be deprecated? #82184</a> によると deprecated という文言を含む文章がその後に削除された。Affinity は高機能且つ高コストであることから、(現時点では) nodeSelector はシンプルで推奨すべき方法とまで書いてあるのですぐになくなるわけではなさそう。</p><p>minikube で nodeSelector の検証を始めたんだけど、いくつかうまくいかないことがあって断念した。multi-node 機能を使って controll plane と worker ノードの2つを起動できたけど、worker ノードから docker host にアクセスできなかった。何かしら設定が必要なのか、別途レジストリが必要になるのかよくわからなかった。あと node にラベル設定したときに worker ノードにラベル設定しても minikube を再起動するとそのラベルが消えてしまっていて保持されないようだった。ちょっと調べてローカルの環境を作るのが面倒になったので早々に断念した。</p><ul><li><a href=https://minikube.sigs.k8s.io/docs/tutorials/multi_node/>Using Multi-Node Clusters</a></li></ul></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0706/>lambda から cronjob へ</a></h1><div class=post-meta><time class=post-date>2022-07-06 (Wed.) ::</time></div><span class=post-tags>#<a href=/diary/tags/cdk/>cdk</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>0時に6時に起きた。今日は晴れたので自転車通勤。</p><h2 id=優雅にドキュメントを書きながら障害対応>優雅にドキュメントを書きながら障害対応</h2><p>昨日は凸凹しながら乗り切ってサービスイン2日目。今日から外部システムとの連携なども絡んでくる。昨日の今日なんで何か起こるだろうと思いつつ、暇だったらドキュメント書くタスクがいくつも溜まっているのでそれを片付けるかと業務を始めた。私ぐらいの人間になると、いつ凸凹が発生してもよいように、この日のために取っておいたようなドキュメントタスクがいくつも溜まっている。午前中に私の出番はなく、優雅にドキュメントの1つを完成させた。</p><p>以前から定期実行やバッチ処理は lambda 関数をトリガーに作られていた。それらを <a href=/diary/posts/2022/0609/#serverless-framework-から-cdk-移行の背景>serverless framework から cdk へ移行した</a> んだけど、その後にバッチ処理を <a href=/diary/posts/2022/0622/>k8s の cronjob で実装した</a> 。この cronjob が思いの外、うまくいって、私がフルスクラッチで cli を作っているのだから、私からみてさいきょうのばっちしょりの土台を実装している。定期実行もすべて cronjob でやればいいやんと気付いて、過去に lambda 関数 (cdk/python) で実装したものをすべて移行することに決めた。昨日の流れからわかるように過去に作成済みの一連の lambda 関数はまだ本番環境にデプロイされていない。<a href=/diary/posts/2022/0704/>m1 chip macbook 問題</a> で同僚のマシンからデプロイできないという不運もあったんだけど、もうデプロイしなくていいよ、すべて cronjob で置き換えるからと伝えて2つ移行した。あと半日もあれば完了できそうな見通し。</p><p>lambda 関数から cronjob への移行作業をしていると、本番環境でのバッチ処理の一部のロジックが誤っているとわかってそれを修正したり、当然のようにテスト環境のデータも誤っているので正しいかどうかは本番環境にデプロイするしかないみたいな凸凹した状況を横切りながら本日の作業を終えた。いくつか残課題は残っているものの、明日中には平常業務に戻れるぐらいの状況にはなりつつあるのかもしれない。サービスイン2日目を終えて致命的な問題は起こっていないようにみえる。ひとまずはよかったという感じ。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0629/>cdk で既存の eks クラスターを管理すべきか</a></h1><div class=post-meta><time class=post-date>2022-06-29 (Wed.) ::</time></div><span class=post-tags>#<a href=/diary/tags/cdk/>cdk</a>&nbsp;
#<a href=/diary/tags/infrastructure/>infrastructure</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>0時に寝て6時に起きた。</p><h2 id=cdk-から既存の-eks-クラスターを制御する>cdk から既存の eks クラスターを制御する</h2><p>1ヶ月ほど前に検証していた <a href=/diary/posts/2022/0518/#cdk-のパッチ検証>cdk による eks クラスターの helm 管理</a> を再検証した。kubectlRoleArn にどういった権限をもつ iam role を設定したらよいかがよくわからなくて苦労していた。最終的にそれが理解できて helm 管理もできるようになったのでまとめておく。</p><blockquote><p>kubectlRoleArn - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually. The trust policy of this role should include the the arn:aws::iam::${accountId}:root principal in order to allow the execution role of the kubectl resource to assume it.</p><p><a href=https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters>https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters</a></p></blockquote><p>aws-auth の configmap に設定されている system:masters に所属している iam role を調べる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl describe configmap -n kube-system aws-auth
</span></span></code></pre></div><p>この iam role には <code>sts:AssumeRole</code> 権限を与え、trust relationships に <code>arn:aws:iam::${accountId}:root</code> といった root ユーザーを含める必要がある。この root ユーザーの設定がないと次のような権限エラーが発生する。この権限エラーの修正方法がわからなくて苦労していた。結果的には関係なかった kubectlLambdaRole の設定も必要なんじゃないかと検証していたのが前回の作業の中心だった。</p><pre tabindex=0><code>An error occurred (AccessDenied) when calling the AssumeRole operation:
  User: arn:aws:sts::${accountId}:assumed-role/xxx is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::${accountId}:role/myrole
Error: Kubernetes cluster unreachable: Get &#34;https://xxx.gr7.ap-northeast-1.eks.amazonaws.com/version
</code></pre><p>ようやく cdk で既存の eks クラスターをインポートして helm パッケージを管理できるようになった。とはいえ、cdk/cf の実行時間を測ってみると次のようになった。</p><ul><li>helm パッケージの新規インストール: 約5分</li><li>helm パッケージのアンインストール: 約25分</li></ul><p>これは cdk が helm パッケージを管理するための lambda 環境を構築/削除するときの時間になる。cdk はアプリケーションの stack から nested stack を作成して、そこに lambda や iam role などをまとめて作成する。一度作成してしまえば、バージョンのアップグレードは30秒ほどで完了した。</p><p>この振る舞いを検証した上で、cdk で eks クラスターをインポートする管理はやめようとチームに提案した。正しい設定を作ってしまえば運用は楽になると言える一面もあるが、新規に helm パッケージを追加するときのちょっとした typo や設定ミスなどがあると、1回の試行に30分かかる。私がこの検証に1週間以上のデバッグ時間を割いている理由がそれに相当する。お手伝い先の運用ではテスト/本番環境ともにローカルから接続できる状態なので helm コマンドを直接実行した方が遥かに管理コストや保守コストを下げると言える。cdk を使って嬉しいことは helm コマンドでわかるバージョン情報と設定内容が cdk のコードとして管理されているぐらいでしかない。ドキュメントと helm コマンドで管理する方が現状ではよいだろうと私は結論付けた。同じような理由で eks クラスターも cdk ではなく eksctl コマンドで管理されている。</p><p>1週間以上の労力と時間を費やしてやらない方がよいとわかったという、一般的には失敗と呼ばれる作業に終わったわけだけど、eks/cdk の勉強にはなった。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0625/>k8s のアップグレードをやってみた</a></h1><div class=post-meta><time class=post-date>2022-06-25 (Sat.) ::</time></div><span class=post-tags>#<a href=/diary/tags/life/>life</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/event/>event</a>&nbsp;</span><div class=post-content><p>0時に寝て6時半に起きた。起きてから1時間ほどだらだらしてた。</p><h2 id=ストレッチ>ストレッチ</h2><p>今日の開脚幅は開始前161cmで、ストレッチ後163cmだった。先週と同じなので現状維持とも言えるし、よい状態を維持しているとも言えるかもしれない。もう1年以上通っているせいか、なにかポイントが溜まっていて使わないといけないという話しで今日は20分延長でやってくれた。とは言っても、基本的なストレッチ項目が変わるわけではなく、いつもより伸ばす時間や手順が少し増えているぐらいだった気がする。今週はとくに腰の負荷もあまり感じなかったせいか、いつもの右腰の張りもなかったように思う。トレーナーさんに聞くと、暑くなると筋肉は伸びやすくなるので季節要因でストレッチをしたときの伸び具合が変わるのは普通とのこと。調子がよくなってきたのでこのまま好調を維持したい。</p><h2 id=eks-k8s-のアップグレード>eks (k8s) のアップグレード</h2><p>お手伝い先のお仕事がもうすぐサービスインなのでそれまでにリスクのある作業をやっとこうみたいな状況にある。たまたま eks (k8s) のバージョンを 1.21 から 1.22 にあげようと思い立って、木曜日に提案したら、どんな障害が起きるかわからないので他メンバーがテスト環境を使っていない時間帯で作業した方がよいだろうという話になって土日にやることにした。</p><ul><li><a href=https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html>Updating an Amazon EKS cluster Kubernetes version</a></li></ul><p>何が起きるか分からなくても、土曜日から始めて致命的なトラブルに見舞われても1日もあれば解決できるだろうという見通しで作業を始めた。その見通しも「私がやれば」という前提に成り立っている。良くも悪くも私がやろうと言ったことに反対されることはほとんどないが、それは私が言ったことは一定時間に私がすべてやり切るという信頼に基づいている。本当の意味でできるかどうか分からないことを必要以上に抱え込んでしまうときもあるのでバランス感覚は必要かもしれない。言わばサービス休日出勤だし、なぜ私がやっているかと言うと、システムの運用や保守の展望を考えたら、サービスインの前にインフラのバージョンを上げておく方が将来の保守コストを下げることに繋がるという1点のみに重要性を見い出していて、それをもっとも強く主張しているのが私だからという理由。</p><p>結論から言って2時間でアップグレード作業を完了した。1つ手順漏れがあって、アプリケーションの pod がすべてエラーになるというトラブルに見舞われたものの、すぐ手順漏れに気付いて難なく復旧できた。今日はテスト環境のアップグレードをしたわけだけど、また後日、本番向けの作業手順書を作れば、ほぼタウンタイムなしで1時間もあればアップグレード作業を完了できそうな見通しではある。</p><p>実際はミスもあったので次の順番でやったわけではないが、おそらくこの手順でやれば正しいはず。</p><ol><li>aws cli と eksctl コマンドのインストール</li><li>aws のアップグレードドキュメンを読む</li><li>cert-manager のアップグレード (1.1.1 から 1.5.4)</li><li>aws-load-balancer-controller のアップグレード (2.2.0 から 2.4.2)</li><li>k8s control plane のアップグレード (1.21 から 1.22)</li><li>(オプション: 不要) autoscaler のアップグレード</li><li>(オプション: 不要) gpu サポートノードのアップグレード</li><li>vpc cni プラグインのアップグレード (1.7.5 から 1.11.2)</li><li>coredns プラグインのアップグレード (1.8.4 から 1.8.7)</li><li>kube-proxy のアップグレード (1.21.2 から 1.22.6)</li><li>k8s nodegroup のアップグレード (1.21 から 1.22)<ul><li>k8s ノードが存在する nodegroup をアップグレードするとそのインスタンスが再作成されて pod が再デプロイされる</li></ul></li></ol><p>細かい手順は aws のドキュメントの指示に従いながらやったらできた。add-on と self-managed add-on の種別の違いがあったり、helm と k8s manifest の手順が別々だったり、どのバージョンからのアップグレードかで作業手順が異なったりと、ドキュメントをちゃんと読まないと正しい作業手順がわからない。基本的にはドキュメント通りの作業で完了できた。</p><h2 id=もくもく会>もくもく会</h2><p>アップグレード作業を終えてから1時間ほど残っていたので16時から <a href=https://kobe-sannomiya-dev.connpass.com/event/251117/>【三宮.dev ＆ KELab 共催】もくもく会</a> に参加した。今回は <a href=https://kobe-engr-lab.studio.site/>Kobe Engineers Lab</a> さんと共催ということで <a href=https://120workplace.jp/>120 WORKPLACE KOBE</a> で開催された。Kobe Engineers Lab の主催者の会社が 120 workplace でオフィスを借りているため、会議室を5時間/月まで無料で借りられるという。私も過去に何度か 120 workplace のコワーキングスペースで作業したこともあった。久しぶりに行ってよい場所だとは思う。会議室は初めて入ったけど、10人ぐらいは余裕で作業できる大きなテーブルがあって広くてよかった。終わってからわたなべさんと3時間ほど立ち呑みしてた。</p><h2 id=はんなりビジネス>はんなりビジネス</h2><p>21時から <a href=https://hannari-python.connpass.com/event/250916/>はんなりビジネス #0</a> に参加した。おがわさんがまた新しいことやるんだなと思って興味本位で参加してみた。現実の課題に対してコミュニティの有志を募ってチームで取り組んでみたら、問題解決能力も身についてプログラミングの知識を活かしてより実践的なスキルが身に着いてよいのではないかといったところから始まった企画らしい。今日は初回だったので参加者でどういう取り組みがよいのかを雑談してた。まだまだこの先どうなるかわからないけど、私はあまりこの手の取り組みには懐疑的かなぁ。自分たちにとってちょうどよい課題レベルの対象をみつけるのは難しいし、誰でも参加できるオープンなビジネスコンテストやアイディアソンが本当に大事な問題を扱っているかも怪しい。現実の課題はお仕事でいくらでもあるので、それをコミュニティでやろうと思うとニッチな何かになるか、価値があるかどうかよりも本人がやりたいかどうかの目的になってしまうような気もする。とはいえ、私自身、ビジネス力はまったくないのでなにかしらやっているうちに価値に気付くこともあるかもしれない。もうしばらく様子をみてみる。</p><ul><li><a href=https://chomoku.notion.site/6872dcf2ba8d4f6887cb16f71879f4ca>https://chomoku.notion.site/6872dcf2ba8d4f6887cb16f71879f4ca</a></li></ul></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0622/>k8s の cronjob を検証中</a></h1><div class=post-meta><time class=post-date>2022-06-22 (Wed.) ::</time></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/cli/>cli</a>&nbsp;</span><div class=post-content><p>0時に寝て6時に起きた。寝不足を解消して体調が戻ってきた。</p><h2 id=k8s-の-cronjob>k8s の cronjob</h2><p>バッチ処理を <a href=https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/>Kubernetes: CronJob</a> で作る。一通り設定して minikube で検証して eks 上でも動くようになった。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>batch/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>CronJob</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app-hourly-job</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>schedule</span>: <span style=color:#e6db74>&#34;5 */1 * * *&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>concurrencyPolicy</span>: <span style=color:#ae81ff>Forbid</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>startingDeadlineSeconds</span>: <span style=color:#ae81ff>600</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>jobTemplate</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>backoffLimit</span>: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>app</span>: <span style=color:#ae81ff>my-app-hourly</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>annotations</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>dapr.io/enabled</span>: <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>dapr.io/app-id</span>: <span style=color:#e6db74>&#34;my-app-hourly&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>          - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-app-hourly-job</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>image</span>: <span style=color:#ae81ff>my-app-image</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>imagePullPolicy</span>: <span style=color:#ae81ff>Always</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>env</span>:
</span></span><span style=display:flex><span>            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>BATCH_ENV</span>
</span></span><span style=display:flex><span>              <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;dev&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>command</span>:
</span></span><span style=display:flex><span>            - <span style=color:#e6db74>&#34;/bin/sh&#34;</span>
</span></span><span style=display:flex><span>            - <span style=color:#e6db74>&#34;/app/scripts/my-app.sh&#34;</span>
</span></span><span style=display:flex><span>            - <span style=color:#e6db74>&#34;param1&#34;</span>
</span></span><span style=display:flex><span>            - <span style=color:#e6db74>&#34;param2&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>restartPolicy</span>: <span style=color:#ae81ff>Never</span>
</span></span></code></pre></div><p>command の設定がわかりにくい。さらに k8s のドキュメントのサンプル設定も誤解を招くような例になっている。どうも実行できるのは1つの cli だけで、複数コマンドを指定できるわけではない。シェルスクリプトを docker イメージに含めて、そこで任意のスクリプトを実装した方がよいだろう。</p><ul><li>&ldquo;/bin/sh&rdquo;</li><li>&ldquo;/app/scripts/my-app.sh&rdquo;</li><li>&ldquo;param1&rdquo;</li><li>&ldquo;param2&rdquo;</li></ul><p>この設定は次の cli として実行される。</p><blockquote><p>/bin/sh /app/scripts/my-app.sh param1 param2</p></blockquote><p><a href=https://stackoverflow.com/questions/51657105/how-to-ensure-kubernetes-cronjob-does-not-restart-on-failure>How to ensure kubernetes cronjob does not restart on failure</a> によると、バッチ処理が失敗したときに再実行したくないときは次の3つの設定をする。</p><ul><li>concurrencyPolicy: Forbid</li><li>backoffLimit: 0</li><li>restartPolicy: Never</li></ul><p>restartPolicy が Never 以外だと、エラーが発生すると永遠に再実行されてしまうので障害時に2次被害を増やしてしまう懸念があったような気がする。</p><p>あと、うちの環境は dapr 経由で他の pod サービスと通信しているので dapr を有効にしないと pod 間通信ができない。dapr はデーモンでずっと起動しているからバッチ処理の終了時に daprd も shutdown してやらないといけない。<a href=https://docs.dapr.io/operations/hosting/kubernetes/kubernetes-job/>Running Dapr with a Kubernetes Job</a> にその方法が書いてある。daprd を shutdown しないと、pod のステータスが NotReady のままで Completed にならない。</p><p>まだまだよくわかってないので <a href=https://kubernetes.io/docs/concepts/workloads/controllers/job/>Jobs</a> のドキュメントに一通り目を通そうと思っている。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0524/>法人として消費税を納めた</a></h1><div class=post-meta><time class=post-date>2022-05-24 (Tue.) ::</time></div><span class=post-tags>#<a href=/diary/tags/founding/>founding</a>&nbsp;
#<a href=/diary/tags/tax/>tax</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>5時に寝て7時過ぎに起きた。前日の夜から法人決算の電子申告に取り組み始めた。本当は紙でやるつもりだったんだけど、eltax が快適だったので e-tax も衝動的にやってみたくなった。</p><h2 id=消費税と地方消費税の申告>消費税と地方消費税の申告</h2><p><a href=/diary/posts/2022/0505/#法人決算>法人決算</a> の一部。今回が初めての消費税と地方消費税の申告になる。簡易課税で支払う。</p><blockquote><p>消費税は、国税（国に納付する税金）であり消費税の納税義務がある事業者が納付します。地方消費税とは、 <strong>消費税と同様で商品の販売やサービスの提供などの取引にかかる税金</strong> です。消費税との違いは、 <strong>地方消費税は国税ではなく地方税（都道府県や市町村に納付する税金）という点です。</strong> しかし実際に納付するときは消費税と分けて納付はせずに、 <strong>消費税と一緒に地方消費税を所管税務署へ納付します。</strong></p><p><a href=https://biz.moneyforward.com/tax_return/basic/70/#i>消費税と地方消費税の違いは？納付対象者や納付方法、計算の仕方まで徹底解説！</a></p></blockquote><p>freee で出力した書類をみながら e-tax の画面で同じ書類の項目を埋めていくだけの作業。1つだけバリデーションエラーが発生して、何度やり直しても数値は正しいようにみえるので無視して処理を継続することにした。メッセージにも値が正しければ継続してくださいと書いてあるのでバリデーションがバグっているのだろうと推測する。書類を作成して、署名して、送信して、納付情報が返ってきて、pay-easy で納付額を振り込む。1時間ほどで完了できた。</p><h2 id=eks-k8s-から-alb-の管理>eks (k8s) から alb の管理</h2><p>eks (k8s) に aws-load-balancer-controller をインストールすると k8s 上のリソースとして alb を管理できるようになる。</p><ul><li><a href=https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/aws-load-balancer-controller.html>AWS Load Balancer Controller アドオンのインストール</a></li></ul><p>具体的には k8s の Ingress と Nodepoint リソースから次の3つのリソースを生成してくれる。</p><ul><li>application load balancer</li><li>http listener</li><li>target groups</li></ul><p>alb からのヘルスチェックは次のようにエンドポイントを記述する。spring boot だと <a href=https://docs.spring.io/spring-boot/docs/current/actuator-api/htmlsingle/>Actuator</a> という web api がヘルスチェックの機能を提供している。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>alb.ingress.kubernetes.io/healthcheck-path</span>: <span style=color:#ae81ff>/actuator/health</span>
</span></span></code></pre></div><p><code>alb.ingress.kubernetes.io/scheme</code> の設定で alb を配置するサブネットを指定できる。デフォルトは <code>internal</code> になる。</p><p>private subnet に配置するとき</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>alb.ingress.kubernetes.io/scheme</span>: <span style=color:#ae81ff>internal</span>
</span></span></code></pre></div><p>public subnet に配置するとき</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>alb.ingress.kubernetes.io/scheme</span>: <span style=color:#ae81ff>internet-facing</span>
</span></span></code></pre></div></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0511/>helm を調べた</a></h1><div class=post-meta><time class=post-date>2022-05-11 (Wed.) ::</time></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/helm/>helm</a>&nbsp;</span><div class=post-content><p>1時に寝て6時半に起きて8時に起きた。前日は資料づくりで遅くまでオフィスに残っていたせいか、なんか寝坊した。</p><h2 id=helm-調査>helm 調査</h2><p><a href=/diary/posts/2022/0201/#kubernetes-のログ管理と-datadog-agent-のログ連携不具合>k8s 上の datadog-agent</a> が <a href=https://helm.sh/>helm</a> で管理されていて、あるバージョンから <a href=https://docs.dapr.io/operations/hosting/kubernetes/kubernetes-deploy/#install-with-helm-advanced>dapr も helm 管理できる</a> ようになった。dapr は cli からもインストールできるけど、helm のことをよくわかってなかったので調べることにした。そんなたくさん記事をみたわけではないけど、いくつか記事を読んで quora のやり取りが一番よかった。</p><ul><li><a href=https://www.quora.com/When-should-you-use-Kubernetes-Helm-and-not-use-it>When should you use Kubernetes Helm and not use it?</a></li></ul><p>ざっくりまとめるとこうかな。</p><ul><li>helm は oss 且つ cncf の公式プロジェクトだからまぁ安心</li><li>helm はサードパーティのパッケージのインストールや設定の利便性を高める<ul><li>k8s はテンプレート機能が弱いので共通設定と特定環境向けの設定を管理するのがあまり得意ではない</li><li>セキュリティを考慮した k8s 設定は自分でやるよりコミュニティに任せた方がよい場合もある</li><li>パッケージなのでバージョン管理は得意</li></ul></li><li>helm は k8s 向けのパッケージマネージャとレポジトリマネージャーとマーケットプレイスを組み合わせたみたいなもの</li></ul><p>k8s 上でサードパーティのパッケージを自分で設定したい特別な理由がない限りは helm を使うのがよさそうという結論になった。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0324/>spring boot の環境とログ設定</a></h1><div class=post-meta><time class=post-date>2022-03-24 (Thu.) ::</time></div><span class=post-tags>#<a href=/diary/tags/spring-boot/>spring boot</a>&nbsp;
#<a href=/diary/tags/logging/>logging</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>0時に寝て4時に起きて6時に起きた。</p><h2 id=spring-のプロファイル設定>spring のプロファイル設定</h2><p>spring の <a href=https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.profiles>Profiles</a> の仕組みを使って環境ごとの設定を作る。デプロイは k8s で管理しているため、spring boot の <a href=https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.external-config>Externalized Configuration</a> の仕組みを使って、環境変数から application.yml に定義された設定を書き換える。k8s は kustomize で管理していて prod, test, dev の3つの環境で任意の設定を記述できる。</p><p>問題はログ出力の設定を環境ごとに変えたい。具体的には datadog に連携されるログは構造化ログ (json lines) を、ローカルの開発ではコンソールログをみたい。<a href=https://logging.apache.org/log4j/2.x/log4j-spring-boot/index.html>Log4j Spring Boot Support</a> によると、1つの設定ファイルに複数のプロファイル設定を記述できるようにもみえるけど、実際にやってみたらうまく動かなかった。xml ではなく yml を使っているせいかもしれないし、私の記述方法が誤っているのかもしれない。いずれにしても yml で複数のプロファイルを設定しているサンプルをみつけられなかった。</p><p>そこで <a href=https://www.baeldung.com/spring-log4j2-config-per-profile>Different Log4j2 Configurations per Spring Profile</a> をみて、環境ごとにログ設定ファイルも分割することにした。application.yml には次のように記述する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span><span style=color:#f92672>spring</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>profiles</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>active</span>: <span style=color:#ae81ff>dev</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>logging</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>config</span>: <span style=color:#ae81ff>classpath:log4j2-${spring.profiles.active}.yml</span>
</span></span></code></pre></div><p>ローカル開発向けの lgo4j2-dev.yml は次のようになる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span><span style=color:#f92672>Configuration</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>status</span>: <span style=color:#ae81ff>warn</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>YAMLConfig</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>appenders</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>Console</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>STDOUT</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>target</span>: <span style=color:#ae81ff>SYSTEM_OUT</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>PatternLayout</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>Pattern</span>: <span style=color:#e6db74>&#34;%d{yyyy-MM-dd HH:mm:ss.SSS}[%t]%-5level %logger{36} - %msg%n&#34;</span>
</span></span></code></pre></div><p>k8s のマニフェストで環境変数を次のように定義すれば prod というプロファイルが設定される。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>env</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>spring.profiles.active</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;prod&#34;</span>
</span></span></code></pre></div><p>クラウド環境向けの log4j2-prod.yml は次のようになる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span><span style=color:#f92672>Configuration</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>status</span>: <span style=color:#ae81ff>warn</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>YAMLConfig</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>appenders</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>Console</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>STDOUT</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>target</span>: <span style=color:#ae81ff>SYSTEM_OUT</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>EcsLayout</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>serviceName</span>: <span style=color:#ae81ff>my-service</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>serviceNodeName</span>: <span style=color:#66d9ef>null</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>includeMarkers</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>KeyValuePair</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>key</span>: <span style=color:#ae81ff>type</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#ae81ff>app</span>
</span></span></code></pre></div></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0313/>k8s のロールバック</a></h1><div class=post-meta><time class=post-date>2022-03-13 (Sun.) ::</time></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>0時に寝て7時に起きた。</p><h2 id=k8s-のロールバック>k8s のロールバック</h2><p><a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-to-a-previous-revision>Rolling Back to a Previous Revision</a> をみながらすぐできた。ロールバックもこれまでと同様、github actions の workflow dispatch で管理できるようにした。基本的にはこれだけでロールバックできる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl rollout undo deployment/my-app-deploy
</span></span></code></pre></div><p>ちょっと工夫したこととして、デプロイ時に <a href=https://kubernetes.io/docs/reference/labels-annotations-taints/#change-cause>kubernetes.io/change-cause</a> というアノテーションに git のリビジョンもセットしておくと確認するときにちょっと楽ができる。apply した後の deployment リソースに docker イメージのタグ情報 (= git のリビジョン) を書き込んでおく。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -k <span style=color:#e6db74>${</span>{ env.DEPLOYMENT_ENV <span style=color:#e6db74>}</span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>$ kubectl annotate deployment my-app-deploy kubernetes.io/change-cause<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>{ env.IMAGE_TAG <span style=color:#e6db74>}</span><span style=color:#f92672>}</span> --overwrite<span style=color:#f92672>=</span>true
</span></span></code></pre></div><p>kubectl から履歴をみたときに k8s のリビジョンがどの git のリビジョンを使っているかがわかりやすい。デフォルトでは何も設定されていないかもしれない。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl rollout history deployment/my-app-deploy
</span></span><span style=display:flex><span>deployment.apps/my-app-deploy
</span></span><span style=display:flex><span>REVISION  CHANGE-CAUSE
</span></span><span style=display:flex><span><span style=color:#ae81ff>15</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>16</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>17</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>18</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>19</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>20</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>21</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>22</span>       &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>24</span>       1f17a22a6659ea0714a21fca034645cd191e189b
</span></span><span style=display:flex><span><span style=color:#ae81ff>27</span>       a84e113d8b7c124178b58e2f40f57b00aae65485 
</span></span><span style=display:flex><span><span style=color:#ae81ff>28</span>       dcf3552db0668d416ed880f6e896455d7bab194c
</span></span></code></pre></div></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0228/>kustomize の動的な設定</a></h1><div class=post-meta><time class=post-date>2022-02-28 (Mon.) ::</time></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>23時に寝て2時に起きてそこから断続的に寝たり起きたりを繰り返して6時に半に起きた。ウクライナ情勢のニュースや記事ばかり読んでてあてられた。</p><h2 id=kustomize-の動的な設定>kustomize の動的な設定</h2><p>kustomize で管理している image のタグをデプロイ処理の中で動的に変更したい。パラメーター渡しなり環境変数なり、なにかしらあるだろうとは予測している。<a href=https://github.com/kubernetes-sigs/kustomize/blob/master/examples/image.md>Demo: change image names and tags</a> のサンプルによると、次のように実行すればよいみたい。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kustomize edit set image busybox<span style=color:#f92672>=</span>alpine:3.6
</span></span></code></pre></div><p>次のような <code>kustomization.yaml</code> をセットしてくれるみたい。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=display:flex><span><span style=color:#f92672>images</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>busybox</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>newName</span>: <span style=color:#ae81ff>alpine</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>newTag</span>: <span style=color:#ae81ff>3.6</span>
</span></span></code></pre></div><p>タグのところをリビジョン指定できればいいだけなのでとくに SECRET を使う必要もない。このやり方で書き換えた <code>newTag</code> が POD のデプロイ対象になってくれればよいはず。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0201/>wiki のドキュメント整理</a></h1><div class=post-meta><time class=post-date>2022-02-01 (Tue.) ::</time></div><span class=post-tags>#<a href=/diary/tags/datadog/>datadog</a>&nbsp;
#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;</span><div class=post-content><p>23時に寝て4時半に起きた。昨日の帰りに自転車でこけて胸を強打してひたすら痛い。起き上がるのも痛い。安静にしてた。</p><h2 id=kubernetes-のログ管理と-datadog-agent-のログ連携不具合>kubernetes のログ管理と datadog-agent のログ連携不具合</h2><p>先日、<a href=/diary/posts/2022/0127/#ログ連携の不具合調査>datadog にログ連携されていない不具合</a> が発生していて、その1次調査を終えたことについて書いた。緊急対応としては datadog-agent を再起動することで改善することはわかっていたので、その後、kubernetes のログ管理と datadog-agent がどうやって kubernetes クラスター上で実行されているアプリケーションのログを収集しているかを調査していた。今日は wiki に調査してわかったことなどをまとめていた。</p><p>kubernetes クラスターはコンテナランタイムに docker を使っていて、アプリケーションの stdout/stderr を docker の logging driver にリダイレクトし、JSON Lines に設定された logging driver が kubernetes ノード上にログファイルとして出力する。datadog-agent は autodiscovery 機能で pod の情報を常にポーリングしていて、pod が新たにデプロイされたらログファイルを pod 内にマウントして、そのマウントしたログファイルを読み込んでログ収集していると思われる。datadog-agent から pod の情報を取得するには kubernetes のサービスアカウントを使っていて、その credential が projected volume としてマウントされて pod 内から利用できる。その credential を使って kubelet api にリクエストすることで pod の情報を取得している。</p><p>文章で書けばたったこれだけのことなんだけど、たったこれだけのことを理解するのに次のドキュメントを読んだ。実際の調査のときはわからなかったのでもっと多くのドキュメントを読んでいる。いま書いたことを理解するならこのドキュメントを読めば理解できるはず。</p><ul><li><a href=https://kubernetes.io/docs/concepts/overview/components/>https://kubernetes.io/docs/concepts/overview/components/</a></li><li><a href=https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/>https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/</a></li><li><a href=https://kubernetes.io/docs/concepts/cluster-administration/logging/>https://kubernetes.io/docs/concepts/cluster-administration/logging/</a></li><li><a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/>https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/</a></li><li><a href=https://kubernetes.io/docs/concepts/storage/projected-volumes/>https://kubernetes.io/docs/concepts/storage/projected-volumes/</a></li><li><a href="https://docs.datadoghq.com/agent/kubernetes/log/?tab=helm">https://docs.datadoghq.com/agent/kubernetes/log/?tab=helm</a></li><li><a href="https://docs.datadoghq.com/getting_started/agent/autodiscovery/?tab=kubernetes">https://docs.datadoghq.com/getting_started/agent/autodiscovery/?tab=kubernetes</a></li></ul><p>ドキュメントに書いてあることを深く理解するために、kubernetes と datadog-agent のソースコードも読んだ。どちらも go 言語で実装されている。</p><ul><li><a href=https://github.com/kubernetes/kubernetes>https://github.com/kubernetes/kubernetes</a></li><li><a href=https://github.com/DataDog/datadog-agent>https://github.com/DataDog/datadog-agent</a></li></ul><p><code>kubectl logs</code> の振る舞いを確認するだけでも、ソースコードからは実際のログファイルを open してストリームを返しているところはわからなかった。api 呼び出しが連携されて抽象化されていて、コンポーネントの役割分担があって、何も知らずにコードを読んでいてもわからなかった。Kubernetes の低レイヤーのところは Container Runtime Interface (CRI) という標準化を行い、1.20 から docker は非推奨となり、将来的に CRI を提供する実装に置き換わるらしい。ログファイルを open する役割は CRI の実装が担うんじゃないかと思うけど、そこまでは調べきれなかった。また機会があれば CRI の実装も読んでみる。</p><figure><img src=/diary/img/2022/0201_kubectl-logs.png></figure></div></article><div class=pagination><div class=pagination__buttons><a href=/diary/tags/kubernetes/ class="button previous"><span class=button__icon>←</span>
<span class=button__text>最近の日記</span></a>
<a href=/diary/tags/kubernetes/page/3/ class="button next"><span class=button__text>過去の日記</span>
<span class=button__icon>→</span></a></div></div></div></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>© 2021 Tetsuya Morimoto</span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script type=text/javascript src=/diary/bundle.min.js></script></div></body></html>