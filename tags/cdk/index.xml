<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cdk on forest nook</title><link>/diary/tags/cdk/</link><description>Recent content in cdk on forest nook</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><copyright>© 2021 Tetsuya Morimoto</copyright><lastBuildDate>Wed, 25 May 2022 08:11:17 +0900</lastBuildDate><atom:icon>/diary/favicon.ico</atom:icon><icon>/diary/favicon.ico</icon><atom:link href="/diary/tags/cdk/index.xml" rel="self" type="application/rss+xml"/><item><title>0525</title><link>/diary/posts/2022/0525/</link><pubDate>Wed, 25 May 2022 08:11:17 +0900</pubDate><guid>/diary/posts/2022/0525/</guid><description>22時に寝て1時に起きて5時半に起きた。前日はあまり寝てなかったので知らないうちに寝てしまった。
severless framework と cdk の比較 serverless framework というツールがある。マルチクラウド対応で aws で言えば lambda のような、サーバーレスのアプリケーションを簡単にデプロイするためのツール。よく使われているようで、私がお手伝いしている会社でも lambda 関数のデプロイにこのツールを使っている。いろいろ調べていたら、serverless framework は cdk と真正面から競合になるツールで、cdk をすでに使っているなら cdk に一元管理した方が保守コストが下がっていいだろうと考え、新たに lambda 関数を作る作業を cdk でやってみた。私はほとんど serverless framework を使ったことがないので正しく理解できていない可能性は高いが、ざっくり比較すると次になる。
serverless framework メリット
cdk より学習コストが低い yaml 設定だけで簡単にデプロイできる デメリット
リソース管理のための s3 バケットを必要とする lambda に関連するリソースしかデプロイできない 開発者が明示的に設定しなくても裏でリソースを勝手に生成するので意図せず適切に管理されていないインフラを作ってしまう懸念がある 大半の実務レベルのアプリケーションでは cf テンプレートの dsl を yaml に設定する必要があり、設定が煩雑になったり見通しが悪くなる cdk メリット
任意の aws インフラのリソースを管理できる プログラミング言語で記述できるので動的なリソースの依存関係を定義できる cf は裏に隠蔽されているが、serverless framework のような dsl を記述する必要はない デメリット
学習コストが高い リファレンス</description><content>&lt;p>22時に寝て1時に起きて5時半に起きた。前日はあまり寝てなかったので知らないうちに寝てしまった。&lt;/p>
&lt;h2 id="severless-framework-と-cdk-の比較">severless framework と cdk の比較&lt;/h2>
&lt;p>&lt;a href="https://www.serverless.com/">serverless framework&lt;/a> というツールがある。マルチクラウド対応で aws で言えば lambda のような、サーバーレスのアプリケーションを簡単にデプロイするためのツール。よく使われているようで、私がお手伝いしている会社でも lambda 関数のデプロイにこのツールを使っている。いろいろ調べていたら、serverless framework は cdk と真正面から競合になるツールで、cdk をすでに使っているなら cdk に一元管理した方が保守コストが下がっていいだろうと考え、新たに lambda 関数を作る作業を cdk でやってみた。私はほとんど serverless framework を使ったことがないので正しく理解できていない可能性は高いが、ざっくり比較すると次になる。&lt;/p>
&lt;h4 id="serverless-framework">serverless framework&lt;/h4>
&lt;p>メリット&lt;/p>
&lt;ul>
&lt;li>cdk より学習コストが低い&lt;/li>
&lt;li>yaml 設定だけで簡単にデプロイできる&lt;/li>
&lt;/ul>
&lt;p>デメリット&lt;/p>
&lt;ul>
&lt;li>リソース管理のための s3 バケットを必要とする&lt;/li>
&lt;li>lambda に関連するリソースしかデプロイできない&lt;/li>
&lt;li>開発者が明示的に設定しなくても裏でリソースを勝手に生成するので意図せず適切に管理されていないインフラを作ってしまう懸念がある&lt;/li>
&lt;li>大半の実務レベルのアプリケーションでは cf テンプレートの dsl を yaml に設定する必要があり、設定が煩雑になったり見通しが悪くなる&lt;/li>
&lt;/ul>
&lt;h4 id="cdk">cdk&lt;/h4>
&lt;p>メリット&lt;/p>
&lt;ul>
&lt;li>任意の aws インフラのリソースを管理できる&lt;/li>
&lt;li>プログラミング言語で記述できるので動的なリソースの依存関係を定義できる&lt;/li>
&lt;li>cf は裏に隠蔽されているが、serverless framework のような dsl を記述する必要はない&lt;/li>
&lt;/ul>
&lt;p>デメリット&lt;/p>
&lt;ul>
&lt;li>学習コストが高い&lt;/li>
&lt;/ul>
&lt;p>リファレンス&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.secjuice.com/aws-cdk-vs-serverless-framework/">AWS CDK vs Serverless Framework&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://dev.to/tastefulelk/serverless-framework-vs-sam-vs-aws-cdk-1g9g">Serverless Framework vs SAM vs AWS CDK&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>結論から言うと、将来的に cdk を使うならまず cdk を使った方がよい。本当にシンプルな要件で lambda 関数のインフラしか扱わないなら serverless framework でもいいかもしれない。serverless framework は cdk がない時代に作られたツールだろうからいまから新規に導入する場合は、多少の学習コストを払っても cdk を学んでおけば、将来的に役に立つ場面が多いと思う。&lt;/p></content></item><item><title>cdk のビルドが難しい話し</title><link>/diary/posts/2022/0518/</link><pubDate>Wed, 18 May 2022 09:55:29 +0900</pubDate><guid>/diary/posts/2022/0518/</guid><description>0時に寝て6時半に起きた。暑くなってきたせいかバテ気味。水曜日は本番リリースの日。3つほど本場環境のインフラ移行作業があったので社員さんの実作業をリモートから指示しながらサポートしていた。私に本番環境の操作権限があれば、この工数はほぼ半分に削減できる。
cdk のパッチ検証 先日 cdk による eks クラスターの helm 管理の調査中断 について書いた。バグっているから適切な設定が反映されないという話しで一時中断していたんだけど、そのときにクラスメソッドの担当者さんとも相談していた。そしたら、その担当者さんが問題を修正して pr を送ってくれた。感謝。
fix(eks): Cluster.FromClusterAttributes ignores KubectlLambdaRole #20373 そこまでやってくれると思ってなかったのですごいなぁと感心しながら、せっかくなのでパッチを当てた cdk で検証してみようと、Contributing to the AWS Cloud Development Kit をみながらローカルでのビルドを試みた。ビルド自体はできたんだけど、パッケージを作るのがうまくいかなくて、cdk はツール自体が大きいので実行時間がかかる。だいたいビルドやパッケージングのそれぞれに20-30分ぐらいかかる。エラーの原因がよく分からなくて面倒になって断念した。私が javascript のパッケージングに疎いせいもあると思うけど、ドキュメントに書いてある通りにうまくいかなかったので早々に諦めた。</description><content>&lt;p>0時に寝て6時半に起きた。暑くなってきたせいかバテ気味。水曜日は本番リリースの日。3つほど本場環境のインフラ移行作業があったので社員さんの実作業をリモートから指示しながらサポートしていた。私に本番環境の操作権限があれば、この工数はほぼ半分に削減できる。&lt;/p>
&lt;h2 id="cdk-のパッチ検証">cdk のパッチ検証&lt;/h2>
&lt;p>先日 &lt;a href="/diary/diary/posts/2022/0516/#eks-クラスターの-helm-管理の調査">cdk による eks クラスターの helm 管理の調査中断&lt;/a> について書いた。バグっているから適切な設定が反映されないという話しで一時中断していたんだけど、そのときにクラスメソッドの担当者さんとも相談していた。そしたら、その担当者さんが問題を修正して pr を送ってくれた。感謝。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/pull/20373">fix(eks): Cluster.FromClusterAttributes ignores KubectlLambdaRole #20373&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>そこまでやってくれると思ってなかったのですごいなぁと感心しながら、せっかくなのでパッチを当てた cdk で検証してみようと、&lt;a href="https://github.com/aws/aws-cdk/blob/master/CONTRIBUTING.md">Contributing to the AWS Cloud Development Kit&lt;/a> をみながらローカルでのビルドを試みた。ビルド自体はできたんだけど、パッケージを作るのがうまくいかなくて、cdk はツール自体が大きいので実行時間がかかる。だいたいビルドやパッケージングのそれぞれに20-30分ぐらいかかる。エラーの原因がよく分からなくて面倒になって断念した。私が javascript のパッケージングに疎いせいもあると思うけど、ドキュメントに書いてある通りにうまくいかなかったので早々に諦めた。&lt;/p></content></item><item><title>helm 調査の一時中断</title><link>/diary/posts/2022/0516/</link><pubDate>Mon, 16 May 2022 08:31:44 +0900</pubDate><guid>/diary/posts/2022/0516/</guid><description>0時に寝て6時半に起きた。
eks クラスターの helm 管理の調査 先週から調査 していて、調査結果から kubectlRoleArn を生成してデプロイを実行してみた。以前発生していた権限エラーは解消したものの、lambda 内からの kubectl と k8s クラスターの認証に失敗する。もう1つ kubectlLambdaRole という設定もあるので、ここに system:masters 権限をもつ iam ロールを設定してみたものの、エラー結果は変わらない。お手上げかなと思ってたら aws-eks: Cluster.FromClusterAttributes ignores KubectlLambdaRole #20008 という issue があって、まさにいま起こっている現象と合致するのでこのせいかもしれない。cdk のバグならそれが解消しないと検証を進められないため、この調査は一旦打ち切って、cdk のバグが修正されて新しいバージョンがリリースされてから再試行することに決めた。
backlog のいろいろ たまたまタイムラインでスライドをみかけて、backlog の開発をしている方の記事をみつけた。scala と play framework で実装されているらしい。もう10年以上開発しているプロダクトなのかな。それ自体はすごいなぁと感心しながらスライドを眺めてた。
ヌーラボ入社後に書いたブログ記事＆プレゼン資料まとめ</description><content>&lt;p>0時に寝て6時半に起きた。&lt;/p>
&lt;h2 id="eks-クラスターの-helm-管理の調査">eks クラスターの helm 管理の調査&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2022/0513/#eks-クラスターの-helm-管理の調査">先週から調査&lt;/a> していて、調査結果から &lt;code>kubectlRoleArn&lt;/code> を生成してデプロイを実行してみた。以前発生していた権限エラーは解消したものの、lambda 内からの kubectl と k8s クラスターの認証に失敗する。もう1つ &lt;code>kubectlLambdaRole&lt;/code> という設定もあるので、ここに &lt;code>system:masters&lt;/code> 権限をもつ iam ロールを設定してみたものの、エラー結果は変わらない。お手上げかなと思ってたら &lt;a href="https://github.com/aws/aws-cdk/issues/20008">aws-eks: Cluster.FromClusterAttributes ignores KubectlLambdaRole #20008&lt;/a> という issue があって、まさにいま起こっている現象と合致するのでこのせいかもしれない。cdk のバグならそれが解消しないと検証を進められないため、この調査は一旦打ち切って、cdk のバグが修正されて新しいバージョンがリリースされてから再試行することに決めた。&lt;/p>
&lt;h2 id="backlog-のいろいろ">backlog のいろいろ&lt;/h2>
&lt;p>たまたまタイムラインでスライドをみかけて、backlog の開発をしている方の記事をみつけた。scala と play framework で実装されているらしい。もう10年以上開発しているプロダクトなのかな。それ自体はすごいなぁと感心しながらスライドを眺めてた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://muziyoshiz.hatenablog.com/entry/2021/08/28/154859">ヌーラボ入社後に書いたブログ記事＆プレゼン資料まとめ&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>マーケティング施策の取り組み開始</title><link>/diary/posts/2022/0513/</link><pubDate>Fri, 13 May 2022 08:42:18 +0900</pubDate><guid>/diary/posts/2022/0513/</guid><description>23時に寝て1時に起きて漫画を読んで6時に起きて漫画読んでた。
隔週の雑談 顧問のはらさんと隔週の打ち合わせ。今日の議題は 先日作成した第4期の展望 について雑談した。あまり深く考えずに起業して初期の頃に作った10ヶ年計画に対してわりとその通りに推移している。来期ぐらいで業務委託のお仕事は終える予定。来期か再来期ぐらいからプロダクト開発の期間に入る。自社プロダクトを作る前から徐々にマーケティングもしていかないといけない。そのため、今期からマーケティング施策を少しずつ増やしていて、まずは会社の信頼度を上げるところからやっていく。売上を上げるためのマーケティングではなく信頼度を上げるためのマーケティングを行う。金森氏が言うようにどんなに優れたプロダクトを作ったとしても売れるかどうかは別問題だ。
eks クラスターの helm 管理の調査 昨日の続き。権限設定がなんもわからんみたいな様相になったので調査のやり方を変えることにした。検証用の eks クラスターを cdk から新規に作成して helm をインストールするときのリソースや権限設定がどうなるのかを調査した。lambda 関数が5個ぐらい、ロールは10個ぐらい作成された。lambda の生成に時間がかかるのか？新規作成するのに25分、削除するときは30分ぐらいかかった。rds もそうだけど、eks のような複雑なインフラを cdk で管理すると実行するのにけっこう時間がかかる。設定が難しくなければ別によいけど、eks のような権限やリソースの設定が複雑なインフラはトライ&amp;amp;エラーで何度も実行する必要があるから、こういうものは cdk で管理するのには向かないインフラだと言えると思う。一定の設定のプラクティスが溜まるまでは eks は cdk で管理しない方がよいかもしれない。
CreationRole というのが設定されて trust relationships に次のような設定が追加される。
{ &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Principal&amp;#34;: { &amp;#34;AWS&amp;#34;: &amp;#34;arn:aws:iam::${accountId}:root&amp;#34; }, &amp;#34;Action&amp;#34;: &amp;#34;sts:AssumeRole&amp;#34; } ] } このロールに含まれるカスタムポリシーには次のような設定がある。たぶんこんな感じのロールを新規に作成して kubectlRoleArn として指定してやればいいんじゃないかと思う。
{ &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Action&amp;#34;: &amp;#34;iam:PassRole&amp;#34;, &amp;#34;Resource&amp;#34;: &amp;#34;arn:aws:iam::${accountId}:role/${EksClusterIamRole}&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: [ &amp;#34;eks:CreateCluster&amp;#34;, &amp;#34;eks:DescribeCluster&amp;#34;, &amp;#34;eks:DescribeUpdate&amp;#34;, &amp;#34;eks:DeleteCluster&amp;#34;, &amp;#34;eks:UpdateClusterVersion&amp;#34;, &amp;#34;eks:UpdateClusterConfig&amp;#34;, &amp;#34;eks:CreateFargateProfile&amp;#34;, &amp;#34;eks:TagResource&amp;#34;, &amp;#34;eks:UntagResource&amp;#34; ], &amp;#34;Resource&amp;#34;: [ &amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:cluster/tmp-test-eks-cluster-by-morimoto&amp;#34;, &amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:cluster/tmp-test-eks-cluster-by-morimoto/*&amp;#34; ], &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: [ &amp;#34;eks:DescribeFargateProfile&amp;#34;, &amp;#34;eks:DeleteFargateProfile&amp;#34; ], &amp;#34;Resource&amp;#34;: &amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:fargateprofile/tmp-test-eks-cluster-by-morimoto/*&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: [ &amp;#34;iam:GetRole&amp;#34;, &amp;#34;iam:listAttachedRolePolicies&amp;#34; ], &amp;#34;Resource&amp;#34;: &amp;#34;*&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: &amp;#34;iam:CreateServiceLinkedRole&amp;#34;, &amp;#34;Resource&amp;#34;: &amp;#34;*&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; }, { &amp;#34;Action&amp;#34;: [ &amp;#34;ec2:DescribeInstances&amp;#34;, &amp;#34;ec2:DescribeNetworkInterfaces&amp;#34;, &amp;#34;ec2:DescribeSecurityGroups&amp;#34;, &amp;#34;ec2:DescribeSubnets&amp;#34;, &amp;#34;ec2:DescribeRouteTables&amp;#34;, &amp;#34;ec2:DescribeDhcpOptions&amp;#34;, &amp;#34;ec2:DescribeVpcs&amp;#34; ], &amp;#34;Resource&amp;#34;: &amp;#34;*&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34; } ] }</description><content>&lt;p>23時に寝て1時に起きて漫画を読んで6時に起きて漫画読んでた。&lt;/p>
&lt;h2 id="隔週の雑談">隔週の雑談&lt;/h2>
&lt;p>顧問のはらさんと隔週の打ち合わせ。今日の議題は &lt;a href="/diary/diary/posts/2022/0503/#打ち合わせ資料の作成">先日作成した第4期の展望&lt;/a> について雑談した。あまり深く考えずに起業して初期の頃に作った10ヶ年計画に対してわりとその通りに推移している。来期ぐらいで業務委託のお仕事は終える予定。来期か再来期ぐらいからプロダクト開発の期間に入る。自社プロダクトを作る前から徐々にマーケティングもしていかないといけない。そのため、今期からマーケティング施策を少しずつ増やしていて、まずは会社の信頼度を上げるところからやっていく。売上を上げるためのマーケティングではなく信頼度を上げるためのマーケティングを行う。金森氏が言うようにどんなに優れたプロダクトを作ったとしても売れるかどうかは別問題だ。&lt;/p>
&lt;iframe width="500" height="250" scrolling="no" src="https://alu.jp/series/%E6%98%A0%E5%83%8F%E7%A0%94%E3%81%AB%E3%81%AF%E6%89%8B%E3%82%92%E5%87%BA%E3%81%99%E3%81%AA%EF%BC%81/crop/embed/X8MNLWdDgPeZ5RIF25sD/0?referer=oembed" style="margin: auto;">&lt;/iframe>
&lt;h2 id="eks-クラスターの-helm-管理の調査">eks クラスターの helm 管理の調査&lt;/h2>
&lt;p>昨日の続き。権限設定がなんもわからんみたいな様相になったので調査のやり方を変えることにした。検証用の eks クラスターを cdk から新規に作成して helm をインストールするときのリソースや権限設定がどうなるのかを調査した。lambda 関数が5個ぐらい、ロールは10個ぐらい作成された。lambda の生成に時間がかかるのか？新規作成するのに25分、削除するときは30分ぐらいかかった。rds もそうだけど、eks のような複雑なインフラを cdk で管理すると実行するのにけっこう時間がかかる。設定が難しくなければ別によいけど、eks のような権限やリソースの設定が複雑なインフラはトライ&amp;amp;エラーで何度も実行する必要があるから、こういうものは cdk で管理するのには向かないインフラだと言えると思う。一定の設定のプラクティスが溜まるまでは eks は cdk で管理しない方がよいかもしれない。&lt;/p>
&lt;p>CreationRole というのが設定されて trust relationships に次のような設定が追加される。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#f92672">&amp;#34;Version&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2012-10-17&amp;#34;&lt;/span>,
&lt;span style="color:#f92672">&amp;#34;Statement&amp;#34;&lt;/span>: [
{
&lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>,
&lt;span style="color:#f92672">&amp;#34;Principal&amp;#34;&lt;/span>: {
&lt;span style="color:#f92672">&amp;#34;AWS&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;arn:aws:iam::${accountId}:root&amp;#34;&lt;/span>
},
&lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;sts:AssumeRole&amp;#34;&lt;/span>
}
]
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>このロールに含まれるカスタムポリシーには次のような設定がある。たぶんこんな感じのロールを新規に作成して &lt;code>kubectlRoleArn&lt;/code> として指定してやればいいんじゃないかと思う。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#f92672">&amp;#34;Version&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2012-10-17&amp;#34;&lt;/span>,
&lt;span style="color:#f92672">&amp;#34;Statement&amp;#34;&lt;/span>: [
{
&lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;iam:PassRole&amp;#34;&lt;/span>,
&lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;arn:aws:iam::${accountId}:role/${EksClusterIamRole}&amp;#34;&lt;/span>,
&lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
},
{
&lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: [
&lt;span style="color:#e6db74">&amp;#34;eks:CreateCluster&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;eks:DescribeCluster&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;eks:DescribeUpdate&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;eks:DeleteCluster&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;eks:UpdateClusterVersion&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;eks:UpdateClusterConfig&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;eks:CreateFargateProfile&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;eks:TagResource&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;eks:UntagResource&amp;#34;&lt;/span>
],
&lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: [
&lt;span style="color:#e6db74">&amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:cluster/tmp-test-eks-cluster-by-morimoto&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:cluster/tmp-test-eks-cluster-by-morimoto/*&amp;#34;&lt;/span>
],
&lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
},
{
&lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: [
&lt;span style="color:#e6db74">&amp;#34;eks:DescribeFargateProfile&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;eks:DeleteFargateProfile&amp;#34;&lt;/span>
],
&lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;arn:aws:eks:ap-northeast-1:${accountId}:fargateprofile/tmp-test-eks-cluster-by-morimoto/*&amp;#34;&lt;/span>,
&lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
},
{
&lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: [
&lt;span style="color:#e6db74">&amp;#34;iam:GetRole&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;iam:listAttachedRolePolicies&amp;#34;&lt;/span>
],
&lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;*&amp;#34;&lt;/span>,
&lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
},
{
&lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;iam:CreateServiceLinkedRole&amp;#34;&lt;/span>,
&lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;*&amp;#34;&lt;/span>,
&lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
},
{
&lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: [
&lt;span style="color:#e6db74">&amp;#34;ec2:DescribeInstances&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;ec2:DescribeNetworkInterfaces&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;ec2:DescribeSecurityGroups&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;ec2:DescribeSubnets&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;ec2:DescribeRouteTables&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;ec2:DescribeDhcpOptions&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;ec2:DescribeVpcs&amp;#34;&lt;/span>
],
&lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;*&amp;#34;&lt;/span>,
&lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Allow&amp;#34;&lt;/span>
}
]
}
&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>cdk と eks と lambda と iam がわからん</title><link>/diary/posts/2022/0512/</link><pubDate>Thu, 12 May 2022 11:42:54 +0900</pubDate><guid>/diary/posts/2022/0512/</guid><description>0時に寝て3時に起きて漫画読んで5時に寝て8時に起きた。
eks クラスターの helm 管理 昨日の続き。helm のよさはわかったので dapr を helm で管理しようとしている。その際になるべく cdk で管理できた方がよい。eks は cdk の外部で管理しているのだけど、既存の eks クラスターをインポートする機能も提供されていることに気付いた。
Using existing clusters それなら既存の eks クラスターをインポートして cdk で helm だけ管理しようと思って始めたものの、これがとても難しくて丸1日作業してわからなかった。設定項目は少ないけど、権限の問題で動かない。1回あたりの実行に15分ぐらいかかるのでトライ&amp;amp;エラーするのもなかなか大変。
kubectlRoleArn - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used.</description><content>&lt;p>0時に寝て3時に起きて漫画読んで5時に寝て8時に起きた。&lt;/p>
&lt;h2 id="eks-クラスターの-helm-管理">eks クラスターの helm 管理&lt;/h2>
&lt;p>昨日の続き。helm のよさはわかったので dapr を helm で管理しようとしている。その際になるべく cdk で管理できた方がよい。eks は cdk の外部で管理しているのだけど、既存の eks クラスターをインポートする機能も提供されていることに気付いた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_eks-readme.html#using-existing-clusters">Using existing clusters&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>それなら既存の eks クラスターをインポートして cdk で helm だけ管理しようと思って始めたものの、これがとても難しくて丸1日作業してわからなかった。設定項目は少ないけど、権限の問題で動かない。1回あたりの実行に15分ぐらいかかるのでトライ&amp;amp;エラーするのもなかなか大変。&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>kubectlRoleArn&lt;/code> - the ARN of an IAM role mapped to the system:masters RBAC role. If the cluster you are importing was created using the AWS CDK, the CloudFormation stack has an output that includes an IAM role that can be used. Otherwise, you can create an IAM role and map it to system:masters manually. The trust policy of this role should include the the arn:aws::iam::${accountId}:root principal in order to allow the execution role of the kubectl resource to assume it.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;code>kubectlRoleArn&lt;/code> の設定をどうするかだけなんだが、この説明でどう設定していいか理解できなかった。cdk で新規に eks クラスターを作成するなら自動で作ってくれるけど、既存の eks クラスターの場合は自分で設定しないといけない。ややこしいことに cdk は kubectl の実行を lambda 経由で実行するので eks と lambda と iam のロールやポリシーを適切に設定する必要がある。lambda にどういう権限を設定するのが適切なのかは本当に難しい。サーバーレスはよいアイディアだとは思うけど、lambda は難し過ぎて私はなるべく使いたくないサービスではある。結局わからなくて翌日に持ち越し。&lt;/p></content></item><item><title>障害調査と先入観</title><link>/diary/posts/2022/0427/</link><pubDate>Wed, 27 Apr 2022 07:37:11 +0900</pubDate><guid>/diary/posts/2022/0427/</guid><description>23時に寝て5時過ぎに起きた。
インフラの不具合調査 本番環境に初回デプロイして ecs から secrets manager のアクセスに失敗しているエラーが出ていたので調査した。エラーメッセージでググるとクラスメソッドさんの記事が出てきた。
ECSでコンテナ環境変数をSecretManagerから取得する際にResourceInitializationErrorが発生したときの対処方法 この記事によると、次のどちらかの原因かなと推測していた。
実行 IAM ロールの権限不足 SecretsManager エンドポイントへの不到達 調べども調べどもテスト環境との違いがわからなくてはまってしまった。ある検証をしていたときに、テスト環境を手動で構築した担当者から datadog の api key を設定してないんじゃない？と言われて、まさにそれが原因だった。cdk のコード上は設定済みのものとして ecs の datadog のサイドカーに設定していた。先入観で rds の credential 情報を取得できずにエラーになっていると思い込んでいたが、サイドカーの datadog の api key が原因ではまるべくしてはまったという感じ。
一方で secrets manager に登録するサードパーティの api key をどこで管理するかというのは難しい問題でもある。cdk のコードの中に書いてしまうというのも1つのやり方ではあるが、昨今の github 関連のサードパーティから派生したセキュリティインシデントで private リポジトリのソースコードにアクセスされることも発生しているのでソースコードには書きたくない。で、手動で secrets manager に設定しないといけないから、今回みたいな初回デプロイ時に誰も気付かないみたいなことが起きる。</description><content>&lt;p>23時に寝て5時過ぎに起きた。&lt;/p>
&lt;h2 id="インフラの不具合調査">インフラの不具合調査&lt;/h2>
&lt;p>本番環境に初回デプロイして ecs から secrets manager のアクセスに失敗しているエラーが出ていたので調査した。エラーメッセージでググるとクラスメソッドさんの記事が出てきた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://dev.classmethod.jp/articles/tsnote-ecs-resourceinitializationerror/">ECSでコンテナ環境変数をSecretManagerから取得する際にResourceInitializationErrorが発生したときの対処方法&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>この記事によると、次のどちらかの原因かなと推測していた。&lt;/p>
&lt;ul>
&lt;li>実行 IAM ロールの権限不足&lt;/li>
&lt;li>SecretsManager エンドポイントへの不到達&lt;/li>
&lt;/ul>
&lt;p>調べども調べどもテスト環境との違いがわからなくてはまってしまった。ある検証をしていたときに、テスト環境を手動で構築した担当者から datadog の api key を設定してないんじゃない？と言われて、まさにそれが原因だった。cdk のコード上は設定済みのものとして ecs の datadog のサイドカーに設定していた。先入観で rds の credential 情報を取得できずにエラーになっていると思い込んでいたが、サイドカーの datadog の api key が原因ではまるべくしてはまったという感じ。&lt;/p>
&lt;p>一方で secrets manager に登録するサードパーティの api key をどこで管理するかというのは難しい問題でもある。cdk のコードの中に書いてしまうというのも1つのやり方ではあるが、昨今の github 関連のサードパーティから派生したセキュリティインシデントで private リポジトリのソースコードにアクセスされることも発生しているのでソースコードには書きたくない。で、手動で secrets manager に設定しないといけないから、今回みたいな初回デプロイ時に誰も気付かないみたいなことが起きる。&lt;/p></content></item><item><title>本番環境反映の監督</title><link>/diary/posts/2022/0425/</link><pubDate>Mon, 25 Apr 2022 19:39:39 +0900</pubDate><guid>/diary/posts/2022/0425/</guid><description>昨日は思いっきり昼寝してしまったせいか、夜に眠れなくて3時ぐらいまで起きてて、寝たのか寝てのかよくわからない雰囲気で7時前に起きた。
インフラ作業の本番反映 先週対応した api gateway のコード化 を本番環境に反映した。手動で作成されていた api gateway, vpc link, セキュリティグループとそれに関連するリソース群を cdk 管理のコードで置き換える。既存のリソースを削除してから cdk でデプロイするため、失敗しても切り戻しできないのでプレッシャーがかかる。とはいえ、想定通りに作業が進捗して2時間ほどで完了した。それと並行してテスト環境では、手動で作成された rds を cdk 管理のコードで置き換える移行作業をしていた。これも一筋縄ではいかなくて右往左往しながら作業してた。これを本番環境でやるのもまた億劫だなぁと思いながら発生したエラーや事象を書き綴っていた。あと本番環境で行う大きな移行作業はこれだけ。</description><content>&lt;p>昨日は思いっきり昼寝してしまったせいか、夜に眠れなくて3時ぐらいまで起きてて、寝たのか寝てのかよくわからない雰囲気で7時前に起きた。&lt;/p>
&lt;h2 id="インフラ作業の本番反映">インフラ作業の本番反映&lt;/h2>
&lt;p>先週対応した &lt;a href="/diary/diary/posts/2022/0419/">api gateway のコード化&lt;/a> を本番環境に反映した。手動で作成されていた api gateway, vpc link, セキュリティグループとそれに関連するリソース群を cdk 管理のコードで置き換える。既存のリソースを削除してから cdk でデプロイするため、失敗しても切り戻しできないのでプレッシャーがかかる。とはいえ、想定通りに作業が進捗して2時間ほどで完了した。それと並行してテスト環境では、手動で作成された rds を cdk 管理のコードで置き換える移行作業をしていた。これも一筋縄ではいかなくて右往左往しながら作業してた。これを本番環境でやるのもまた億劫だなぁと思いながら発生したエラーや事象を書き綴っていた。あと本番環境で行う大きな移行作業はこれだけ。&lt;/p></content></item><item><title>rds の再作成</title><link>/diary/posts/2022/0421/</link><pubDate>Thu, 21 Apr 2022 07:37:32 +0900</pubDate><guid>/diary/posts/2022/0421/</guid><description>23時に寝て5時半に起きた。久しぶりによく眠れた気がする。
rds を独立したスタックに分離 昨日の続き。ライフサイクルにあわせたスタックに分割し、スタック間の依存関係を適切に定義することで堅牢なインフラコードになる。最後に残った DatabaseStack を分離するところを朝からやっていた。
DatabaseStack BackendStack GatewayStack FrontendStack rds を壊して、バージョンを aurora postgresql 最新の 13.x にアップグレードして、cf のテンプレートで小細工をしながら既存の環境を移行したりしていた。pg_dump を使ってテキストに dump したデータを、psql を使ってリストアしたりもした。データ量が少なかったらこれで移行してもいいかもしれない。
$ pg_dump --host mydb.xxx.ap-northeast-1.rds.amazonaws.com --username user mydb &amp;gt; mydb.dump $ psql --host mydb.xxx.ap-northeast-1.rds.amazonaws.com --username user --dbname mydb --file ./mydb.dump</description><content>&lt;p>23時に寝て5時半に起きた。久しぶりによく眠れた気がする。&lt;/p>
&lt;h2 id="rds-を独立したスタックに分離">rds を独立したスタックに分離&lt;/h2>
&lt;p>昨日の続き。ライフサイクルにあわせたスタックに分割し、スタック間の依存関係を適切に定義することで堅牢なインフラコードになる。最後に残った DatabaseStack を分離するところを朝からやっていた。&lt;/p>
&lt;ul>
&lt;li>DatabaseStack
&lt;ul>
&lt;li>BackendStack
&lt;ul>
&lt;li>GatewayStack
&lt;ul>
&lt;li>FrontendStack&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>rds を壊して、バージョンを aurora postgresql 最新の 13.x にアップグレードして、cf のテンプレートで小細工をしながら既存の環境を移行したりしていた。&lt;a href="https://www.postgresql.org/docs/13/app-pgdump.html">pg_dump&lt;/a> を使ってテキストに dump したデータを、psql を使ってリストアしたりもした。データ量が少なかったらこれで移行してもいいかもしれない。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ pg_dump --host mydb.xxx.ap-northeast-1.rds.amazonaws.com --username user mydb &amp;gt; mydb.dump
$ psql --host mydb.xxx.ap-northeast-1.rds.amazonaws.com --username user --dbname mydb --file ./mydb.dump
&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>堅牢なインフラコード</title><link>/diary/posts/2022/0420/</link><pubDate>Wed, 20 Apr 2022 07:38:23 +0900</pubDate><guid>/diary/posts/2022/0420/</guid><description>23時に寝て5時に起きた。
駐輪場の定期更新 3ヶ月ごとの更新。ちょうどいまのお仕事の契約と同じ更新月になっている。前回と同じ金額だったのでまだ駐輪場の料金は値上げされていない。世界的にインフレしているのに日本が全然インフレしていない理由の1つとして不動産が値上げしていないからというのを日銀の記事で見かけた気がする。どこかのタイミングで不動産関連の値上げも始まるのかもしれない。
インフラコードの抜本的リファクタリング 約2週間かけて、新規インフラ環境の構築、既存インフラの cdk/cf と同期されていなかったインフラ (rds, security group, croudfront, api gateway, waf) の同期など、インフラコードの大きな変更をやり終えた。一部 fromLookup でインポートしているリソースもあるが、いま完全に cdk/cf 管理なインフラとなった。ここからはせっかく cdk でコードを書いているので、モジュール化や共通化など、再利用可能なリソースとして定義して、複数の Stack でコードを再利用するといったリファクタリングをしていく。ひとまずこのことをボーナスステージと呼ぼう。いま完全に同期されたインフラがあるため、インフラ上のリソースの差分が出なければリファクタリングは正しいことが保証される。cdk は Stack 間の依存関係 も定義できるため、適切な依存関係を定義することでより堅牢なインフラコードとなるはずである。具体的には次のような依存関係になる。然るべき堅牢なインフラコードに書き換えていく。
DatabaseStack BackendStack GatewayStack FrontendStack</description><content>&lt;p>23時に寝て5時に起きた。&lt;/p>
&lt;h2 id="駐輪場の定期更新">駐輪場の定期更新&lt;/h2>
&lt;p>3ヶ月ごとの更新。ちょうどいまのお仕事の契約と同じ更新月になっている。前回と同じ金額だったのでまだ駐輪場の料金は値上げされていない。世界的にインフレしているのに日本が全然インフレしていない理由の1つとして不動産が値上げしていないからというのを日銀の記事で見かけた気がする。どこかのタイミングで不動産関連の値上げも始まるのかもしれない。&lt;/p>
&lt;h2 id="インフラコードの抜本的リファクタリング">インフラコードの抜本的リファクタリング&lt;/h2>
&lt;p>約2週間かけて、新規インフラ環境の構築、既存インフラの cdk/cf と同期されていなかったインフラ (rds, security group, croudfront, api gateway, waf) の同期など、インフラコードの大きな変更をやり終えた。一部 &lt;code>fromLookup&lt;/code> でインポートしているリソースもあるが、いま完全に cdk/cf 管理なインフラとなった。ここからはせっかく cdk でコードを書いているので、モジュール化や共通化など、再利用可能なリソースとして定義して、複数の Stack でコードを再利用するといったリファクタリングをしていく。ひとまずこのことをボーナスステージと呼ぼう。いま完全に同期されたインフラがあるため、インフラ上のリソースの差分が出なければリファクタリングは正しいことが保証される。cdk は &lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib-readme.html#dependencies">Stack 間の依存関係&lt;/a> も定義できるため、適切な依存関係を定義することでより堅牢なインフラコードとなるはずである。具体的には次のような依存関係になる。然るべき堅牢なインフラコードに書き換えていく。&lt;/p>
&lt;ul>
&lt;li>DatabaseStack
&lt;ul>
&lt;li>BackendStack
&lt;ul>
&lt;li>GatewayStack
&lt;ul>
&lt;li>FrontendStack&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></content></item><item><title>api gateway のデプロイ検証</title><link>/diary/posts/2022/0419/</link><pubDate>Tue, 19 Apr 2022 07:38:47 +0900</pubDate><guid>/diary/posts/2022/0419/</guid><description>0時に寝て6時に起きた。
api gateway のデプロイ検証 昨日の続き。api gateway を再作成すると、当然、既存のテスト環境は疎通できなくなってしまう。他の開発者を妨害しないようにまた21時から深夜にかけて作業しようと考えていたら、いまは閑散期でテスト環境を使わないといけないような開発の状況にはないと他の開発者から教えてもらった。PO の人たちも検証は一段落しているとのこと。午後からテスト環境壊してもよいという確認が取れたので13時からインフラ作業をしていた。早ければ1時間ぐらい、遅ければ4時間と見積もって、実際にデプロイしてみると、あれやこれやの抜け・漏れ、手動で設定されていたリソースの弊害などもあって、3時間ぐらいかかった。もう2週間ほどインフラの作業ばかりやっているせいか、何が起こっても4時間ぐらいあれば調査とリストアを完了できるぐらいの自信がついてきた。最終的に cdk で再作成した api gateway を使って cloudfront の distribution 経由で web api 呼び出しが繋がった。当たり前の話なんだけど、繋がる瞬間、ローカルで web api のエンドポイントを叩いてレスポンスが返ってくるときが嬉しい。最後の大物だったインフラもたった2日でやっつけることができた。これからはボーナスステージ。</description><content>&lt;p>0時に寝て6時に起きた。&lt;/p>
&lt;h2 id="api-gateway-のデプロイ検証">api gateway のデプロイ検証&lt;/h2>
&lt;p>昨日の続き。api gateway を再作成すると、当然、既存のテスト環境は疎通できなくなってしまう。他の開発者を妨害しないようにまた21時から深夜にかけて作業しようと考えていたら、いまは閑散期でテスト環境を使わないといけないような開発の状況にはないと他の開発者から教えてもらった。PO の人たちも検証は一段落しているとのこと。午後からテスト環境壊してもよいという確認が取れたので13時からインフラ作業をしていた。早ければ1時間ぐらい、遅ければ4時間と見積もって、実際にデプロイしてみると、あれやこれやの抜け・漏れ、手動で設定されていたリソースの弊害などもあって、3時間ぐらいかかった。もう2週間ほどインフラの作業ばかりやっているせいか、何が起こっても4時間ぐらいあれば調査とリストアを完了できるぐらいの自信がついてきた。最終的に cdk で再作成した api gateway を使って cloudfront の distribution 経由で web api 呼び出しが繋がった。当たり前の話なんだけど、繋がる瞬間、ローカルで web api のエンドポイントを叩いてレスポンスが返ってくるときが嬉しい。最後の大物だったインフラもたった2日でやっつけることができた。これからはボーナスステージ。&lt;/p></content></item><item><title>api gateway のコード化</title><link>/diary/posts/2022/0418/</link><pubDate>Mon, 18 Apr 2022 07:39:00 +0900</pubDate><guid>/diary/posts/2022/0418/</guid><description>1時に寝て7時に起きた。
api gateway のコード化 いま cdk で管理していない大きなインフラとして api gateway がある。さらに restful api ではなく http api という新しい仕組みを使っているため、cdk 側も experimental な機能として提供されている。experimental と言っても本番環境で使われいるので十分に production ready と言える。弊害としては cdk のサンプルコードが少なく、公式の api reference に付いているサンプルコードぐらいしか参考になるものがなく、適当にコードを書いてでデプロイし、管理画面で意図した設定になっているかを確認するといったトライ&amp;amp;エラーみたいなやり方しかない。今日の時点では pr まで作ってデプロイと検証は翌日にまわることにした。
さらに cdk の v2 系では -alpha というパッケージが experimental 向けの機能を提供している。バージョン番号ではなく、パッケージ名に alpha が付くという分かりにくさに拍車をかけている。お手伝い先で使っている設定は次の3つのパッケージを使うことで実現できた。api gateway, vpc link, integration, route, authorization, stage など、複数のリソースを連携して設定しないといけないのでちょっとややこしい。
https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-alpha https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-authorizers-alpha https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-integrations-alpha</description><content>&lt;p>1時に寝て7時に起きた。&lt;/p>
&lt;h2 id="api-gateway-のコード化">api gateway のコード化&lt;/h2>
&lt;p>いま cdk で管理していない大きなインフラとして &lt;a href="https://aws.amazon.com/jp/api-gateway/">api gateway&lt;/a> がある。さらに restful api ではなく http api という新しい仕組みを使っているため、cdk 側も experimental な機能として提供されている。experimental と言っても本番環境で使われいるので十分に production ready と言える。弊害としては cdk のサンプルコードが少なく、公式の api reference に付いているサンプルコードぐらいしか参考になるものがなく、適当にコードを書いてでデプロイし、管理画面で意図した設定になっているかを確認するといったトライ&amp;amp;エラーみたいなやり方しかない。今日の時点では pr まで作ってデプロイと検証は翌日にまわることにした。&lt;/p>
&lt;p>さらに cdk の v2 系では &lt;code>-alpha&lt;/code> というパッケージが experimental 向けの機能を提供している。バージョン番号ではなく、パッケージ名に alpha が付くという分かりにくさに拍車をかけている。お手伝い先で使っている設定は次の3つのパッケージを使うことで実現できた。api gateway, vpc link, integration, route, authorization, stage など、複数のリソースを連携して設定しないといけないのでちょっとややこしい。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-alpha">https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-alpha&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-authorizers-alpha">https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-authorizers-alpha&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-integrations-alpha">https://www.npmjs.com/package/@aws-cdk/aws-apigatewayv2-integrations-alpha&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>cdk の cloudfront の distribution 設定の移行</title><link>/diary/posts/2022/0413/</link><pubDate>Wed, 13 Apr 2022 15:22:21 +0900</pubDate><guid>/diary/posts/2022/0413/</guid><description>0時に寝て5時過ぎに起きた。
フロントエンドのインフラ作業の続き 昨日、cloudfront の distribution 設定のビヘイビアのキャッシュ設定が誤っていることに気付いたので cdk のコードを修正していく。Amazon CloudFront キャッシュポリシーとオリジンリクエストポリシーを発表 によると、2020年ぐらいに distribution ごとに個別設定していたのをマネージドポリシーというリソースを参照することで一元管理できるようになったらしい。cdk のコードで言えば、次の issue で対応されているが、これらの機能は aws_cloudfront.Distribution のみに追加されている。従来の CloudFrontWebDistribution では使えないので distribution 設定そのものを新しいやり方に移行する必要がある。
https://github.com/aws/aws-cdk/issues/9644 https://github.com/aws/aws-cdk/issues/9647 基本的には同じ設定を行うので新しいクラスのメンバーや構造にあわせて移行するだけなので難しくはないけれど、1つずつ設定内容の移行方法を確認していかないといけないから手間暇はかかる。cdk のドキュメントをみると、型に対してどういった設定をすればよいかが書いてあって、あとはメソッドの定義などもみながら自分たちの設定に近いものを選択していくといった作業になる。難しくはないけど時間はかかる。半日ほどやって移行のための pr を作成した。</description><content>&lt;p>0時に寝て5時過ぎに起きた。&lt;/p>
&lt;h2 id="フロントエンドのインフラ作業の続き">フロントエンドのインフラ作業の続き&lt;/h2>
&lt;p>昨日、cloudfront の distribution 設定のビヘイビアのキャッシュ設定が誤っていることに気付いたので cdk のコードを修正していく。&lt;a href="https://aws.amazon.com/jp/blogs/news/amazon-cloudfront-announces-cache-and-origin-request-policies/">Amazon CloudFront キャッシュポリシーとオリジンリクエストポリシーを発表&lt;/a> によると、2020年ぐらいに distribution ごとに個別設定していたのをマネージドポリシーというリソースを参照することで一元管理できるようになったらしい。cdk のコードで言えば、次の issue で対応されているが、これらの機能は &lt;a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_cloudfront.Distribution.html">aws_cloudfront.Distribution&lt;/a> のみに追加されている。従来の CloudFrontWebDistribution では使えないので distribution 設定そのものを新しいやり方に移行する必要がある。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/9644">https://github.com/aws/aws-cdk/issues/9644&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/issues/9647">https://github.com/aws/aws-cdk/issues/9647&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>基本的には同じ設定を行うので新しいクラスのメンバーや構造にあわせて移行するだけなので難しくはないけれど、1つずつ設定内容の移行方法を確認していかないといけないから手間暇はかかる。cdk のドキュメントをみると、型に対してどういった設定をすればよいかが書いてあって、あとはメソッドの定義などもみながら自分たちの設定に近いものを選択していくといった作業になる。難しくはないけど時間はかかる。半日ほどやって移行のための pr を作成した。&lt;/p></content></item><item><title>cdk/cf の Stack とライフサイクル</title><link>/diary/posts/2022/0411/</link><pubDate>Mon, 11 Apr 2022 07:26:57 +0900</pubDate><guid>/diary/posts/2022/0411/</guid><description>0時に寝て5時に起きた。
インフラ変更の本番作業 先週やっていた様々なインフラ構築の改善を本番環境に適用する。cf の changeset は23とかになっていた気がする。大きな括りで次の3つの移行作業をした。
rds をスタックから切り離す cdk の v1 から v2 へのアップグレード ポリシーとセキュリティグループのドリフト解消 私は本番環境へのアクセス権限をもっていないので社員さんの作業内容を伝えてやってもらう。cf のテンプレートの更新やドリフトの解消など、テスト環境で10時間以上は費やした検証結果が功を奏して、本番作業は想定外の事象も発生せず1.5時間で完了した。cdk のコードも意図した設定になるように修正済みだし、なにも問題は発生しない。cdk のコードを書くときは cf のイベントログやドリフト結果と実際のインフラの振る舞いを確認するといった検証には時間がかかるが、それができてしまえば本番環境の構築はうまくできる。それがわかっているインフラ担当者がいなくなると、また1から担当者が検証する必要があって保守は大変かもしれないけど。
同じ Stack でどういったリソースを管理するかというライフサイクルは難しい問題かもしれない。今回 rds を削除したのは、なんらかの理由で手動運用で rds を変更することがあって、アプリケーション Stack から外してしまった方が保守しやすいのではないかという意見が出たため。それが正しいかどうかはわからないが、一度作ったリソースを削除しないもの (vpc や s3 bucket など) をアプリケーション Stack で管理すると、再作成できなくて面倒くさいことがある。というのは、再作成は新規に作成 → 古いリソースを削除の順番で処理されるため、一意な名前をもつリソースは基本的に再作成できない。</description><content>&lt;p>0時に寝て5時に起きた。&lt;/p>
&lt;h2 id="インフラ変更の本番作業">インフラ変更の本番作業&lt;/h2>
&lt;p>先週やっていた様々なインフラ構築の改善を本番環境に適用する。cf の changeset は23とかになっていた気がする。大きな括りで次の3つの移行作業をした。&lt;/p>
&lt;ul>
&lt;li>rds をスタックから切り離す&lt;/li>
&lt;li>cdk の v1 から v2 へのアップグレード&lt;/li>
&lt;li>ポリシーとセキュリティグループのドリフト解消&lt;/li>
&lt;/ul>
&lt;p>私は本番環境へのアクセス権限をもっていないので社員さんの作業内容を伝えてやってもらう。cf のテンプレートの更新やドリフトの解消など、テスト環境で10時間以上は費やした検証結果が功を奏して、本番作業は想定外の事象も発生せず1.5時間で完了した。cdk のコードも意図した設定になるように修正済みだし、なにも問題は発生しない。cdk のコードを書くときは cf のイベントログやドリフト結果と実際のインフラの振る舞いを確認するといった検証には時間がかかるが、それができてしまえば本番環境の構築はうまくできる。それがわかっているインフラ担当者がいなくなると、また1から担当者が検証する必要があって保守は大変かもしれないけど。&lt;/p>
&lt;p>同じ Stack でどういったリソースを管理するかというライフサイクルは難しい問題かもしれない。今回 rds を削除したのは、なんらかの理由で手動運用で rds を変更することがあって、アプリケーション Stack から外してしまった方が保守しやすいのではないかという意見が出たため。それが正しいかどうかはわからないが、一度作ったリソースを削除しないもの (vpc や s3 bucket など) をアプリケーション Stack で管理すると、再作成できなくて面倒くさいことがある。というのは、再作成は新規に作成 → 古いリソースを削除の順番で処理されるため、一意な名前をもつリソースは基本的に再作成できない。&lt;/p></content></item><item><title>壊れた cf スタックのリストアと cdk の再同期</title><link>/diary/posts/2022/0408/</link><pubDate>Fri, 08 Apr 2022 10:13:19 +0900</pubDate><guid>/diary/posts/2022/0408/</guid><description>2時に寝て6時半に起きた。インフラエンジニアになったのでみんなが作業していない時間にインフラの保守作業をするようにしている。昼はアプリケーションエンジニア、夜はインフラエンジニアみたいな生活になっていてしんどい。
壊れた cf スタックの更新 テスト環境の cf スタックを手動で更新して壊れているのを cdk で管理できるように直した。壊れていたのは次の3つ。
rds をスナップショットからリストアしたので cf が管理している rds リソースが存在しない iam の acl 設定が異なる セキュリティグループのインバウンドルールが異なる aws 的にもそういった状況は認識していて cdk で同期できなくなった cf スタックを更新する手順を提供している。
AWS CloudFormation スタックを削除したときにリソースの一部を保持する方法を教えてください。 ざっくり手順をまとめると次になる。
対象のリソースに DeletetionPolicy=Retain にセットする テンプレートからリソースを削除して、スタックの更新を実行する テンプレート内のリソースの実際の状態を describe して、スタック内に既存のリソースをインポートする リソースの設定ぐらいなら既存のリソースからインポートしなくても cf のテンプレートを直接書き換えたものをアップロードしてスタックを更新するのでも大丈夫だったりする。しかし、cdk もそのテンプレートにあうように修正しないといけないため、cdk のコードとテンプレートのコードの両方をチェックしながら検証する必要がある。cdk でリソース管理ができるようになったからといって、それが変更前の既存のリソースの設定と同じかどうかは人間が目でみて検証しないといけない。これがあちこちで参照されているリソースだと追跡するのが面倒くさいといった手間暇がかかる。
cdk がよいものかどうか、私はまだ判断がつかないけど、cf を抽象化して便利になっているところは認めるものの、cf のスタックが壊れたときのトラブルシューティングが必要以上に複雑で厄介というのも事実ではある。一方で壊れた cf スタックを5時間ぐらいかけて直したのではまりポイントはいくつかも学ぶことができた。しんどかったけど。例えば、あるセキュリティグループのインバウンドルールに別のセキュリティグループを関連付けるとき、1つの設定ではうまくいかなくて次の2つの設定を追加した。これが適切かどうかわからないが、この設定で cdk でデプロイしたスタックの環境と既存リソースとの環境が整合した状態 (ドリフトが解消される) になった。こういうのが cdk の抽象化による訳のわからないところの1つ。
otherSecurityGroup.addIngressRule( ec2.SecurityGroup.fromSecurityGroupId(this, &amp;#39;my security group&amp;#39;, mySgId), ec2.Port.tcp(80), &amp;#34;my inboud rule&amp;#34;, ) otherSecurityGroup.</description><content>&lt;p>2時に寝て6時半に起きた。インフラエンジニアになったのでみんなが作業していない時間にインフラの保守作業をするようにしている。昼はアプリケーションエンジニア、夜はインフラエンジニアみたいな生活になっていてしんどい。&lt;/p>
&lt;h2 id="壊れた-cf-スタックの更新">壊れた cf スタックの更新&lt;/h2>
&lt;p>テスト環境の cf スタックを手動で更新して壊れているのを cdk で管理できるように直した。壊れていたのは次の3つ。&lt;/p>
&lt;ul>
&lt;li>rds をスナップショットからリストアしたので cf が管理している rds リソースが存在しない&lt;/li>
&lt;li>iam の acl 設定が異なる&lt;/li>
&lt;li>セキュリティグループのインバウンドルールが異なる&lt;/li>
&lt;/ul>
&lt;p>aws 的にもそういった状況は認識していて cdk で同期できなくなった cf スタックを更新する手順を提供している。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://aws.amazon.com/jp/premiumsupport/knowledge-center/delete-cf-stack-retain-resources/">AWS CloudFormation スタックを削除したときにリソースの一部を保持する方法を教えてください。&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>ざっくり手順をまとめると次になる。&lt;/p>
&lt;ol>
&lt;li>対象のリソースに DeletetionPolicy=Retain にセットする&lt;/li>
&lt;li>テンプレートからリソースを削除して、スタックの更新を実行する&lt;/li>
&lt;li>テンプレート内のリソースの実際の状態を describe して、スタック内に既存のリソースをインポートする&lt;/li>
&lt;/ol>
&lt;p>リソースの設定ぐらいなら既存のリソースからインポートしなくても cf のテンプレートを直接書き換えたものをアップロードしてスタックを更新するのでも大丈夫だったりする。しかし、cdk もそのテンプレートにあうように修正しないといけないため、cdk のコードとテンプレートのコードの両方をチェックしながら検証する必要がある。cdk でリソース管理ができるようになったからといって、それが変更前の既存のリソースの設定と同じかどうかは人間が目でみて検証しないといけない。これがあちこちで参照されているリソースだと追跡するのが面倒くさいといった手間暇がかかる。&lt;/p>
&lt;p>cdk がよいものかどうか、私はまだ判断がつかないけど、cf を抽象化して便利になっているところは認めるものの、cf のスタックが壊れたときのトラブルシューティングが必要以上に複雑で厄介というのも事実ではある。一方で壊れた cf スタックを5時間ぐらいかけて直したのではまりポイントはいくつかも学ぶことができた。しんどかったけど。例えば、あるセキュリティグループのインバウンドルールに別のセキュリティグループを関連付けるとき、1つの設定ではうまくいかなくて次の2つの設定を追加した。これが適切かどうかわからないが、この設定で cdk でデプロイしたスタックの環境と既存リソースとの環境が整合した状態 (ドリフトが解消される) になった。こういうのが cdk の抽象化による訳のわからないところの1つ。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-typescript" data-lang="typescript">&lt;span style="color:#a6e22e">otherSecurityGroup&lt;/span>.&lt;span style="color:#a6e22e">addIngressRule&lt;/span>(
&lt;span style="color:#a6e22e">ec2&lt;/span>.&lt;span style="color:#a6e22e">SecurityGroup&lt;/span>.&lt;span style="color:#a6e22e">fromSecurityGroupId&lt;/span>(&lt;span style="color:#66d9ef">this&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;my security group&amp;#39;&lt;/span>, &lt;span style="color:#a6e22e">mySgId&lt;/span>),
&lt;span style="color:#a6e22e">ec2&lt;/span>.&lt;span style="color:#a6e22e">Port&lt;/span>.&lt;span style="color:#a6e22e">tcp&lt;/span>(&lt;span style="color:#ae81ff">80&lt;/span>),
&lt;span style="color:#e6db74">&amp;#34;my inboud rule&amp;#34;&lt;/span>,
)
&lt;span style="color:#a6e22e">otherSecurityGroup&lt;/span>.&lt;span style="color:#a6e22e">addIngressRule&lt;/span>(
&lt;span style="color:#a6e22e">ec2&lt;/span>.&lt;span style="color:#a6e22e">Peer&lt;/span>.&lt;span style="color:#a6e22e">securityGroupId&lt;/span>(&lt;span style="color:#a6e22e">mySgId&lt;/span>),
&lt;span style="color:#a6e22e">ec2&lt;/span>.&lt;span style="color:#a6e22e">Port&lt;/span>.&lt;span style="color:#a6e22e">tcp&lt;/span>(&lt;span style="color:#ae81ff">80&lt;/span>),
&lt;span style="color:#e6db74">&amp;#34;my inboud rule&amp;#34;&lt;/span>,
)
&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>cdk のメジャーバージョンのマイグレーション</title><link>/diary/posts/2022/0407/</link><pubDate>Thu, 07 Apr 2022 06:10:00 +0900</pubDate><guid>/diary/posts/2022/0407/</guid><description>0時に寝て5時に起きた。開発者にインフラ変更の影響を出さないように6時半からインフラのお仕事してた。
cdk v1 と v2 の違い AWS CDK Versions には v1 と v2 の2つがある。新規で作るものは v2 を選択すればよいけど、既存のスタックが v1 だとマイグレーションが必要になる。cdk は bootstrap したときに CDKToolkit というスタックを生成する。cdk をアップグレードするというのはこのスタックの設定も更新する必要がある。デフォルト設定をそのまま使っていればマイグレーションはそんなに難しくはないはずだけど、設定をカスタマイズしていたりするといくつかパラメーターを調整したりしなかったりしてややこしいかもしれない。
Migrating to AWS CDK v2 Bootstrapping また v2 は v1 の experimental な機能は移行されていないため、v1 のライブラリを直接使うか、自前でその機能を実装するといったことも必要になる可能性がある。
cdk v2 を使っていて v1 にある機能が v2 になくてあれー？って感じで調べてたら experimental な機能はまだ移行されてないらしい。https://t.co/SbRZ5ddrTj
&amp;mdash; Tetsuya Morimoto (@t2y) April 7, 2022 例えば、v1 の apigwv2.VpcLink というメソッドは experimental で v2 に移行されていないため、v2 に移行されている stable な CfnVpcLink という機能を使って次のように実装した。これは v1 の cdk の実装をみて同じように実装しただけ。</description><content>&lt;p>0時に寝て5時に起きた。開発者にインフラ変更の影響を出さないように6時半からインフラのお仕事してた。&lt;/p>
&lt;h2 id="cdk-v1-と-v2-の違い">cdk v1 と v2 の違い&lt;/h2>
&lt;p>&lt;a href="https://docs.aws.amazon.com/cdk/api/versions.html">AWS CDK Versions&lt;/a> には v1 と v2 の2つがある。新規で作るものは v2 を選択すればよいけど、既存のスタックが v1 だとマイグレーションが必要になる。cdk は bootstrap したときに CDKToolkit というスタックを生成する。cdk をアップグレードするというのはこのスタックの設定も更新する必要がある。デフォルト設定をそのまま使っていればマイグレーションはそんなに難しくはないはずだけど、設定をカスタマイズしていたりするといくつかパラメーターを調整したりしなかったりしてややこしいかもしれない。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.aws.amazon.com/cdk/v2/guide/migrating-v2.html">Migrating to AWS CDK v2&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/cdk/v2/guide/bootstrapping.html">Bootstrapping&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>また v2 は v1 の experimental な機能は移行されていないため、v1 のライブラリを直接使うか、自前でその機能を実装するといったことも必要になる可能性がある。&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="ja" dir="ltr">cdk v2 を使っていて v1 にある機能が v2 になくてあれー？って感じで調べてたら experimental な機能はまだ移行されてないらしい。&lt;a href="https://t.co/SbRZ5ddrTj">https://t.co/SbRZ5ddrTj&lt;/a>&lt;/p>&amp;mdash; Tetsuya Morimoto (@t2y) &lt;a href="https://twitter.com/t2y/status/1511924087450640386?ref_src=twsrc%5Etfw">April 7, 2022&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>例えば、v1 の apigwv2.VpcLink というメソッドは experimental で v2 に移行されていないため、v2 に移行されている stable な CfnVpcLink という機能を使って次のように実装した。これは v1 の cdk の実装をみて同じように実装しただけ。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/aws/aws-cdk/pull/10531/files#diff-1cf3aaf7b2b6b2e72123b93cb3108eb9e9a3291e588d62eb2cd34dd0509d3045">https://github.com/aws/aws-cdk/pull/10531/files#diff-1cf3aaf7b2b6b2e72123b93cb3108eb9e9a3291e588d62eb2cd34dd0509d3045&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-diff" data-lang="diff">&lt;span style="color:#f92672">- const apiGwVpcLink = new apigwv2.VpcLink(this, &amp;#39;ApiGwVpcLink&amp;#39;, {
&lt;/span>&lt;span style="color:#f92672">- vpc: vpc,
&lt;/span>&lt;span style="color:#f92672">- vpcLinkName: &amp;#39;my-vpc-link&amp;#39;,
&lt;/span>&lt;span style="color:#f92672">- securityGroups: [mySecurityGroup]
&lt;/span>&lt;span style="color:#f92672">&lt;/span>&lt;span style="color:#a6e22e">+ const apiGwVpcLink = new apigwv2.CfnVpcLink(this, &amp;#39;ApiGwVpcLink&amp;#39;, {
&lt;/span>&lt;span style="color:#a6e22e">+ name: &amp;#39;my-vpc-link&amp;#39;,
&lt;/span>&lt;span style="color:#a6e22e">+ subnetIds: vpc.privateSubnets.map(sb =&amp;gt; sb.subnetId),
&lt;/span>&lt;span style="color:#a6e22e">+ securityGroupIds: [mySecurityGroup.securityGroupId]
&lt;/span>&lt;/code>&lt;/pre>&lt;/div></content></item></channel></rss>