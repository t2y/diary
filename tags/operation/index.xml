<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>operation on forest nook</title><link>/diary/tags/operation/</link><description>Recent content in operation on forest nook</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><copyright>© 2021 Tetsuya Morimoto</copyright><lastBuildDate>Wed, 21 Jun 2023 08:24:11 +0900</lastBuildDate><atom:icon>/diary/favicon.ico</atom:icon><icon>/diary/favicon.ico</icon><atom:link href="/diary/tags/operation/index.xml" rel="self" type="application/rss+xml"/><item><title>厄介なインフラ問題をやっつけた</title><link>/diary/posts/2023/0621/</link><pubDate>Wed, 21 Jun 2023 08:24:11 +0900</pubDate><guid>/diary/posts/2023/0621/</guid><description>2時に寝て6時に起きて7時に起きた。夜に作業していたら遅くなった。
厄介なインフラの問題 解決編 運用のトラブルシューティング の続き。アプリケーションアカウントを作って compose 環境を構築したら nginx のコンテナが起動して即時終了する状態になったという。これまで起きていた現象とまた違う問題が発生してさらに混迷をもたらすかに思えたが、私の中では nginx のコンテナでなにかがおかしいと問題の発生箇所を局所化できたのでそこからの調査はそんなに時間を必要としなかった。
結論からいうと podman の aardvark-dns の不具合だった。なんらかのトリガーでコンテナネットワーク内の名前解決が不整合な状態に陥る。
vagrant@bookworm:$ podman-compose exec proxy /bin/bash ... root@3742c45c7c60:/# dig app ; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.16.37-Debian &amp;lt;&amp;lt;&amp;gt;&amp;gt; app ;; global options: +cmd ;; Got answer: ;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NOERROR, id: 56696 ;; flags: qr rd ra ad; QUERY: 1, ANSWER: 8, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: 37ff0fd63315d70e (echoed) ;; QUESTION SECTION: ;app.</description><content>&lt;p>2時に寝て6時に起きて7時に起きた。夜に作業していたら遅くなった。&lt;/p>
&lt;h2 id="厄介なインフラの問題-解決編">厄介なインフラの問題 解決編&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2023/0619/#運用のトラブルシューティング">運用のトラブルシューティング&lt;/a> の続き。アプリケーションアカウントを作って compose 環境を構築したら nginx のコンテナが起動して即時終了する状態になったという。これまで起きていた現象とまた違う問題が発生してさらに混迷をもたらすかに思えたが、私の中では nginx のコンテナでなにかがおかしいと問題の発生箇所を局所化できたのでそこからの調査はそんなに時間を必要としなかった。&lt;/p>
&lt;p>結論からいうと podman の &lt;a href="https://github.com/containers/aardvark-dns">aardvark-dns&lt;/a> の不具合だった。なんらかのトリガーでコンテナネットワーク内の名前解決が不整合な状態に陥る。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>vagrant@bookworm:$ podman-compose exec proxy /bin/bash
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@3742c45c7c60:/# dig app
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.16.37-Debian &amp;lt;&amp;lt;&amp;gt;&amp;gt; app
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; global options: +cmd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; Got answer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; -&amp;gt;&amp;gt;HEADER&lt;span style="color:#e6db74">&amp;lt;&amp;lt;- opco&lt;/span>de: QUERY, status: NOERROR, id: &lt;span style="color:#ae81ff">56696&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; flags: qr rd ra ad; QUERY: 1, ANSWER: 8, AUTHORITY: 0, ADDITIONAL: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; OPT PSEUDOSECTION:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>; EDNS: version: 0, flags:; udp: &lt;span style="color:#ae81ff">4096&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>; COOKIE: 37ff0fd63315d70e &lt;span style="color:#f92672">(&lt;/span>echoed&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; QUESTION SECTION:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;app. IN A
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; ANSWER SECTION:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.36
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.36
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.136
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.136
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.146
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.146
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.156
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app. 86400 IN A 10.89.0.156
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; Query time: &lt;span style="color:#ae81ff">4&lt;/span> msec
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; SERVER: 10.89.0.1#53&lt;span style="color:#f92672">(&lt;/span>10.89.0.1&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; WHEN: Thu Jun &lt;span style="color:#ae81ff">22&lt;/span> 02:45:26 UTC &lt;span style="color:#ae81ff">2023&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>;; MSG SIZE rcvd: &lt;span style="color:#ae81ff">172&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>podman 4.0 から aardvark-dns がコンテナネットワーク内での dns を提供する。nginx が app を名前解決したときに起動しているコンテナの ip アドレスではなく、削除された過去のコンテナの ip アドレスが返される状況が発生する。app という名前に対して複数の ip アドレスが返る。&lt;/p>
&lt;p>このとき nginx は複数の ip アドレスのうちの1つに接続しようとするが、正しい ip アドレスでない場合、リクエストがタイムアウトする。タイムアウトした後に fallback で他の ip アドレスに接続しにいく。このときに正しい ip アドレスがみつかればクライアントにレスポンスが返る。この fallback のリトライの回数分だけリクエストのレイテンシの時間がかかっていた。&lt;/p>
&lt;pre tabindex="0">&lt;code>vagrant@bookworm:$ podman logs -f proxy
...
2023/06/22 02:46:26 [error] 15#15: *41 connect() failed (113: No route to host) while connecting to upstream, client: 10.89.0.38, server: ucidmsv1-app, request: &amp;#34;GET / HTTP/1.1&amp;#34;, upstream: &amp;#34;http://10.89.0.136:3000/&amp;#34;, host: &amp;#34;localhost:4430&amp;#34;
2023/06/22 02:46:29 [error] 15#15: *41 connect() failed (113: No route to host) while connecting to upstream, client: 10.89.0.38, server: ucidmsv1-app, request: &amp;#34;GET / HTTP/1.1&amp;#34;, upstream: &amp;#34;http://10.89.0.156:3000/&amp;#34;, host: &amp;#34;localhost:4430&amp;#34;
2023/06/22 02:46:32 [error] 15#15: *41 connect() failed (113: No route to host) while connecting to upstream, client: 10.89.0.38, server: ucidmsv1-app, request: &amp;#34;GET / HTTP/1.1&amp;#34;, upstream: &amp;#34;http://10.89.0.136:3000/&amp;#34;, host: &amp;#34;localhost:4430&amp;#34;
10.89.0.38 - - [22/Jun/2023:02:46:32 +0000] &amp;#34;GET / HTTP/1.1&amp;#34; 200 2864 &amp;#34;-&amp;#34; &amp;#34;curl/7.88.1&amp;#34;
&lt;/code>&lt;/pre>&lt;p>ワークアラウンドとして、次のファイルに複数の app の ip アドレスが登録されていれば不整合な状態なのでネットワークを削除して、このファイルも手動で削除してしまえばよい。&lt;/p>
&lt;pre tabindex="0">&lt;code>$ cat /run/user/$(id -u)/containers/networks/aardvark-dns/mynetwork
&lt;/code>&lt;/pre>&lt;p>ファイルを監視していると、どうやら mynetwork ファイルから名前と ip アドレスの情報が削除されるのは該当のコンテナが削除されるタイミングになる。なんらかのエラーにより、コンテナ削除時にマッピングの削除が実行されないと、古いコンテナのマッピング設定が残ったままとなり、compose サービスを起動したときに複数の ip アドレスの名前解決できる状態になってしまう。ちょっと調べても aardvark-dns に関する issue はたくさん登録されている。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containers/podman/issues/18783">https://github.com/containers/podman/issues/18783&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/containers/podman/issues/18530">https://github.com/containers/podman/issues/18530&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/containers/podman/issues/17370">https://github.com/containers/podman/issues/17370&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="コワーキングのオンラインイベント">コワーキングのオンラインイベント&lt;/h2>
&lt;p>月例のカフーツさんのオンラインイベントに参加した。&lt;a href="/diary/diary/posts/2023/0517/#コワーキングのオンラインイベント">先月の所感はここ&lt;/a> 。今日はもともと予定していた話しをする参加者が急遽参加できなくなってしまったので他の参加者での雑談会になった。&lt;/p>
&lt;p>いとうさん曰く、これまで外国人のデジタルノマドは自分で業務時間を選べるフリーランスの、さらにお金に余裕をもった人たちが多いと考えられていた。しかし、実際にコワーキングスペースに来られている外国人にキャリアを伺うと、大企業の普通の社員であることがわかってきた。グローバルな会社だと、働く場所に制限のない会社もあって、ただ日本へ行ってみたかった的な理由で日本へ来られて数ヶ月滞在して普通に会社のお仕事をするといったデジタルノマドもいるという。過去に私が働いていた職場の同僚も、コロナのときに会社がフルリモートワークの体制を設けて、airbnb で全国を旅しながら1年ほど働いていた。日本でもそういう社員はいるのだから外国人はなおさらという感じ。&lt;/p>
&lt;p>そういった外国人のデジタルノマドが要求することが3つある。&lt;/p>
&lt;ul>
&lt;li>24時間利用できること (勤め先の会社と時差があるから)&lt;/li>
&lt;li>セカンドモニターがあること&lt;/li>
&lt;li>・・・ (あともう1つあったが、忘れてしまった)&lt;/li>
&lt;/ul>
&lt;p>コワーキングスペースに外国人のデジタルノマドを呼び込むにはどうすればよいか。実際にコワーキングスペースへ来られた外国人に理由を伺うと英語のホームページをみて来ましたということらしい。至極、当たり前の話し。英語のホームページをちゃんと作ろうねみたいな話題で話していた。&lt;/p></content></item><item><title>運用トラブルの調査</title><link>/diary/posts/2023/0619/</link><pubDate>Mon, 19 Jun 2023 08:31:20 +0900</pubDate><guid>/diary/posts/2023/0619/</guid><description>0時に寝て5時に起きて7時に起きた。もう暑くて家でもエアコンを解禁した。エアコンがあると寝心地が違う、快適。
運用のトラブルシューティング 厄介なインフラの問題 のクリティカルな方から着手し始めた。podman-compose を使って rootless な環境構築をやってみたところ、nginx を tls 終端としてリバースプロキシとするアプリケーションサーバーとの通信が数回に1回ぐらいの頻度で遅くなる。通常は 100msec 程度でレスポンスが返るのが数秒から数十秒かかる。
もともと podman-compose はサポート対象外なのでそんながんばる必要はない。しかし、これも調査の過程でコンテナの技術を学ぶ1つだと考え再現環境を構築しようとした。vagrant の debian 12 と podman-compose をインストールして同様に環境構築してみたが、仮想環境では再現しない。どうやら環境要因のようだ。そこで問題が発生しているマシンで私のアカウントで環境構築してみたが、やはり再現しない。なんと個人アカウントの違いによって起きる現象のようだ。また質が悪いのは私のアカウントでは再現しないが、メンバー2人のアカウントでは再現している。一般ユーザーから他人のユーザーのプロセスやコンテナの情報にアクセスできるわけがないので調査ができない。個人アカウントで compose 環境を構築するのは諦めてアプリケーションアカウントを作ってやりましょうという話しにした。アプリケーションアカウントで再現すれば調査するし、再現しなければこんな環境要因のトラブルシューティングの優先度を下げてもいいかなぁとも考えている。どうなるかなぁ。
サイトデザインのサンプルページ サイトデザイン打ち合わせ の続き。実際のサンプルが出てきたのでデザインの雰囲気やコードも含めて確認していく。ざっとサンプルページを確認した。デザインはとても気に入っている。あとは私が hugo のテーマとしてテンプレートの組み込めるかどうか次第。今週末には実家帰らないといけないし、毎日やることがいっぱいいっぱい。</description><content>&lt;p>0時に寝て5時に起きて7時に起きた。もう暑くて家でもエアコンを解禁した。エアコンがあると寝心地が違う、快適。&lt;/p>
&lt;h2 id="運用のトラブルシューティング">運用のトラブルシューティング&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2023/0613/#厄介なインフラの問題-x-2">厄介なインフラの問題&lt;/a> のクリティカルな方から着手し始めた。&lt;a href="https://github.com/containers/podman-compose">podman-compose&lt;/a> を使って rootless な環境構築をやってみたところ、nginx を tls 終端としてリバースプロキシとするアプリケーションサーバーとの通信が数回に1回ぐらいの頻度で遅くなる。通常は 100msec 程度でレスポンスが返るのが数秒から数十秒かかる。&lt;/p>
&lt;p>もともと podman-compose はサポート対象外なのでそんながんばる必要はない。しかし、これも調査の過程でコンテナの技術を学ぶ1つだと考え再現環境を構築しようとした。vagrant の debian 12 と podman-compose をインストールして同様に環境構築してみたが、仮想環境では再現しない。どうやら環境要因のようだ。そこで問題が発生しているマシンで私のアカウントで環境構築してみたが、やはり再現しない。なんと個人アカウントの違いによって起きる現象のようだ。また質が悪いのは私のアカウントでは再現しないが、メンバー2人のアカウントでは再現している。一般ユーザーから他人のユーザーのプロセスやコンテナの情報にアクセスできるわけがないので調査ができない。個人アカウントで compose 環境を構築するのは諦めてアプリケーションアカウントを作ってやりましょうという話しにした。アプリケーションアカウントで再現すれば調査するし、再現しなければこんな環境要因のトラブルシューティングの優先度を下げてもいいかなぁとも考えている。どうなるかなぁ。&lt;/p>
&lt;h2 id="サイトデザインのサンプルページ">サイトデザインのサンプルページ&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2023/0522/#サイトデザイン打ち合わせ">サイトデザイン打ち合わせ&lt;/a> の続き。実際のサンプルが出てきたのでデザインの雰囲気やコードも含めて確認していく。ざっとサンプルページを確認した。デザインはとても気に入っている。あとは私が hugo のテーマとしてテンプレートの組み込めるかどうか次第。今週末には実家帰らないといけないし、毎日やることがいっぱいいっぱい。&lt;/p></content></item><item><title>簡単な現象の組み合わせ障害</title><link>/diary/posts/2022/0826/</link><pubDate>Fri, 26 Aug 2022 12:03:43 +0900</pubDate><guid>/diary/posts/2022/0826/</guid><description>0時に寝て6時に起きた。
eks クラスター障害の原因判明 過去に2回発生していた eks クラスター障害 の原因がようやくわかった。テスト環境も本番環境は5日ごとに再現していて、datadog で k8s のダッシュボードでそれぞれの pod 単位のメモリ使用量をみると datadog-agent の pod がメモリリークしていることに気付いた。そこから当たりをつけて datadog-agent の issue を調べると次のバグに遭遇していた。
[BUG] agent leaves defunct processes with version 7.38.0 #12997 ゾンビプロセスが生成されて、それが os のプロセス数上限に達してしまい、それによってプロセス (スレッド) が生成できなくなって、その結果として aws/amazon-vpc-cni-k8s の aws-node という eks クラスターの管理アプリケーションが動かなくなって、それが動かないと k8s ノードのステータスが NotReady になってしまって、通常の pod のアプリケーションも動かなくなってしまうという現象が発生していた。datadog-agent のアップグレードは私が行ったものだし、その後の k8s ノードの監視や調査で気付きが足りなかったと反省した。
datadog-agent の新しいバージョンをテスト環境でもうしばらく検証してもよかった datadog-agent をリソースリークの可能性を私の中の調査対象から外していた 世の中で使われているものに致命的なバグが起きないだろうという先入観があった プロセスを生成できない原因として考えられる背景を調査すべきだった ulimit を確認してリソース制限はないようにみえた プロセス数やゾンビプロセスを調べていなかった kernel に /proc/sys/kernel/pid_max という上限設定があることを知らなかった テスト環境と本番環境で5日程度で落ちるという周期性から気付くべきだった たしかにテスト環境から1日遅れて本番環境で障害が発生していた 周期性があることでリソースリークの可能性は高いとすぐに調査すべきだった datadog で k8s のダッシュボードを調べるべきだった すでに用意されているものがあったのでみようと思えばみえた aws のインフラ要因ではないかと疑っていた ごめんなさい これは悔しい。自分の無能さや気付きの低さを実感した事件だった。私が注意深く観察していればもう1週間早く気付けた。そのせいで余分な障害と調査に時間を費やした。1つ1つは全く難しくない現象が巧妙に絡みあって隠蔽された結果としての状況に気付けなかった。注意して1つずつ観察して追跡していけばすぐに気付けた。本当に悔しい。</description><content>&lt;p>0時に寝て6時に起きた。&lt;/p>
&lt;h2 id="eks-クラスター障害の原因判明">eks クラスター障害の原因判明&lt;/h2>
&lt;p>&lt;a href="/diary/diary/posts/2022/0820/#aws-インフラの調子が悪い">過去に2回発生していた eks クラスター障害&lt;/a> の原因がようやくわかった。テスト環境も本番環境は5日ごとに再現していて、datadog で k8s のダッシュボードでそれぞれの pod 単位のメモリ使用量をみると datadog-agent の pod がメモリリークしていることに気付いた。そこから当たりをつけて datadog-agent の issue を調べると次のバグに遭遇していた。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/DataDog/datadog-agent/issues/12997">[BUG] agent leaves defunct processes with version 7.38.0 #12997&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>ゾンビプロセスが生成されて、それが os のプロセス数上限に達してしまい、それによってプロセス (スレッド) が生成できなくなって、その結果として &lt;a href="https://github.com/aws/amazon-vpc-cni-k8s">aws/amazon-vpc-cni-k8s&lt;/a> の &lt;code>aws-node&lt;/code> という eks クラスターの管理アプリケーションが動かなくなって、それが動かないと k8s ノードのステータスが NotReady になってしまって、通常の pod のアプリケーションも動かなくなってしまうという現象が発生していた。datadog-agent のアップグレードは私が行ったものだし、その後の k8s ノードの監視や調査で気付きが足りなかったと反省した。&lt;/p>
&lt;ul>
&lt;li>datadog-agent の新しいバージョンをテスト環境でもうしばらく検証してもよかった&lt;/li>
&lt;li>datadog-agent をリソースリークの可能性を私の中の調査対象から外していた
&lt;ul>
&lt;li>世の中で使われているものに致命的なバグが起きないだろうという先入観があった&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>プロセスを生成できない原因として考えられる背景を調査すべきだった
&lt;ul>
&lt;li>ulimit を確認してリソース制限はないようにみえた&lt;/li>
&lt;li>プロセス数やゾンビプロセスを調べていなかった&lt;/li>
&lt;li>kernel に &lt;code>/proc/sys/kernel/pid_max&lt;/code> という上限設定があることを知らなかった&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>テスト環境と本番環境で5日程度で落ちるという周期性から気付くべきだった
&lt;ul>
&lt;li>たしかにテスト環境から1日遅れて本番環境で障害が発生していた&lt;/li>
&lt;li>周期性があることでリソースリークの可能性は高いとすぐに調査すべきだった&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>datadog で k8s のダッシュボードを調べるべきだった
&lt;ul>
&lt;li>すでに用意されているものがあったのでみようと思えばみえた&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>aws のインフラ要因ではないかと疑っていた
&lt;ul>
&lt;li>ごめんなさい&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>これは悔しい。自分の無能さや気付きの低さを実感した事件だった。私が注意深く観察していればもう1週間早く気付けた。そのせいで余分な障害と調査に時間を費やした。1つ1つは全く難しくない現象が巧妙に絡みあって隠蔽された結果としての状況に気付けなかった。注意して1つずつ観察して追跡していけばすぐに気付けた。本当に悔しい。&lt;/p>
&lt;p>1つだけ言い訳をさせてもらうと、私は本番環境にアクセスできない。だからテスト環境と本番環境で発生している現象が同じかどうかを判断できず、調査を進める確証をもてなかった。&lt;/p>
&lt;h2 id="呑み">呑み&lt;/h2>
&lt;p>あまりに悔しかったのと調査してたら遅くなって晩ご飯食べる気力もなかったので気分転換に仲のよい焼き鳥屋さんに寄ってみた。あとから常連客のセブンイレブンの店長さんも来られて、私は初対面かなと思ってたんだけど先方は知っていると言ってたから以前にもカウンターでご一緒していたみたい。何気はなしに3人で2時前ぐらいまで雑談していた。&lt;/p>
&lt;p>その店長さんがロレックスを購入しようと考えているという話しになって、資産または投資商品としてのロレックスの話しになった。たまたまヒカキンが1億円で買ったロレックスがいま2億円になっているといった話しがあったそうで、いまがバブルな状態らしいが、ロレックスをはじめとした高級時計の資産価値が上がっているらしい。私は腕時計を身につけないし高級時計もまったく興味はないが、投資商品の1つなんだというところに関心がもてた。&lt;/p>
&lt;div class="video-container">
&lt;iframe src="https://www.youtube.com/embed/1knsQZLeh7U" allowfullscreen title="1億円で買った時計が大変なことになってしまいました…">&lt;/iframe>
&lt;/div>
&lt;p>中小企業の社長の一般的な節税方法の1つに外車を買ったり売ったりするという話しがある。儲かったときに経費で外車を買って、赤字のときに外車を売って雑所得に変える。車は社用車として経費で落とせるから可能なことだが、高級時計はどうなのだろうか？ 結論から言うと、普通の会社では高級時計は経費にできない。経費の原則は売上を上げるために必要な支出を経費とできる。普通の会社は高級時計で売上を上げることはできない。一方で経費として認められる職業もある。芸能人がそうだという。それは番組のために必要だという理屈で経費で落とせる。おそらくヒカキンも経費で高級時計を購入して、そのことを動画にしているのも仕事で必要だという言い訳作りの目的もあるのだと推測する。&lt;/p></content></item><item><title>vuejs の template 調査</title><link>/diary/posts/2022/0824/</link><pubDate>Wed, 24 Aug 2022 07:58:08 +0900</pubDate><guid>/diary/posts/2022/0824/</guid><description>0時に寝て6時に起きた。
連日のサービスイン作業 引き続きサービスインの運用対応は大変そうでちゃんと検証していない修正を慌ててマージしようとしているからテスト環境まで壊れてて関係ない開発にも影響が出ていた。今日も別の施設のサービスインだったらしくて、ある機能がないとそのサービスインの切り替え作業ができないという話しだったそうで、当日に慌てて pr を作ってマージしてた。先週からわかっていた必要な機能を実装してなくて、週末は残業も休出もしてなくて、今日になって慌てて修正してマージしてた。昔の開発と比べてがんばっててできないのではなくて、いまの開発はがんばってないからできないという雰囲気になったなという印象。
vuejs の template と expression あるフォームのコンポーネントを作ろうと思って interface を定義していてデフォルト値をテンプレート側に指定できるといいんじゃないかと考えた。というのは typescript の interface のメンバーは値を保持できないから。例えば、次のようなコードで :cols=&amp;quot;item.col ?? 2&amp;quot; のように表現できたら嬉しいように思う。
&amp;lt;v-row dense v-for=&amp;#34;item in conditions&amp;#34; :key=&amp;#34;item.label&amp;#34;&amp;gt; &amp;lt;v-col :cols=&amp;#34;item.col ?? 2&amp;#34;&amp;gt; {{ element }} &amp;lt;/v-col&amp;gt; &amp;lt;/v-row&amp;gt; 余談だけど、?? は null 合体演算子という名前は知っていたけど、これを英語で何と呼ぶのか知らなかった。Nullish coalescing operator と言う。ググってみると vuejs の issue でもそこそこ議論されていて vue3 からサポートするとしながら、根強い要望があるのか？ vue2 でも 2.7 でサポートしたらしい。こういうモダンな javascript の expression を ESNext syntax と呼んだりするみたい。それすらも知らなかった。
Optional chaining in templates does not seem to work #11088 たまたまうちで使っているのは vue 2.</description><content>&lt;p>0時に寝て6時に起きた。&lt;/p>
&lt;h2 id="連日のサービスイン作業">連日のサービスイン作業&lt;/h2>
&lt;p>引き続きサービスインの運用対応は大変そうでちゃんと検証していない修正を慌ててマージしようとしているからテスト環境まで壊れてて関係ない開発にも影響が出ていた。今日も別の施設のサービスインだったらしくて、ある機能がないとそのサービスインの切り替え作業ができないという話しだったそうで、当日に慌てて pr を作ってマージしてた。先週からわかっていた必要な機能を実装してなくて、週末は残業も休出もしてなくて、今日になって慌てて修正してマージしてた。昔の開発と比べてがんばっててできないのではなくて、いまの開発はがんばってないからできないという雰囲気になったなという印象。&lt;/p>
&lt;h2 id="vuejs-の-template-と-expression">vuejs の template と expression&lt;/h2>
&lt;p>あるフォームのコンポーネントを作ろうと思って interface を定義していてデフォルト値をテンプレート側に指定できるといいんじゃないかと考えた。というのは typescript の interface のメンバーは値を保持できないから。例えば、次のようなコードで &lt;code>:cols=&amp;quot;item.col ?? 2&amp;quot;&lt;/code> のように表現できたら嬉しいように思う。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-xml" data-lang="xml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;lt;v-row&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">dense&lt;/span> &lt;span style="color:#a6e22e">v-for=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;item in conditions&amp;#34;&lt;/span> &lt;span style="color:#a6e22e">:key=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;item.label&amp;#34;&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;v-col&lt;/span> &lt;span style="color:#a6e22e">:cols=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;item.col ?? 2&amp;#34;&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {{ element }}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;/v-col&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;lt;/v-row&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>余談だけど、&lt;code>??&lt;/code> は null 合体演算子という名前は知っていたけど、これを英語で何と呼ぶのか知らなかった。&lt;a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Nullish_coalescing_operator">Nullish coalescing operator&lt;/a> と言う。ググってみると vuejs の issue でもそこそこ議論されていて vue3 からサポートするとしながら、根強い要望があるのか？ vue2 でも 2.7 でサポートしたらしい。こういうモダンな javascript の expression を ESNext syntax と呼んだりするみたい。それすらも知らなかった。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/vuejs/vue/issues/11088">Optional chaining in templates does not seem to work #11088&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>たまたまうちで使っているのは vue 2.6.14 なので vue 2.7 で動くのかどうか検証できないけど、いま使っている nuxtjs2 との依存関係があるのでそれ次第で vue 2.7 にアップグレードの可否が決まるらしい。全然フロントエンドの開発がわからないので、こういう基本的なところで引っかかると背景を調べるのに時間がかかる。&lt;/p></content></item><item><title>週明けのサービスイン</title><link>/diary/posts/2022/0822/</link><pubDate>Mon, 22 Aug 2022 08:27:42 +0900</pubDate><guid>/diary/posts/2022/0822/</guid><description>0時に寝て7時に起きた。
3つ目のサービスイン またまた私は勘違いしていて明日だと思っていたら今日が3つ目のサービスインだった。約1ヶ月ぶりのサービスイン になる。もう3回目なので要領よく切り替え作業やるのかなと眺めてたけど、新たなトラブルもいくつかあって、これまで同様、ドタバタしているようにみえる。私は本番環境にアクセスできないので何かトラブルがあっても聞いた内容から類推で助言を述べるしかできず、とはいえ、何かあったら質問がくるかもしれないからハドルに入って成り行きを見守ってないといけない。とくに手伝うこともないのにメンバーの作業が完了するまで待ってないといけない。この切り替え作業や運用対応をやっていると要件定義やコードレビューなどは放置されるのでまた作業のスケジュールが先送りになる。私は別に困らないけど、しばらくだらだらした開発が続く。</description><content>&lt;p>0時に寝て7時に起きた。&lt;/p>
&lt;h2 id="3つ目のサービスイン">3つ目のサービスイン&lt;/h2>
&lt;p>またまた私は勘違いしていて明日だと思っていたら今日が3つ目のサービスインだった。&lt;a href="/diary/diary/posts/2022/0719/#2つ目のサービスイン">約1ヶ月ぶりのサービスイン&lt;/a> になる。もう3回目なので要領よく切り替え作業やるのかなと眺めてたけど、新たなトラブルもいくつかあって、これまで同様、ドタバタしているようにみえる。私は本番環境にアクセスできないので何かトラブルがあっても聞いた内容から類推で助言を述べるしかできず、とはいえ、何かあったら質問がくるかもしれないからハドルに入って成り行きを見守ってないといけない。とくに手伝うこともないのにメンバーの作業が完了するまで待ってないといけない。この切り替え作業や運用対応をやっていると要件定義やコードレビューなどは放置されるのでまた作業のスケジュールが先送りになる。私は別に困らないけど、しばらくだらだらした開発が続く。&lt;/p></content></item><item><title>休日の本番障害</title><link>/diary/posts/2022/0820/</link><pubDate>Sat, 20 Aug 2022 12:17:43 +0900</pubDate><guid>/diary/posts/2022/0820/</guid><description>夕方から寝ていて何度か起きたものの、そのままずっと寝ていた。あまりないことなんだけど、珍しくたくさん眠れた。
ストレッチ 今日の開脚幅は開始前160cmで、ストレッチ後163cmだった。計測の仕方がややいい加減だった気もしたが、先週より少しよくなったということにしておく。右腰の張りが強いのと肩が前に入りがちなので肩を開いて姿勢を保つように心がけるとよいとアドバイスをいただいた。もう通い始めて1年半ぐらい経つ。トレーナーさんも大半が入れ替わっていて通い始めたときに話しかけてくれた私が知っているトレーナーさんはほとんどいない。1年半も経つと人は変わっていくなというのを実感している。私の最初のトレーナーさんは社内制度で別の店舗の助っ人に行っているのでいなくなった人たちが辞めているわけでもないとは思うけど、1-2年で人が入れ替わってもサービスは継続していかないといけないし、会社ってそういうものだなと実感する機会でもある。
aws インフラの調子が悪い？ 1-2週間ぐらい前からテスト環境を含めると複数回発生している eks クラスターの障害 がたまたま土曜日の夜という休日に発生した。いま eks クラスターのインフラの振る舞いを把握しているのは私だけなので、気付いてから指示を出して問題が発生している k8s ノードの削除 (ec2 インスタンスの削除) で復旧させるワークアラウンドで復旧させた。私は本番環境にアクセスできないので詳しい調査はできない。状況を正しく把握できてはいないけれど、k8s ノードが死んだり生き返ったりする不安定な状況に発生しているらしく、k8s ノードを削除して新規に作り直すと復旧することがわかっている。NotReady と Ready を繰り返したりしてアプリケーションの振る舞いが不安定になる。NotReady,SchedulingDisabled になれば、おそらく drain して k8s ノードが入れ替わってくれるのだけど、そうならない不安定な状況があるみたい。これ以上の調査は aws のサポートに問い合わせないとわからない。</description><content>&lt;p>夕方から寝ていて何度か起きたものの、そのままずっと寝ていた。あまりないことなんだけど、珍しくたくさん眠れた。&lt;/p>
&lt;h2 id="ストレッチ">ストレッチ&lt;/h2>
&lt;p>今日の開脚幅は開始前160cmで、ストレッチ後163cmだった。計測の仕方がややいい加減だった気もしたが、先週より少しよくなったということにしておく。右腰の張りが強いのと肩が前に入りがちなので肩を開いて姿勢を保つように心がけるとよいとアドバイスをいただいた。もう通い始めて1年半ぐらい経つ。トレーナーさんも大半が入れ替わっていて通い始めたときに話しかけてくれた私が知っているトレーナーさんはほとんどいない。1年半も経つと人は変わっていくなというのを実感している。私の最初のトレーナーさんは社内制度で別の店舗の助っ人に行っているのでいなくなった人たちが辞めているわけでもないとは思うけど、1-2年で人が入れ替わってもサービスは継続していかないといけないし、会社ってそういうものだなと実感する機会でもある。&lt;/p>
&lt;h2 id="aws-インフラの調子が悪い">aws インフラの調子が悪い？&lt;/h2>
&lt;p>1-2週間ぐらい前からテスト環境を含めると複数回発生している &lt;a href="/diary/diary/posts/2022/0815/">eks クラスターの障害&lt;/a> がたまたま土曜日の夜という休日に発生した。いま eks クラスターのインフラの振る舞いを把握しているのは私だけなので、気付いてから指示を出して問題が発生している k8s ノードの削除 (ec2 インスタンスの削除) で復旧させるワークアラウンドで復旧させた。私は本番環境にアクセスできないので詳しい調査はできない。状況を正しく把握できてはいないけれど、k8s ノードが死んだり生き返ったりする不安定な状況に発生しているらしく、k8s ノードを削除して新規に作り直すと復旧することがわかっている。NotReady と Ready を繰り返したりしてアプリケーションの振る舞いが不安定になる。NotReady,SchedulingDisabled になれば、おそらく drain して k8s ノードが入れ替わってくれるのだけど、そうならない不安定な状況があるみたい。これ以上の調査は aws のサポートに問い合わせないとわからない。&lt;/p></content></item><item><title>k8s ノードの削除方法がわからない</title><link>/diary/posts/2022/0815/</link><pubDate>Mon, 15 Aug 2022 08:12:40 +0900</pubDate><guid>/diary/posts/2022/0815/</guid><description>1時に寝て7時に起きた。寝冷えしてお腹痛い。
eks クラスターの障害 日曜日にテスト環境の eks クラスターで障害が発生していた。k8s ノードが NotReady になっていて、しばらくすると NotReady,SchedulingDisabled に変わって、それから新しい k8s ノードが起動して古いものが削除されて置き換わった。おそらくエラーが発生し始めてから1時間ほどはかかっていたと思う。わりと時間がかかるので明らかに k8s ノードが不調だと人間が判断しているなら ec2 インスタンスの切り替えを早くやりたい。k8s の公式ドキュメントの Use kubectl drain to remove a node from service では次の手順で行うように書いてある。
$ kubectl drain &amp;lt;node name&amp;gt; drain が正常終了すれば安全に k8s ノードを削除してよいのかな？
$ kubectl delete node &amp;lt;node name&amp;gt; eks クラスターで障害が発生していたときに drain を実行するとエラーになったのでそのまま delete node したら k8s ノードは削除されたものの、自動的に新しい k8s ノードが起動しなかった。aws のマネジメントコンソールから ec2 インスタンスを調べたら起動したままだったので強制的に ec2 インスタンスを終了させたところ、オートスケールの設定から ec2 インスタンスが起動してきて復旧した。但し、このやり方は k8s が意図した手順ではないようにも思える。軽く調べた範囲では k8s ノードの正しい削除方法 (置き換え方法？) がみつからなかった。そんなことを日曜日に確認していたら月曜日にほぼ同じ現象が本番環境の eks クラスターでも発生した。私は一度経験していたので同僚に指示して経過を観察していた。ここで書いたのと同じような手順で復旧した。おそらく aws 側のなにかのメンテナンス作業でうちの eks クラスターだと k8s ノードが死んでしまうような作業があったのではないか？と疑いをもっている。</description><content>&lt;p>1時に寝て7時に起きた。寝冷えしてお腹痛い。&lt;/p>
&lt;h2 id="eks-クラスターの障害">eks クラスターの障害&lt;/h2>
&lt;p>日曜日にテスト環境の eks クラスターで障害が発生していた。k8s ノードが NotReady になっていて、しばらくすると NotReady,SchedulingDisabled に変わって、それから新しい k8s ノードが起動して古いものが削除されて置き換わった。おそらくエラーが発生し始めてから1時間ほどはかかっていたと思う。わりと時間がかかるので明らかに k8s ノードが不調だと人間が判断しているなら ec2 インスタンスの切り替えを早くやりたい。k8s の公式ドキュメントの &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#use-kubectl-drain-to-remove-a-node-from-service">Use kubectl drain to remove a node from service&lt;/a> では次の手順で行うように書いてある。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl drain &amp;lt;node name&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>drain が正常終了すれば安全に k8s ノードを削除してよいのかな？&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl delete node &amp;lt;node name&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>eks クラスターで障害が発生していたときに drain を実行するとエラーになったのでそのまま delete node したら k8s ノードは削除されたものの、自動的に新しい k8s ノードが起動しなかった。aws のマネジメントコンソールから ec2 インスタンスを調べたら起動したままだったので強制的に ec2 インスタンスを終了させたところ、オートスケールの設定から ec2 インスタンスが起動してきて復旧した。但し、このやり方は k8s が意図した手順ではないようにも思える。軽く調べた範囲では k8s ノードの正しい削除方法 (置き換え方法？) がみつからなかった。そんなことを日曜日に確認していたら月曜日にほぼ同じ現象が本番環境の eks クラスターでも発生した。私は一度経験していたので同僚に指示して経過を観察していた。ここで書いたのと同じような手順で復旧した。おそらく aws 側のなにかのメンテナンス作業でうちの eks クラスターだと k8s ノードが死んでしまうような作業があったのではないか？と疑いをもっている。&lt;/p></content></item></channel></rss>