<!doctype html><html lang=en><head><title>operation :: forest nook</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=/diary/tags/operation/><link rel=stylesheet href=/diary/styles.css><link rel=stylesheet href=/diary/style.css><link rel="shortcut icon" href=/diary/favicon.ico><meta name=twitter:card content="summary"><meta name=twitter:site content="t2y"><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta property="og:title" content="operation"><meta property="og:description" content><meta property="og:url" content="/diary/tags/operation/"><meta property="og:site_name" content="forest nook"><meta property="og:image" content="/diary/favicon.ico"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><link href=/diary/tags/operation/index.xml rel=alternate type=application/rss+xml title="forest nook"></head><body class=green><div class="container full headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/diary><div class=logo>forest nook</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/diary/about>自己紹介</a></li><li><a href=/diary/dates>月別一覧</a></li><li><a href=/diary/tags>タグ一覧</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/diary/about>自己紹介</a></li><li><a href=/diary/dates>月別一覧</a></li><li><a href=/diary/tags>タグ一覧</a></li></ul></nav></header><div class=content><div class=posts><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2023/0922/>また podman に苦戦する</a></h1><div class=post-meta><time class=post-date>2023-09-22 (Fri.) ::</time></div><span class=post-tags>#<a href=/diary/tags/infrastructure/>infrastructure</a>&nbsp;
#<a href=/diary/tags/operation/>operation</a>&nbsp;
#<a href=/diary/tags/podman/>podman</a>&nbsp;</span><div class=post-content><p>23時に寝て何度か起きて7時に起きた。出張帰りでなんかバテててなにもせず休んでいた。少し喉に引っかかりがある。出張で飲み歩いたし、そろそろコロナ感染？の疑いをもって生活してみる。</p><h2 id=podman-と-dbus-daemon-とsystemd-の調査>podman と dbus-daemon とsystemd の調査</h2><p>2次開発の成果物をドッグフーディングの目的で社内へ導入する。メンバーが作業していて nginx が正常に動作しないという。ログをみろとすぐにコンテナネットワーク内の dns サービスが正常に動いていないということはわかった。podman は <a href=https://github.com/containers/aardvark-dns>aardvark-dns</a> というサービスを使って dns を管理する。但し、このサービスがまだまだ安定していなくて不具合があるのを以前にも確認した。このサービスの振る舞いがよく分からなくて、意図しない状況や状態に対して正常に動作してくれない。</p><p>他にも調査をしていて rootless で podman コマンドを実行すると次の issue で書かれているようなワーニングが出力される。dbus-user-session というパッケージを導入すれば解決するとある。</p><ul><li><a href=https://github.com/containers/podman/issues/12983>WARN[0000] The cgroupv2 manager is set to systemd but there is no systemd user session available #12983</a></li></ul><p>dbus-daemon のサービスは systemd で動いていて、systemd のユーザーモードと dbus が正常に動いていないというところまではすぐに分かった。その状態だと rootless な podman が正常に動作しないということもすぐに分かった。ここまではすぐに調査できたが、問題はどうやれば sytemd のユーザーモードを dbus を正常に動くように復旧できるのかがまったく分からない。systemd がそもそも難しいのに、そのユーザーモードは権限管理が関係するのでさらにもっと難しい。1日調べてお手上げで他の社員さんにも相談してみた。</p><p>今日は自分の作業は進捗しなかったけど、メンバーの作業の進捗をみていて、メンバーがはまっていたところを助言して、その問題は解決してうまくいって、それだけで満足していた。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2023/0720/>最後は人のチカラがモノを言う</a></h1><div class=post-meta><time class=post-date>2023-07-20 (Thu.) ::</time></div><span class=post-tags>#<a href=/diary/tags/operation/>operation</a>&nbsp;
#<a href=/diary/tags/design/>design</a>&nbsp;
#<a href=/diary/tags/management/>management</a>&nbsp;
#<a href=/diary/tags/communication/>communication</a>&nbsp;
#<a href=/diary/tags/event/>event</a>&nbsp;</span><div class=post-content><p>2時半に寝て寝たのかどうかよく分からないながら5時頃に寝落ちして7時半に起きた。</p><h2 id=差分比較のための機能>差分比較のための機能</h2><p>id 連携で運用のための非機能要件の1つとして更新された内容を確認できるようにしたい。非機能要件だから私が作るかと思ってあたためておいた issue に着手した。wikipedia によると差分という言葉には次の2つの用語があるのをみつけた。</p><ul><li><a href=https://en.wikipedia.org/wiki/Delta_update>Delta update</a></li><li><a href=https://en.wikipedia.org/wiki/Data_differencing>Data differencing</a></li></ul><p>数学やコンピューターの用語的には delta (ギリシャ語で変化を表す) という言葉を使う。os のパッケージングシステムで一部のパッケージをアップデートするようなことをデルタアップデートと呼ぶ。一方でコンピューターサイエンスにおいて2つのデータセット間の差分については diff という用語を使う。データの差分においては diff でよいのではないかと思う。そういった用語の定義から始めた。mongodb のコレクションのデータ定義をしたり、結合テストを書いて動かしてみたり、インフラのレイヤーから開発に着手した。</p><h2 id=上司道-企業家として生き様と人として求められること>上司道 企業家として生き様と、人として求められること</h2><p><a href="https://www.facebook.com/events/1287346985493718?ref=newsfeed">第92回上司道 企業家として生き様と、人として求められること</a> に参加した。なんとなくタイトルに惹かれた。<a href=/diary/posts/2023/0228/#上司道-野村監督から学ぶリーダーの器のつくり方>上司道</a> に参加するのは3回目。</p><p>講師の牛島さんは昨日が誕生日だったらしく90歳だという。90歳になって zoom でオンライン勉強会の講師を務めるというのを、私はまったく想像できないけど、コンサルタントの第一線で活躍されてきた方の貫禄があった。もともとどういう主旨の勉強会だったのかよく分かっていないけれど、内容はビジネスの自己啓発セミナーに近いものになった。牛島さんが90年も生きてきて大事だと思える内容には普遍性や汎用性があるのだと思う。いくつか共感できる考え方もあった。</p><ul><li>これからは頭の良い人 (IQ が高い) よりも心が豊かな人 (EQ が高い) の方が大事で組織に貢献する</li><li>一番大切なのは幸せであること</li><li>楽しく生きる (働く)</li></ul><p>過去に働いた会社でも頭がよくて何でもよく理解しているのにプロジェクトにあまり貢献しない人がいることに気付いた。さぼっているわけでもない。その違いを「心が豊かな人 (EQ が高い) 人」という言葉でいくつか説明できるのではないかと思えた。新規プロジェクトのような、常に変化して、正解もわからないまま進める業務において、論理や頭のよさだけでうまくいくことはなくなってきつつあるのではないか。なんのためにそのプロジェクトをやるのか、自分はなぜここで働いているのか、といった問いに答えをもっている人は普通の社員とは行動が異なる。自身の価値観や展望と比較して、現状の課題や改善に気付くのでプロジェクトを前向きに進めていける。頭のよい人は「あれが問題」「これが問題」と問題を指摘してエスカレーションするだけで自らが課題をどう解決するかの答えをもっていない。そんなことを考えながらこの話しを聞いていた。</p><p>次の2つは最近の私の人生観や働き方と重なるところがある。私はもう無理してがんばったりしないし、自分が嫌なお仕事も一切しないように決めている。一般的にいう「働きたくない」という生き方を目指している。もちろん実際には働いているわけだけど、それはなるべく働く時間を、遊んでいる時間に置き換えられないかと模索している。その過程で辛いことやしんどいことも避けようと考えている。</p><p>なぜそれができるかというのも、20代30代と約20年働いてきて自身の価値観を育ててきたからだと捉えている。私はなにが楽しくて、なにが辛くて、なにをやりたくないか。これは人それぞれに違う。私には私にしかない価値観をもっている。それがわかってきたから、いま自分の会社を経営していて、毎日がとても楽しいし、自分の価値観にあわないことはすべて断るという判断基準も明確になっている。そんな勝手気ままでやっていけるの？という懸念を抱く人も多いと思う。ダメかもしれない。仮にやっていけなかったとしても、いまの自分は幸せで楽しいのだからそれでいいんじゃないかと思う。無理をしていまがしんどくても将来がよくなる保証なんてどこにもない。</p><p>牛島さんはマザーテレサとインドで実際に会って10日間ほど一緒に過ごしてその体験がその後の人生を大きく変えたように話されていた。マザーテレサに「社員を大事にしていますか？」と聞かれたときに「しています。」と答え、その後に「社員全員の名前を覚えていますか？」と聞かれたという。当時の牛島さんの会社の社員は300人以上いて全員は覚えていなかった。それで「愛情の反対は無関心なのですよ。」とマザーテレサに指摘されて大きな衝撃を受けたという。その後、帰国してから300人以上の社員全員の名前を覚え、日々の業務で社員の行動などに気を配ってすべての社員に声をかけたりするようになったという。このエピソードもなかなか私には効く話しで、私は他人にかなりのレベルで関心がない。もし自分の会社で社員を雇うことになったら待遇がどうとか以前に、その人そのものに関心をもつという姿勢を覚えておこうと思う。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2023/0718/>1行のミスによる1行の修正</a></h1><div class=post-meta><time class=post-date>2023-07-18 (Tue.) ::</time></div><span class=post-tags>#<a href=/diary/tags/operation/>operation</a>&nbsp;
#<a href=/diary/tags/debug/>debug</a>&nbsp;</span><div class=post-content><p>0時に寝て何度か起きて6時半に起きた。朝から外はめっちゃ暑い。冷房をつけっぱなしのオフィスも朝からやっぱり暑い。根本的な空調の問題。</p><h2 id=agent-アプリケーションのメモリリーク正体>agent アプリケーションのメモリリーク正体</h2><p><a href=/diary/posts/2023/0714/#agent-アプリケーションのメモリリーク調査>先週のメモリリーク調査</a> の続き。本当は週末にやればよかったんだけど、遊んでたりさぼってたりして放置してた。先週時点でリークしているのは <a href=https://github.com/go-zeromq/zmq4>go-zeromq/zmq4</a> 側だというのはわかっていたが、何が原因でリークしているのかは分からなかった。一通りソースも読んでみたけど、いまひとつよく分からない。仕方ないから動的デバッグでソースコードに手を入れながら調査していて、すぐにみつけた。socket 構造体が保持しているコネクションの map がどんどん肥大化していく。なにも使っていない map にコネクションの値を保持して解放する処理がないことに気付いた。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#a6e22e>sck</span>.<span style=color:#a6e22e>ids</span>[<span style=color:#a6e22e>uuid</span>] = <span style=color:#a6e22e>c</span>
</span></span></code></pre></div><p>修正するかと思ってリポジトリの最新ブランチをみてもそのコードが見当たらない。すると次の pr で4月に修正されていた。まだリリースされていないからうちらのアプリケーションで使っているリビジョンにはその修正が含まれていなかった。</p><ul><li><a href=https://github.com/go-zeromq/zmq4/pull/140>Deal with empty Identity for ROUTER socket #140</a></li></ul><blockquote><p>Additionally, remove sck.ids, which is unused and leaks *Conn.</p></blockquote><p>メモリリークの調査を始めたときに github issues/pr を leak で検索して一通りチェックしているので、先週もこの pr をみかけているはずだが見逃してしまった。タイトルが全然違うし、ほんの1行の typo に近いミスなので修正内容をみて気付かなかったのだと思う。自分の観察力の無さに気付いた。leak で検索ヒットしているのだから、それが自分たちのアプリケーションで使っているコードに入っているのかどうか、その内容をもっと注意して調べるべきだった。そうすればこの調査時間を数時間は短縮できた。これは私のミスだと認めて <code>Postmortem</code> のラベルを付けた。次回の定例会議でふりかえりに使う。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2023/0714/>メモリリークに遭遇</a></h1><div class=post-meta><time class=post-date>2023-07-14 (Fri.) ::</time></div><span class=post-tags>#<a href=/diary/tags/operation/>operation</a>&nbsp;
#<a href=/diary/tags/testing/>testing</a>&nbsp;
#<a href=/diary/tags/go/>go</a>&nbsp;
#<a href=/diary/tags/design/>design</a>&nbsp;</span><div class=post-content><p>23時に寝て何度か起きて5時に起きてからだらだらネットしながら記事を読んだりしていて7時に起き上がった。</p><h2 id=agent-アプリケーションのメモリリーク調査>agent アプリケーションのメモリリーク調査</h2><p>qa テストの一環として先月からテスト環境で毎分 agent アプリケーションにリクエストを投げる長時間稼働テストを実行している。なんとなく気になるところがあったからやったわけではあるけれど、長時間稼働テストによってメモリリークを検出できてしまった。自分を過信せずちゃんと検証しないといけないなと思えた。top コマンドの実メモリー (RES) を1ヶ月前と比較して増えているからメモリリークだと気付いたところ。これからメモリプロファイリングをしながら原因を追求していく。私が書いた (レビューした) go のコードでメモリリークはないだろうと高をくくっていただけにちょっとショックではあった。</p><p>go は標準ライブラリに pprof というプロファイラがあるので簡単にデバッグできる。プロファイラで昨日から調査していたところ、<a href=https://github.com/go-zeromq/zmq4>go-zeromq/zmq4</a> の処理でメモリリークしていることはわかった。それがライブラリの使い方が誤っているのか、潜在的な不具合なのかはまだこれから調査するところ。</p><p>ライブラリ側の問題を調査するので厄介ではあるけど、私が書いた (レビューした) go のコードでメモリリークしているわけじゃないことがわかって少しほっとした。</p><h2 id=go-の-generics-勉強会>go の generics 勉強会</h2><p><a href="/diary/posts/2023/0707/#go-の-generics 勉強会の準備">先日準備した資料</a> を使って勉強会を開催した。</p><ul><li><a href=https://github.com/t2y/go-generics-study>https://github.com/t2y/go-generics-study</a></li></ul><p>この勉強会はある意味、うちのチームのメンバーが理解しておくべき内容なので go のプログラミングをやっていないメンバーが聞いてもあまり関心をもてない内容となっている。そういうお断りもした上で最悪2-3人ぐらいの参加者になるかと思ったもののプログラミングに関心がある人たちは参加してくれて5-6人ぐらいの規模にはなった。一方で内容も難しいし、私の説明がどれだけわかりやすかったか、私自身にはわからないのでなんとも言えない。質問も一切なかったので喋りきって疲れたという疲労感と、伝わったのか伝わらなかったのか分からない消化不良感と、金曜日だから今日はもういいや感でどっと疲れたというのが率直な感想になる。</p><p>とはいえ、私もずっと generics の仕様をちゃんと追いかけたいと思いながら先送りしていたものではあるので私の中では自分が go の generics の理解度をあげて実際の開発の中で使い分けるだけの判断基準をもてたことが収穫だったと言える。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2023/0621/>厄介なインフラ問題をやっつけた</a></h1><div class=post-meta><time class=post-date>2023-06-21 (Wed.) ::</time></div><span class=post-tags>#<a href=/diary/tags/infrastructure/>infrastructure</a>&nbsp;
#<a href=/diary/tags/operation/>operation</a>&nbsp;
#<a href=/diary/tags/podman/>podman</a>&nbsp;
#<a href=/diary/tags/event/>event</a>&nbsp;</span><div class=post-content><p>2時に寝て6時に起きて7時に起きた。夜に作業していたら遅くなった。</p><h2 id=厄介なインフラの問題-解決編>厄介なインフラの問題 解決編</h2><p><a href=/diary/posts/2023/0619/#運用のトラブルシューティング>運用のトラブルシューティング</a> の続き。アプリケーションアカウントを作って compose 環境を構築したら nginx のコンテナが起動して即時終了する状態になったという。これまで起きていた現象とまた違う問題が発生してさらに混迷をもたらすかに思えたが、私の中では nginx のコンテナでなにかがおかしいと問題の発生箇所を局所化できたのでそこからの調査はそんなに時間を必要としなかった。</p><p>結論からいうと podman の <a href=https://github.com/containers/aardvark-dns>aardvark-dns</a> の不具合だった。なんらかのトリガーでコンテナネットワーク内の名前解決が不整合な状態に陥る。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>vagrant@bookworm:$ podman-compose exec proxy /bin/bash
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>root@3742c45c7c60:/# dig app
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>; &lt;&lt;&gt;&gt; DiG 9.16.37-Debian &lt;&lt;&gt;&gt; app
</span></span><span style=display:flex><span>;; global options: +cmd
</span></span><span style=display:flex><span>;; Got answer:
</span></span><span style=display:flex><span>;; -&gt;&gt;HEADER<span style=color:#e6db74>&lt;&lt;- opco</span>de: QUERY, status: NOERROR, id: <span style=color:#ae81ff>56696</span>
</span></span><span style=display:flex><span>;; flags: qr rd ra ad; QUERY: 1, ANSWER: 8, AUTHORITY: 0, ADDITIONAL: <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>;; OPT PSEUDOSECTION:
</span></span><span style=display:flex><span>; EDNS: version: 0, flags:; udp: <span style=color:#ae81ff>4096</span>
</span></span><span style=display:flex><span>; COOKIE: 37ff0fd63315d70e <span style=color:#f92672>(</span>echoed<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>;; QUESTION SECTION:
</span></span><span style=display:flex><span>;app.				IN	A
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>;; ANSWER SECTION:
</span></span><span style=display:flex><span>app.			86400	IN	A	10.89.0.36
</span></span><span style=display:flex><span>app.			86400	IN	A	10.89.0.36
</span></span><span style=display:flex><span>app.			86400	IN	A	10.89.0.136
</span></span><span style=display:flex><span>app.			86400	IN	A	10.89.0.136
</span></span><span style=display:flex><span>app.			86400	IN	A	10.89.0.146
</span></span><span style=display:flex><span>app.			86400	IN	A	10.89.0.146
</span></span><span style=display:flex><span>app.			86400	IN	A	10.89.0.156
</span></span><span style=display:flex><span>app.			86400	IN	A	10.89.0.156
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>;; Query time: <span style=color:#ae81ff>4</span> msec
</span></span><span style=display:flex><span>;; SERVER: 10.89.0.1#53<span style=color:#f92672>(</span>10.89.0.1<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>;; WHEN: Thu Jun <span style=color:#ae81ff>22</span> 02:45:26 UTC <span style=color:#ae81ff>2023</span>
</span></span><span style=display:flex><span>;; MSG SIZE  rcvd: <span style=color:#ae81ff>172</span>
</span></span></code></pre></div><p>podman 4.0 から aardvark-dns がコンテナネットワーク内での dns を提供する。nginx が app を名前解決したときに起動しているコンテナの ip アドレスではなく、削除された過去のコンテナの ip アドレスが返される状況が発生する。app という名前に対して複数の ip アドレスが返る。</p><p>このとき nginx は複数の ip アドレスのうちの1つに接続しようとするが、正しい ip アドレスでない場合、リクエストがタイムアウトする。タイムアウトした後に fallback で他の ip アドレスに接続しにいく。このときに正しい ip アドレスがみつかればクライアントにレスポンスが返る。この fallback のリトライの回数分だけリクエストのレイテンシの時間がかかっていた。</p><pre tabindex=0><code>vagrant@bookworm:$ podman logs -f proxy
...
2023/06/22 02:46:26 [error] 15#15: *41 connect() failed (113: No route to host) while connecting to upstream, client: 10.89.0.38, server: ucidmsv1-app, request: &#34;GET / HTTP/1.1&#34;, upstream: &#34;http://10.89.0.136:3000/&#34;, host: &#34;localhost:4430&#34;
2023/06/22 02:46:29 [error] 15#15: *41 connect() failed (113: No route to host) while connecting to upstream, client: 10.89.0.38, server: ucidmsv1-app, request: &#34;GET / HTTP/1.1&#34;, upstream: &#34;http://10.89.0.156:3000/&#34;, host: &#34;localhost:4430&#34;
2023/06/22 02:46:32 [error] 15#15: *41 connect() failed (113: No route to host) while connecting to upstream, client: 10.89.0.38, server: ucidmsv1-app, request: &#34;GET / HTTP/1.1&#34;, upstream: &#34;http://10.89.0.136:3000/&#34;, host: &#34;localhost:4430&#34;
10.89.0.38 - - [22/Jun/2023:02:46:32 +0000] &#34;GET / HTTP/1.1&#34; 200 2864 &#34;-&#34; &#34;curl/7.88.1&#34;
</code></pre><p>ワークアラウンドとして、次のファイルに複数の app の ip アドレスが登録されていれば不整合な状態なのでネットワークを削除して、このファイルも手動で削除してしまえばよい。</p><pre tabindex=0><code>$ cat /run/user/$(id -u)/containers/networks/aardvark-dns/mynetwork
</code></pre><p>ファイルを監視していると、どうやら mynetwork ファイルから名前と ip アドレスの情報が削除されるのは該当のコンテナが削除されるタイミングになる。なんらかのエラーにより、コンテナ削除時にマッピングの削除が実行されないと、古いコンテナのマッピング設定が残ったままとなり、compose サービスを起動したときに複数の ip アドレスの名前解決できる状態になってしまう。ちょっと調べても aardvark-dns に関する issue はたくさん登録されている。</p><ul><li><a href=https://github.com/containers/podman/issues/18783>https://github.com/containers/podman/issues/18783</a></li><li><a href=https://github.com/containers/podman/issues/18530>https://github.com/containers/podman/issues/18530</a></li><li><a href=https://github.com/containers/podman/issues/17370>https://github.com/containers/podman/issues/17370</a></li></ul><h2 id=コワーキングのオンラインイベント>コワーキングのオンラインイベント</h2><p>月例のカフーツさんのオンラインイベントに参加した。<a href=/diary/posts/2023/0517/#コワーキングのオンラインイベント>先月の所感はここ</a> 。今日はもともと予定していた話しをする参加者が急遽参加できなくなってしまったので他の参加者での雑談会になった。</p><p>いとうさん曰く、これまで外国人のデジタルノマドは自分で業務時間を選べるフリーランスの、さらにお金に余裕をもった人たちが多いと考えられていた。しかし、実際にコワーキングスペースに来られている外国人にキャリアを伺うと、大企業の普通の社員であることがわかってきた。グローバルな会社だと、働く場所に制限のない会社もあって、ただ日本へ行ってみたかった的な理由で日本へ来られて数ヶ月滞在して普通に会社のお仕事をするといったデジタルノマドもいるという。過去に私が働いていた職場の同僚も、コロナのときに会社がフルリモートワークの体制を設けて、airbnb で全国を旅しながら1年ほど働いていた。日本でもそういう社員はいるのだから外国人はなおさらという感じ。</p><p>そういった外国人のデジタルノマドが要求することが3つある。</p><ul><li>24時間利用できること (勤め先の会社と時差があるから)</li><li>セカンドモニターがあること</li><li>・・・ (あともう1つあったが、忘れてしまった)</li></ul><p>コワーキングスペースに外国人のデジタルノマドを呼び込むにはどうすればよいか。実際にコワーキングスペースへ来られた外国人に理由を伺うと英語のホームページをみて来ましたということらしい。至極、当たり前の話し。英語のホームページをちゃんと作ろうねみたいな話題で話していた。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2023/0619/>運用トラブルの調査</a></h1><div class=post-meta><time class=post-date>2023-06-19 (Mon.) ::</time></div><span class=post-tags>#<a href=/diary/tags/infrastructure/>infrastructure</a>&nbsp;
#<a href=/diary/tags/operation/>operation</a>&nbsp;
#<a href=/diary/tags/web-design/>web design</a>&nbsp;</span><div class=post-content><p>0時に寝て5時に起きて7時に起きた。もう暑くて家でもエアコンを解禁した。エアコンがあると寝心地が違う、快適。</p><h2 id=運用のトラブルシューティング>運用のトラブルシューティング</h2><p><a href=/diary/posts/2023/0613/#厄介なインフラの問題-x-2>厄介なインフラの問題</a> のクリティカルな方から着手し始めた。<a href=https://github.com/containers/podman-compose>podman-compose</a> を使って rootless な環境構築をやってみたところ、nginx を tls 終端としてリバースプロキシとするアプリケーションサーバーとの通信が数回に1回ぐらいの頻度で遅くなる。通常は 100msec 程度でレスポンスが返るのが数秒から数十秒かかる。</p><p>もともと podman-compose はサポート対象外なのでそんながんばる必要はない。しかし、これも調査の過程でコンテナの技術を学ぶ1つだと考え再現環境を構築しようとした。vagrant の debian 12 と podman-compose をインストールして同様に環境構築してみたが、仮想環境では再現しない。どうやら環境要因のようだ。そこで問題が発生しているマシンで私のアカウントで環境構築してみたが、やはり再現しない。なんと個人アカウントの違いによって起きる現象のようだ。また質が悪いのは私のアカウントでは再現しないが、メンバー2人のアカウントでは再現している。一般ユーザーから他人のユーザーのプロセスやコンテナの情報にアクセスできるわけがないので調査ができない。個人アカウントで compose 環境を構築するのは諦めてアプリケーションアカウントを作ってやりましょうという話しにした。アプリケーションアカウントで再現すれば調査するし、再現しなければこんな環境要因のトラブルシューティングの優先度を下げてもいいかなぁとも考えている。どうなるかなぁ。</p><h2 id=サイトデザインのサンプルページ>サイトデザインのサンプルページ</h2><p><a href=/diary/posts/2023/0522/#サイトデザイン打ち合わせ>サイトデザイン打ち合わせ</a> の続き。実際のサンプルが出てきたのでデザインの雰囲気やコードも含めて確認していく。ざっとサンプルページを確認した。デザインはとても気に入っている。あとは私が hugo のテーマとしてテンプレートの組み込めるかどうか次第。今週末には実家帰らないといけないし、毎日やることがいっぱいいっぱい。</p></div></article><article class="post on-list"><h1 class=post-title><a href=/diary/posts/2022/0826/>簡単な現象の組み合わせ障害</a></h1><div class=post-meta><time class=post-date>2022-08-26 (Fri.) ::</time></div><span class=post-tags>#<a href=/diary/tags/kubernetes/>kubernetes</a>&nbsp;
#<a href=/diary/tags/datadog/>datadog</a>&nbsp;
#<a href=/diary/tags/operation/>operation</a>&nbsp;
#<a href=/diary/tags/drinking/>drinking</a>&nbsp;
#<a href=/diary/tags/tax/>tax</a>&nbsp;</span><div class=post-content><p>0時に寝て6時に起きた。</p><h2 id=eks-クラスター障害の原因判明>eks クラスター障害の原因判明</h2><p><a href=/diary/posts/2022/0820/#aws-インフラの調子が悪い>過去に2回発生していた eks クラスター障害</a> の原因がようやくわかった。テスト環境も本番環境は5日ごとに再現していて、datadog で k8s のダッシュボードでそれぞれの pod 単位のメモリ使用量をみると datadog-agent の pod がメモリリークしていることに気付いた。そこから当たりをつけて datadog-agent の issue を調べると次のバグに遭遇していた。</p><ul><li><a href=https://github.com/DataDog/datadog-agent/issues/12997>[BUG] agent leaves defunct processes with version 7.38.0 #12997</a></li></ul><p>ゾンビプロセスが生成されて、それが os のプロセス数上限に達してしまい、それによってプロセス (スレッド) が生成できなくなって、その結果として <a href=https://github.com/aws/amazon-vpc-cni-k8s>aws/amazon-vpc-cni-k8s</a> の <code>aws-node</code> という eks クラスターの管理アプリケーションが動かなくなって、それが動かないと k8s ノードのステータスが NotReady になってしまって、通常の pod のアプリケーションも動かなくなってしまうという現象が発生していた。datadog-agent のアップグレードは私が行ったものだし、その後の k8s ノードの監視や調査で気付きが足りなかったと反省した。</p><ul><li>datadog-agent の新しいバージョンをテスト環境でもうしばらく検証してもよかった</li><li>datadog-agent をリソースリークの可能性を私の中の調査対象から外していた<ul><li>世の中で使われているものに致命的なバグが起きないだろうという先入観があった</li></ul></li><li>プロセスを生成できない原因として考えられる背景を調査すべきだった<ul><li>ulimit を確認してリソース制限はないようにみえた</li><li>プロセス数やゾンビプロセスを調べていなかった</li><li>kernel に <code>/proc/sys/kernel/pid_max</code> という上限設定があることを知らなかった</li></ul></li><li>テスト環境と本番環境で5日程度で落ちるという周期性から気付くべきだった<ul><li>たしかにテスト環境から1日遅れて本番環境で障害が発生していた</li><li>周期性があることでリソースリークの可能性は高いとすぐに調査すべきだった</li></ul></li><li>datadog で k8s のダッシュボードを調べるべきだった<ul><li>すでに用意されているものがあったのでみようと思えばみえた</li></ul></li><li>aws のインフラ要因ではないかと疑っていた<ul><li>ごめんなさい</li></ul></li></ul><p>これは悔しい。自分の無能さや気付きの低さを実感した事件だった。私が注意深く観察していればもう1週間早く気付けた。そのせいで余分な障害と調査に時間を費やした。1つ1つは全く難しくない現象が巧妙に絡みあって隠蔽された結果としての状況に気付けなかった。注意して1つずつ観察して追跡していけばすぐに気付けた。本当に悔しい。</p><p>1つだけ言い訳をさせてもらうと、私は本番環境にアクセスできない。だからテスト環境と本番環境で発生している現象が同じかどうかを判断できず、調査を進める確証をもてなかった。</p><h2 id=呑み>呑み</h2><p>あまりに悔しかったのと調査してたら遅くなって晩ご飯食べる気力もなかったので気分転換に仲のよい焼き鳥屋さんに寄ってみた。あとから常連客のセブンイレブンの店長さんも来られて、私は初対面かなと思ってたんだけど先方は知っていると言ってたから以前にもカウンターでご一緒していたみたい。何気はなしに3人で2時前ぐらいまで雑談していた。</p><p>その店長さんがロレックスを購入しようと考えているという話しになって、資産または投資商品としてのロレックスの話しになった。たまたまヒカキンが1億円で買ったロレックスがいま2億円になっているといった話しがあったそうで、いまがバブルな状態らしいが、ロレックスをはじめとした高級時計の資産価値が上がっているらしい。私は腕時計を身につけないし高級時計もまったく興味はないが、投資商品の1つなんだというところに関心がもてた。</p><div class=video-container><iframe src=https://www.youtube.com/embed/1knsQZLeh7U allowfullscreen title=1億円で買った時計が大変なことになってしまいました…></iframe></div><p>中小企業の社長の一般的な節税方法の1つに外車を買ったり売ったりするという話しがある。儲かったときに経費で外車を買って、赤字のときに外車を売って雑所得に変える。車は社用車として経費で落とせるから可能なことだが、高級時計はどうなのだろうか？ 結論から言うと、普通の会社では高級時計は経費にできない。経費の原則は売上を上げるために必要な支出を経費とできる。普通の会社は高級時計で売上を上げることはできない。一方で経費として認められる職業もある。芸能人がそうだという。それは番組のために必要だという理屈で経費で落とせる。おそらくヒカキンも経費で高級時計を購入して、そのことを動画にしているのも仕事で必要だという言い訳作りの目的もあるのだと推測する。</p></div></article><div class=pagination><div class=pagination__buttons><a href=/diary/tags/operation/page/2/ class="button next"><span class=button__text>過去の日記</span>
<span class=button__icon>→</span></a></div></div></div></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>© 2021 Tetsuya Morimoto</span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script type=text/javascript src=/diary/bundle.min.js></script></div></body></html>